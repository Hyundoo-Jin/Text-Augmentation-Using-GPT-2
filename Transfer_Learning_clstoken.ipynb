{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /media/jin/Workspace/text_augmentation/gpt2_kor/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse, json, os, sys, time, tqdm, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import sentencepiece as spm\n",
    "from load_dataset import load_dataset, Sampler\n",
    "\n",
    "import encoder\n",
    "import sample_cls as sample\n",
    "import transfer_clstoken as model\n",
    "# import model\n",
    "from accumulate import AccumulatingOptimizer\n",
    "import memory_saving_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python train.py --dataset ../long_paragraphs.txt --sample_every 1000 --encoder_path ../long_train.model --n_ctx 512 --sample_tokens 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments() :\n",
    "    def __init__(self, path) :\n",
    "        with open(path, 'r') as f :\n",
    "            args = json.load(f)\n",
    "        for k, v in args.items() :\n",
    "            if type(v) == str :\n",
    "                exec(\"self.{} = '{}'\".format(k, v))\n",
    "            else :\n",
    "                exec(\"self.{} = {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maketree(path) :\n",
    "    try :\n",
    "        os.makedirs(path)\n",
    "    except :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(context, hparams, p) :\n",
    "    if p > 0 :\n",
    "        mask = tf.random.uniform(shape = tf.shape(context)) < p\n",
    "        noise = tf.random.uniform(shape = tf.shape(context), minval = 0, maxval = hparams.n_vocab, dtype = tf.int32)\n",
    "        return tf.where(mask, noise, context)\n",
    "    else :\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "SAMPLE_DIR = 'samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arugment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_path = 'parameter.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments(parameter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.run_name = 'no_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.label_weight = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(args.encoder_path)\n",
    "hparams = model.default_hparams()\n",
    "with open('hparams.json') as f:\n",
    "        hparams.override_from_dict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.n_vocab = hparams.n_vocab + 9   # cls 토큰 수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sample_length > hparams.n_ctx :\n",
    "        raise ValueError(\n",
    "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "# config = tf.ConfigProto(\n",
    "#             device_count = {'GPU': 0}\n",
    "#         )\n",
    "config.gpu_options.allow_growth = True\n",
    "config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.sample_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jin-lab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jin-lab/workspace/text_augmentation/gpt2_kor/src/sample_cls.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jin-lab/workspace/text_augmentation/gpt2_kor/src/sample_cls.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-075266368060>:36: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From /home/jin-lab/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jin-lab/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/jin-lab/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/transfer3/model-25976\n",
      "dataset has 2115216 tokens\n",
      "Training...\n",
      "[25977 | 9.62] loss=0.68 avg=0.68\n",
      "[25978 | 9.74] loss=1.35 avg=1.02\n",
      "[25979 | 9.85] loss=1.12 avg=1.05\n",
      "[25980 | 9.98] loss=0.57 avg=0.93\n",
      "[25981 | 10.10] loss=1.89 avg=1.13\n",
      "[25982 | 10.21] loss=0.54 avg=1.02\n",
      "[25983 | 10.36] loss=2.96 avg=1.31\n",
      "[25984 | 10.48] loss=1.10 avg=1.28\n",
      "[25985 | 10.60] loss=0.56 avg=1.20\n",
      "[25986 | 10.70] loss=0.64 avg=1.14\n",
      "[25987 | 10.82] loss=1.22 avg=1.15\n",
      "[25988 | 10.94] loss=1.25 avg=1.16\n",
      "[25989 | 11.06] loss=0.41 avg=1.10\n",
      "[25990 | 11.17] loss=0.65 avg=1.06\n",
      "[25991 | 11.30] loss=0.48 avg=1.02\n",
      "[25992 | 11.41] loss=2.89 avg=1.15\n",
      "[25993 | 11.55] loss=2.93 avg=1.26\n",
      "[25994 | 11.66] loss=0.87 avg=1.24\n",
      "[25995 | 11.77] loss=0.96 avg=1.22\n",
      "[25996 | 11.92] loss=0.56 avg=1.19\n",
      "[25997 | 12.03] loss=2.05 avg=1.23\n",
      "[25998 | 12.13] loss=2.36 avg=1.29\n",
      "[25999 | 12.24] loss=2.50 avg=1.35\n",
      "Saving checkpoint/transfer3/model-26000\n",
      "Generating samples...\n",
      "Seed text : 보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다.\n",
      "[10001 10002]\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 보온병 안에 물을 따뜻하게 유지하지 천은 열의 온도가 열이 빠져나가지 않는다. 열기구 안을 내가지 못하게 하는것 같다\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 물을 흡수해주는 역할을 한다. 액체 공기가  ⁇ 문에 열이 전달을 차단하기때문이다. 열이 흡수하여 온도가 높은\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 복에서는 열이가 열이 이동하게 해 주 위해 보온 병 안쪽이 있기 때문에 열이 이동되기 때문이다. 온도가 빨라진다.\n",
      "[26000 | 22.59] loss=0.48 avg=1.31\n",
      "[26001 | 22.75] loss=5.16 avg=1.48\n",
      "[26002 | 22.90] loss=3.21 avg=1.56\n",
      "[26003 | 23.01] loss=1.17 avg=1.54\n",
      "[26004 | 23.13] loss=0.61 avg=1.50\n",
      "[26005 | 23.26] loss=0.12 avg=1.45\n",
      "[26006 | 23.38] loss=0.86 avg=1.42\n",
      "[26007 | 23.52] loss=0.11 avg=1.38\n",
      "[26008 | 23.65] loss=0.93 avg=1.36\n",
      "[26009 | 23.76] loss=1.20 avg=1.35\n",
      "[26010 | 23.86] loss=0.76 avg=1.33\n",
      "[26011 | 23.99] loss=0.12 avg=1.29\n",
      "[26012 | 24.10] loss=1.65 avg=1.30\n",
      "[26013 | 24.23] loss=0.84 avg=1.29\n",
      "[26014 | 24.35] loss=1.06 avg=1.28\n",
      "[26015 | 24.47] loss=0.45 avg=1.26\n",
      "[26016 | 24.58] loss=0.86 avg=1.24\n",
      "[26017 | 24.69] loss=0.92 avg=1.23\n",
      "[26018 | 24.82] loss=0.60 avg=1.22\n",
      "[26019 | 24.94] loss=0.83 avg=1.21\n",
      "[26020 | 25.06] loss=1.20 avg=1.20\n",
      "[26021 | 25.17] loss=0.88 avg=1.20\n",
      "[26022 | 25.29] loss=0.37 avg=1.17\n",
      "[26023 | 25.41] loss=0.81 avg=1.16\n",
      "[26024 | 25.54] loss=1.60 avg=1.18\n",
      "[26025 | 25.66] loss=0.30 avg=1.15\n",
      "[26026 | 25.78] loss=1.07 avg=1.15\n",
      "[26027 | 25.90] loss=1.44 avg=1.16\n",
      "[26028 | 26.03] loss=2.87 avg=1.20\n",
      "[26029 | 26.14] loss=2.28 avg=1.23\n",
      "[26030 | 26.26] loss=0.25 avg=1.20\n",
      "[26031 | 26.37] loss=0.84 avg=1.19\n",
      "[26032 | 26.50] loss=0.86 avg=1.19\n",
      "[26033 | 26.64] loss=4.88 avg=1.27\n",
      "[26034 | 26.77] loss=0.50 avg=1.25\n",
      "[26035 | 26.88] loss=0.55 avg=1.24\n",
      "[26036 | 27.01] loss=1.05 avg=1.23\n",
      "[26037 | 27.12] loss=2.92 avg=1.27\n",
      "[26038 | 27.23] loss=0.95 avg=1.26\n",
      "[26039 | 27.36] loss=1.18 avg=1.26\n",
      "[26040 | 27.47] loss=3.43 avg=1.31\n",
      "[26041 | 27.58] loss=1.19 avg=1.31\n",
      "[26042 | 27.72] loss=1.75 avg=1.31\n",
      "[26043 | 27.83] loss=2.45 avg=1.34\n",
      "[26044 | 27.97] loss=0.97 avg=1.33\n",
      "[26045 | 28.10] loss=0.64 avg=1.32\n",
      "[26046 | 28.22] loss=1.59 avg=1.32\n",
      "[26047 | 28.34] loss=1.51 avg=1.33\n",
      "[26048 | 28.45] loss=2.21 avg=1.34\n",
      "[26049 | 28.59] loss=2.15 avg=1.36\n",
      "[26050 | 28.71] loss=0.81 avg=1.35\n",
      "[26051 | 28.84] loss=1.79 avg=1.36\n",
      "[26052 | 28.96] loss=0.60 avg=1.34\n",
      "[26053 | 29.12] loss=4.16 avg=1.39\n",
      "[26054 | 29.25] loss=0.41 avg=1.38\n",
      "[26055 | 29.36] loss=2.61 avg=1.40\n",
      "[26056 | 29.47] loss=1.25 avg=1.40\n",
      "[26057 | 29.59] loss=0.76 avg=1.38\n",
      "[26058 | 29.70] loss=2.11 avg=1.40\n",
      "[26059 | 29.82] loss=1.85 avg=1.41\n",
      "[26060 | 29.94] loss=2.41 avg=1.42\n",
      "[26061 | 30.05] loss=1.78 avg=1.43\n",
      "[26062 | 30.17] loss=1.02 avg=1.42\n",
      "[26063 | 30.28] loss=0.39 avg=1.40\n",
      "[26064 | 30.39] loss=0.49 avg=1.39\n",
      "[26065 | 30.50] loss=1.12 avg=1.38\n",
      "[26066 | 30.61] loss=1.17 avg=1.38\n",
      "[26067 | 30.74] loss=1.00 avg=1.37\n",
      "[26068 | 30.86] loss=1.27 avg=1.37\n",
      "[26069 | 30.99] loss=3.73 avg=1.41\n",
      "[26070 | 31.12] loss=1.06 avg=1.41\n",
      "[26071 | 31.25] loss=3.46 avg=1.44\n",
      "[26072 | 31.36] loss=3.91 avg=1.48\n",
      "[26073 | 31.49] loss=0.40 avg=1.46\n",
      "[26074 | 31.60] loss=0.95 avg=1.45\n",
      "[26075 | 31.72] loss=1.10 avg=1.45\n",
      "[26076 | 31.85] loss=0.39 avg=1.43\n",
      "[26077 | 31.96] loss=2.62 avg=1.45\n",
      "[26078 | 32.07] loss=0.77 avg=1.44\n",
      "[26079 | 32.18] loss=0.79 avg=1.43\n",
      "[26080 | 32.29] loss=2.09 avg=1.44\n",
      "[26081 | 32.40] loss=0.80 avg=1.43\n",
      "[26082 | 32.51] loss=0.16 avg=1.41\n",
      "[26083 | 32.65] loss=3.70 avg=1.44\n",
      "[26084 | 32.76] loss=0.84 avg=1.44\n",
      "[26085 | 32.89] loss=2.85 avg=1.46\n",
      "[26086 | 33.00] loss=1.43 avg=1.46\n",
      "[26087 | 33.13] loss=0.34 avg=1.44\n",
      "[26088 | 33.26] loss=3.41 avg=1.47\n",
      "[26089 | 33.38] loss=0.39 avg=1.45\n",
      "[26090 | 33.49] loss=1.76 avg=1.46\n",
      "[26091 | 33.61] loss=0.81 avg=1.45\n",
      "[26092 | 33.72] loss=1.02 avg=1.44\n",
      "[26093 | 33.84] loss=1.48 avg=1.44\n",
      "[26094 | 33.98] loss=2.02 avg=1.45\n",
      "[26095 | 34.11] loss=3.00 avg=1.47\n",
      "[26096 | 34.24] loss=0.33 avg=1.46\n",
      "[26097 | 34.37] loss=0.11 avg=1.44\n",
      "[26098 | 34.50] loss=1.64 avg=1.44\n",
      "[26099 | 34.63] loss=3.94 avg=1.48\n",
      "Generating samples...\n",
      "Seed text : 긴 겨울을 지나고 따뜻한 봄이 오더니 어느새 무더운 여름이 됩니다. 다시 시원한 가을이 되어 어느새 추운 겨울이 됩니다. 매년 반복되는 계절의 변화는 어떤 원리로 생기는 것일까요? 지구와 계절의 변화에 관한 지식을 활용하여 그 과정을 자세하게 설명해 봅시다.\n",
      "[10000]\n",
      "긴 겨울을 지나고 따뜻한 봄이 오더니 어느새 무더운 여름이 됩니다. 다시 시원한 가을이 되어 어느새 추운 겨울이 됩니다. 매년 반복되는 계절의 변화는 어떤 원리로 생기는 것일까요? 지구와 계절의 변화에 관한 지식을 활용하여 그 과정을 자세하게 설명해 봅시다. 지구가 태양의 공전하면서 계절이 기울어져 자전되기 때문이다.. 이 계절이 생긴다..........\n",
      "긴 겨울을 지나고 따뜻한 봄이 오더니 어느새 무더운 여름이 됩니다. 다시 시원한 가을이 되어 어느새 추운 겨울이 됩니다. 매년 반복되는 계절의 변화는 어떤 원리로 생기는 것일까요? 지구와 계절의 변화에 관한 지식을 활용하여 그 과정을 자세하게 설명해 봅시다. 지구가 태양주위를 공전하는데 기울어져 공전하기 때문에 계절이 생기게 됩니다..가 태양이 달라진 태양과 달의 변화가\n",
      "긴 겨울을 지나고 따뜻한 봄이 오더니 어느새 무더운 여름이 됩니다. 다시 시원한 가을이 되어 어느새 추운 겨울이 됩니다. 매년 반복되는 계절의 변화는 어떤 원리로 생기는 것일까요? 지구와 계절의 변화에 관한 지식을 활용하여 그 과정을 자세하게 설명해 봅시다. 지구가 태양에너지를위를 공전해서 계절이 생긴다. 그 지구는 태양을 중심으로 기울 되어 불이 날마다 계절이 달라진다.\n",
      "[26100 | 36.67] loss=0.74 avg=1.46\n",
      "[26101 | 36.80] loss=0.53 avg=1.45\n",
      "[26102 | 36.93] loss=1.34 avg=1.45\n",
      "[26103 | 37.07] loss=1.38 avg=1.45\n",
      "[26104 | 37.19] loss=1.31 avg=1.45\n",
      "[26105 | 37.32] loss=1.63 avg=1.45\n",
      "[26106 | 37.43] loss=2.78 avg=1.47\n",
      "[26107 | 37.58] loss=0.72 avg=1.46\n",
      "[26108 | 37.70] loss=0.51 avg=1.45\n",
      "[26109 | 37.81] loss=0.92 avg=1.44\n",
      "[26110 | 37.94] loss=4.34 avg=1.48\n",
      "[26111 | 38.06] loss=1.08 avg=1.47\n",
      "[26112 | 38.17] loss=2.13 avg=1.48\n",
      "[26113 | 38.29] loss=0.18 avg=1.46\n",
      "[26114 | 38.41] loss=1.32 avg=1.46\n",
      "[26115 | 38.53] loss=1.05 avg=1.46\n",
      "[26116 | 38.64] loss=1.76 avg=1.46\n",
      "[26117 | 38.77] loss=2.60 avg=1.47\n",
      "[26118 | 38.88] loss=1.17 avg=1.47\n",
      "[26119 | 39.02] loss=2.85 avg=1.49\n",
      "[26120 | 39.14] loss=2.86 avg=1.51\n",
      "[26121 | 39.26] loss=2.91 avg=1.53\n",
      "[26122 | 39.37] loss=0.89 avg=1.52\n",
      "[26123 | 39.48] loss=0.46 avg=1.50\n",
      "[26124 | 39.60] loss=1.09 avg=1.50\n",
      "[26125 | 39.73] loss=0.83 avg=1.49\n",
      "[26126 | 39.84] loss=0.32 avg=1.47\n",
      "[26127 | 39.96] loss=0.75 avg=1.46\n",
      "[26128 | 40.07] loss=1.66 avg=1.47\n",
      "[26129 | 40.20] loss=1.00 avg=1.46\n",
      "[26130 | 40.33] loss=3.09 avg=1.48\n",
      "[26131 | 40.44] loss=0.47 avg=1.47\n",
      "[26132 | 40.56] loss=0.83 avg=1.46\n",
      "[26133 | 40.69] loss=3.25 avg=1.48\n",
      "[26134 | 40.82] loss=0.95 avg=1.48\n",
      "[26135 | 40.93] loss=2.65 avg=1.49\n",
      "[26136 | 41.06] loss=1.41 avg=1.49\n",
      "[26137 | 41.17] loss=2.26 avg=1.50\n",
      "[26138 | 41.28] loss=0.62 avg=1.49\n",
      "[26139 | 41.41] loss=1.08 avg=1.48\n",
      "[26140 | 41.56] loss=0.30 avg=1.47\n",
      "[26141 | 41.68] loss=0.40 avg=1.46\n",
      "[26142 | 41.80] loss=0.48 avg=1.44\n",
      "[26143 | 41.93] loss=0.34 avg=1.43\n",
      "[26144 | 42.04] loss=0.83 avg=1.42\n",
      "[26145 | 42.18] loss=3.20 avg=1.45\n",
      "[26146 | 42.29] loss=2.31 avg=1.46\n",
      "[26147 | 42.40] loss=2.76 avg=1.47\n",
      "[26148 | 42.51] loss=0.13 avg=1.46\n",
      "[26149 | 42.62] loss=2.63 avg=1.47\n",
      "[26150 | 42.73] loss=1.87 avg=1.47\n",
      "[26151 | 42.84] loss=3.92 avg=1.50\n",
      "[26152 | 42.95] loss=2.68 avg=1.52\n",
      "[26153 | 43.07] loss=0.86 avg=1.51\n",
      "[26154 | 43.18] loss=1.75 avg=1.51\n",
      "[26155 | 43.29] loss=0.75 avg=1.50\n",
      "[26156 | 43.42] loss=1.14 avg=1.50\n",
      "[26157 | 43.52] loss=0.19 avg=1.48\n",
      "[26158 | 43.64] loss=1.74 avg=1.49\n",
      "[26159 | 43.76] loss=0.14 avg=1.47\n",
      "[26160 | 43.87] loss=0.99 avg=1.47\n",
      "[26161 | 44.00] loss=0.67 avg=1.46\n",
      "[26162 | 44.11] loss=0.87 avg=1.45\n",
      "[26163 | 44.23] loss=0.87 avg=1.44\n",
      "[26164 | 44.35] loss=3.20 avg=1.46\n",
      "[26165 | 44.47] loss=0.49 avg=1.45\n",
      "[26166 | 44.59] loss=0.38 avg=1.44\n",
      "[26167 | 44.70] loss=2.00 avg=1.45\n",
      "[26168 | 44.83] loss=0.45 avg=1.43\n",
      "[26169 | 44.95] loss=0.52 avg=1.42\n",
      "[26170 | 45.09] loss=1.05 avg=1.42\n",
      "[26171 | 45.20] loss=1.09 avg=1.42\n",
      "[26172 | 45.33] loss=0.82 avg=1.41\n",
      "[26173 | 45.44] loss=1.53 avg=1.41\n",
      "[26174 | 45.57] loss=1.05 avg=1.41\n",
      "[26175 | 45.70] loss=3.42 avg=1.43\n",
      "[26176 | 45.82] loss=1.81 avg=1.43\n",
      "[26177 | 45.93] loss=0.61 avg=1.42\n",
      "[26178 | 46.05] loss=0.49 avg=1.41\n",
      "[26179 | 46.18] loss=1.39 avg=1.41\n",
      "[26180 | 46.31] loss=0.69 avg=1.40\n",
      "[26181 | 46.47] loss=1.15 avg=1.40\n",
      "[26182 | 46.59] loss=2.72 avg=1.42\n",
      "[26183 | 46.72] loss=3.57 avg=1.44\n",
      "[26184 | 46.84] loss=0.07 avg=1.43\n",
      "[26185 | 46.98] loss=1.37 avg=1.42\n",
      "[26186 | 47.12] loss=0.22 avg=1.41\n",
      "[26187 | 47.24] loss=0.57 avg=1.40\n",
      "[26188 | 47.36] loss=0.75 avg=1.39\n",
      "[26189 | 47.48] loss=1.86 avg=1.40\n",
      "[26190 | 47.60] loss=1.33 avg=1.40\n",
      "[26191 | 47.74] loss=0.59 avg=1.39\n",
      "[26192 | 47.86] loss=0.55 avg=1.38\n",
      "[26193 | 48.00] loss=2.04 avg=1.39\n",
      "[26194 | 48.13] loss=0.37 avg=1.38\n",
      "[26195 | 48.25] loss=0.73 avg=1.37\n",
      "[26196 | 48.36] loss=1.48 avg=1.37\n",
      "[26197 | 48.49] loss=0.71 avg=1.36\n",
      "[26198 | 48.60] loss=1.28 avg=1.36\n",
      "[26199 | 48.72] loss=1.21 avg=1.36\n",
      "Generating samples...\n",
      "Seed text : 20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요?\n",
      "[10000]\n",
      "20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요? 생활에 편리하게 해준다... 그리고.....................\n",
      "20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요? 과학기술의 발달은 생활에서는 필요한함을 소리지만 편리할 수있고 과학기술은 편리하게 없다. ⁇  정보를 더 좋은 가격도있다..\n",
      "20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요? 과학기술이 발달으로 발달할 수있게 해준다. 먹이가 많아지고 있게 해준다.. 하지만 더 많이 만들어진다.. 그리고.  ⁇ \n",
      "[26200 | 50.67] loss=3.00 avg=1.38\n",
      "[26201 | 50.78] loss=0.70 avg=1.37\n",
      "[26202 | 50.89] loss=2.26 avg=1.38\n",
      "[26203 | 51.02] loss=0.69 avg=1.37\n",
      "[26204 | 51.15] loss=0.65 avg=1.36\n",
      "[26205 | 51.27] loss=1.23 avg=1.36\n",
      "[26206 | 51.40] loss=2.29 avg=1.37\n",
      "[26207 | 51.52] loss=2.95 avg=1.39\n",
      "[26208 | 51.63] loss=1.90 avg=1.40\n",
      "[26209 | 51.74] loss=2.38 avg=1.41\n",
      "[26210 | 51.85] loss=1.16 avg=1.41\n",
      "[26211 | 51.96] loss=0.94 avg=1.40\n",
      "[26212 | 52.07] loss=0.25 avg=1.39\n",
      "[26213 | 52.20] loss=0.99 avg=1.38\n",
      "[26214 | 52.32] loss=2.10 avg=1.39\n",
      "[26215 | 52.45] loss=2.30 avg=1.40\n",
      "[26216 | 52.56] loss=1.04 avg=1.40\n",
      "[26217 | 52.67] loss=0.96 avg=1.39\n",
      "[26218 | 52.80] loss=0.74 avg=1.38\n",
      "[26219 | 52.97] loss=3.84 avg=1.41\n",
      "[26220 | 53.09] loss=0.67 avg=1.40\n",
      "[26221 | 53.20] loss=2.09 avg=1.41\n",
      "[26222 | 53.31] loss=2.57 avg=1.42\n",
      "[26223 | 53.43] loss=1.86 avg=1.43\n",
      "[26224 | 53.55] loss=0.54 avg=1.42\n",
      "[26225 | 53.66] loss=0.49 avg=1.41\n",
      "[26226 | 53.77] loss=0.29 avg=1.40\n",
      "[26227 | 53.88] loss=0.85 avg=1.39\n",
      "[26228 | 54.02] loss=1.38 avg=1.39\n",
      "[26229 | 54.13] loss=0.35 avg=1.38\n",
      "[26230 | 54.26] loss=0.34 avg=1.37\n",
      "[26231 | 54.41] loss=1.00 avg=1.36\n",
      "[26232 | 54.53] loss=0.57 avg=1.36\n",
      "[26233 | 54.67] loss=0.73 avg=1.35\n",
      "[26234 | 54.79] loss=0.91 avg=1.34\n",
      "[26235 | 54.90] loss=1.71 avg=1.35\n",
      "[26236 | 55.05] loss=1.23 avg=1.35\n",
      "[26237 | 55.17] loss=0.42 avg=1.34\n",
      "[26238 | 55.29] loss=2.94 avg=1.35\n",
      "[26239 | 55.40] loss=0.92 avg=1.35\n",
      "[26240 | 55.50] loss=0.54 avg=1.34\n",
      "[26241 | 55.63] loss=0.83 avg=1.33\n",
      "[26242 | 55.76] loss=1.00 avg=1.33\n",
      "[26243 | 55.90] loss=3.98 avg=1.36\n",
      "[26244 | 56.02] loss=0.44 avg=1.35\n",
      "[26245 | 56.17] loss=0.82 avg=1.34\n",
      "[26246 | 56.28] loss=1.00 avg=1.34\n",
      "[26247 | 56.39] loss=0.85 avg=1.34\n",
      "[26248 | 56.51] loss=0.17 avg=1.32\n",
      "[26249 | 56.64] loss=1.71 avg=1.33\n",
      "[26250 | 56.77] loss=0.36 avg=1.32\n",
      "[26251 | 56.88] loss=0.60 avg=1.31\n",
      "[26252 | 57.01] loss=2.03 avg=1.32\n",
      "[26253 | 57.15] loss=1.61 avg=1.32\n",
      "[26254 | 57.26] loss=3.26 avg=1.34\n",
      "[26255 | 57.40] loss=5.79 avg=1.39\n",
      "[26256 | 57.51] loss=2.98 avg=1.40\n",
      "[26257 | 57.65] loss=0.34 avg=1.39\n",
      "[26258 | 57.76] loss=0.64 avg=1.39\n",
      "[26259 | 57.87] loss=0.64 avg=1.38\n",
      "[26260 | 57.98] loss=0.76 avg=1.37\n",
      "[26261 | 58.09] loss=0.77 avg=1.36\n",
      "[26262 | 58.21] loss=0.32 avg=1.35\n",
      "[26263 | 58.34] loss=2.75 avg=1.37\n",
      "[26264 | 58.46] loss=2.17 avg=1.38\n",
      "[26265 | 58.58] loss=0.36 avg=1.37\n",
      "[26266 | 58.69] loss=1.50 avg=1.37\n",
      "[26267 | 58.82] loss=2.19 avg=1.38\n",
      "[26268 | 58.93] loss=0.86 avg=1.37\n",
      "[26269 | 59.04] loss=0.18 avg=1.36\n",
      "[26270 | 59.18] loss=1.25 avg=1.36\n",
      "[26271 | 59.28] loss=2.19 avg=1.37\n",
      "[26272 | 59.41] loss=0.28 avg=1.35\n",
      "[26273 | 59.52] loss=1.45 avg=1.36\n",
      "[26274 | 59.64] loss=0.42 avg=1.35\n",
      "[26275 | 59.75] loss=3.65 avg=1.37\n",
      "[26276 | 59.86] loss=0.64 avg=1.36\n",
      "[26277 | 60.01] loss=1.15 avg=1.36\n",
      "[26278 | 60.13] loss=0.72 avg=1.35\n",
      "[26279 | 60.24] loss=1.04 avg=1.35\n",
      "[26280 | 60.37] loss=0.95 avg=1.35\n",
      "[26281 | 60.48] loss=0.23 avg=1.33\n",
      "[26282 | 60.61] loss=1.45 avg=1.34\n",
      "[26283 | 60.72] loss=1.41 avg=1.34\n",
      "[26284 | 60.83] loss=0.72 avg=1.33\n",
      "[26285 | 60.96] loss=3.52 avg=1.35\n",
      "[26286 | 61.08] loss=0.14 avg=1.34\n",
      "[26287 | 61.21] loss=1.29 avg=1.34\n",
      "[26288 | 61.33] loss=0.44 avg=1.33\n",
      "[26289 | 61.47] loss=0.16 avg=1.32\n",
      "[26290 | 61.59] loss=0.60 avg=1.31\n",
      "[26291 | 61.72] loss=1.84 avg=1.32\n",
      "[26292 | 61.84] loss=0.34 avg=1.31\n",
      "[26293 | 61.97] loss=1.59 avg=1.31\n",
      "[26294 | 62.08] loss=0.78 avg=1.30\n",
      "[26295 | 62.20] loss=0.64 avg=1.30\n",
      "[26296 | 62.33] loss=2.15 avg=1.30\n",
      "[26297 | 62.45] loss=0.97 avg=1.30\n",
      "[26298 | 62.58] loss=1.25 avg=1.30\n",
      "[26299 | 62.69] loss=2.74 avg=1.32\n",
      "Generating samples...\n",
      "Seed text : 석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요?\n",
      "[10000 10001]\n",
      "석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요? 환경 오염이 심해져서 지구의 환경 오염 ⁇ 가 일어나. ⁇ 가 없다. 하지만 ⁇  석탄과 석유와 같은 환경오염과 석유는\n",
      "석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요? 환경오염이 유지되기  ⁇  태양광 에너지가 고갈되어 자원이 사라지지않아질 것 같다. 따라서 지구가 심한다.\n",
      "석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요? 석탄과 석유기 힘의 양이 이산화탄소만어 지구 온난화가종 등의 지구 지구가 될 것 이다. 또한 지구는\n",
      "[26300 | 64.63] loss=0.85 avg=1.31\n",
      "[26301 | 64.78] loss=0.33 avg=1.30\n",
      "[26302 | 64.90] loss=0.43 avg=1.29\n",
      "[26303 | 65.02] loss=0.43 avg=1.28\n",
      "[26304 | 65.13] loss=1.76 avg=1.29\n",
      "[26305 | 65.24] loss=1.18 avg=1.29\n",
      "[26306 | 65.36] loss=0.58 avg=1.28\n",
      "[26307 | 65.50] loss=1.49 avg=1.28\n",
      "[26308 | 65.63] loss=0.38 avg=1.27\n",
      "[26309 | 65.74] loss=1.24 avg=1.27\n",
      "[26310 | 65.85] loss=1.34 avg=1.27\n",
      "[26311 | 65.98] loss=0.73 avg=1.27\n",
      "[26312 | 66.09] loss=1.29 avg=1.27\n",
      "[26313 | 66.23] loss=1.18 avg=1.27\n",
      "[26314 | 66.36] loss=1.10 avg=1.26\n",
      "[26315 | 66.51] loss=0.80 avg=1.26\n",
      "[26316 | 66.64] loss=0.23 avg=1.25\n",
      "[26317 | 66.75] loss=2.91 avg=1.27\n",
      "[26318 | 66.86] loss=2.01 avg=1.27\n",
      "[26319 | 66.99] loss=0.11 avg=1.26\n",
      "[26320 | 67.11] loss=0.61 avg=1.26\n",
      "[26321 | 67.29] loss=0.59 avg=1.25\n",
      "[26322 | 67.40] loss=0.26 avg=1.24\n",
      "[26323 | 67.53] loss=0.84 avg=1.23\n",
      "[26324 | 67.66] loss=1.39 avg=1.24\n",
      "[26325 | 67.78] loss=0.67 avg=1.23\n",
      "[26326 | 67.90] loss=1.32 avg=1.23\n",
      "[26327 | 68.03] loss=2.04 avg=1.24\n",
      "[26328 | 68.15] loss=0.63 avg=1.23\n",
      "[26329 | 68.29] loss=0.22 avg=1.22\n",
      "[26330 | 68.40] loss=0.80 avg=1.22\n",
      "[26331 | 68.52] loss=0.59 avg=1.21\n",
      "[26332 | 68.64] loss=0.12 avg=1.20\n",
      "[26333 | 68.77] loss=0.80 avg=1.20\n",
      "[26334 | 68.89] loss=0.76 avg=1.19\n",
      "[26335 | 69.00] loss=0.99 avg=1.19\n",
      "[26336 | 69.11] loss=0.63 avg=1.18\n",
      "[26337 | 69.22] loss=0.81 avg=1.18\n",
      "[26338 | 69.33] loss=2.89 avg=1.20\n",
      "[26339 | 69.45] loss=1.12 avg=1.20\n",
      "[26340 | 69.57] loss=3.86 avg=1.22\n",
      "[26341 | 69.68] loss=0.87 avg=1.22\n",
      "[26342 | 69.81] loss=0.48 avg=1.21\n",
      "[26343 | 69.95] loss=1.17 avg=1.21\n",
      "[26344 | 70.06] loss=1.75 avg=1.22\n",
      "[26345 | 70.18] loss=0.14 avg=1.21\n",
      "[26346 | 70.30] loss=0.64 avg=1.20\n",
      "[26347 | 70.42] loss=2.68 avg=1.22\n",
      "[26348 | 70.54] loss=1.04 avg=1.21\n",
      "[26349 | 70.65] loss=0.52 avg=1.21\n",
      "[26350 | 70.76] loss=0.90 avg=1.20\n",
      "[26351 | 70.87] loss=0.35 avg=1.20\n",
      "[26352 | 70.99] loss=0.53 avg=1.19\n",
      "[26353 | 71.11] loss=1.16 avg=1.19\n",
      "[26354 | 71.22] loss=2.11 avg=1.20\n",
      "[26355 | 71.35] loss=0.76 avg=1.19\n",
      "[26356 | 71.46] loss=1.23 avg=1.19\n",
      "[26357 | 71.59] loss=0.69 avg=1.19\n",
      "[26358 | 71.71] loss=0.34 avg=1.18\n",
      "[26359 | 71.85] loss=2.51 avg=1.19\n",
      "[26360 | 71.99] loss=0.97 avg=1.19\n",
      "[26361 | 72.10] loss=1.05 avg=1.19\n",
      "[26362 | 72.22] loss=0.71 avg=1.19\n",
      "[26363 | 72.36] loss=0.46 avg=1.18\n",
      "[26364 | 72.47] loss=0.23 avg=1.17\n",
      "[26365 | 72.58] loss=1.74 avg=1.17\n",
      "[26366 | 72.71] loss=0.73 avg=1.17\n",
      "[26367 | 72.84] loss=0.95 avg=1.17\n",
      "[26368 | 72.95] loss=1.61 avg=1.17\n",
      "[26369 | 73.10] loss=0.51 avg=1.16\n",
      "[26370 | 73.22] loss=1.24 avg=1.17\n",
      "[26371 | 73.34] loss=2.88 avg=1.18\n",
      "[26372 | 73.45] loss=1.64 avg=1.19\n",
      "[26373 | 73.58] loss=1.63 avg=1.19\n",
      "[26374 | 73.69] loss=0.82 avg=1.19\n",
      "[26375 | 73.81] loss=0.99 avg=1.19\n",
      "[26376 | 73.93] loss=0.64 avg=1.18\n",
      "[26377 | 74.04] loss=1.61 avg=1.19\n",
      "[26378 | 74.16] loss=0.37 avg=1.18\n",
      "[26379 | 74.27] loss=0.81 avg=1.17\n",
      "[26380 | 74.41] loss=0.25 avg=1.16\n",
      "[26381 | 74.53] loss=0.49 avg=1.16\n",
      "[26382 | 74.65] loss=0.73 avg=1.15\n",
      "[26383 | 74.76] loss=2.49 avg=1.17\n",
      "[26384 | 74.88] loss=0.27 avg=1.16\n",
      "[26385 | 74.99] loss=0.29 avg=1.15\n",
      "[26386 | 75.13] loss=2.37 avg=1.16\n",
      "[26387 | 75.26] loss=2.98 avg=1.18\n",
      "[26388 | 75.39] loss=1.80 avg=1.19\n",
      "[26389 | 75.52] loss=0.82 avg=1.18\n",
      "[26390 | 75.65] loss=1.55 avg=1.19\n",
      "[26391 | 75.78] loss=0.97 avg=1.18\n",
      "[26392 | 75.89] loss=0.45 avg=1.18\n",
      "[26393 | 76.00] loss=0.38 avg=1.17\n",
      "[26394 | 76.12] loss=1.93 avg=1.18\n",
      "[26395 | 76.23] loss=2.31 avg=1.19\n",
      "[26396 | 76.35] loss=0.65 avg=1.18\n",
      "[26397 | 76.48] loss=1.17 avg=1.18\n",
      "[26398 | 76.61] loss=2.17 avg=1.19\n",
      "[26399 | 76.75] loss=0.37 avg=1.18\n",
      "Generating samples...\n",
      "Seed text : 낮 시간 동안에도 별들이 하늘에 떠 있습니다. 하지만 우리가 낮 시간에 별을 볼 수 없는 까닭은 무엇 때문일까요?\n",
      "[]\n",
      "낮 시간 동안에도 별들이 하늘에 떠 있습니다. 하지만 우리가 낮 시간에 별을 볼 수 없는 까닭은 무엇 때문일까요? 보이지 없다. ⁇  태양 햇빛인 볼국 별을 볼 수 없다. ⁇  낮 시간 별의 빛이 낮 시간 없다. ⁇  태양빛 때문에 별\n",
      "낮 시간 동안에도 별들이 하늘에 떠 있습니다. 하지만 우리가 낮 시간에 별을 볼 수 없는 까닭은 무엇 때문일까요? 볼 수 없다. ⁇  빛이기 때문이다.......................\n",
      "낮 시간 동안에도 별들이 하늘에 떠 있습니다. 하지만 우리가 낮 시간에 별을 볼 수 없는 까닭은 무엇 때문일까요? 해빛에있는 별을 볼 수 없다.. 그러므로가 더 강한다....??? ⁇ 모르서....\n",
      "[26400 | 78.68] loss=0.59 avg=1.18\n",
      "[26401 | 78.80] loss=2.23 avg=1.19\n",
      "[26402 | 78.90] loss=0.61 avg=1.18\n",
      "[26403 | 79.04] loss=2.41 avg=1.19\n",
      "[26404 | 79.15] loss=1.32 avg=1.20\n",
      "[26405 | 79.27] loss=1.42 avg=1.20\n",
      "[26406 | 79.39] loss=0.52 avg=1.19\n",
      "[26407 | 79.51] loss=0.62 avg=1.19\n",
      "[26408 | 79.64] loss=0.62 avg=1.18\n",
      "[26409 | 79.78] loss=1.60 avg=1.18\n",
      "[26410 | 79.91] loss=0.32 avg=1.18\n",
      "[26411 | 80.02] loss=1.75 avg=1.18\n",
      "[26412 | 80.13] loss=1.72 avg=1.19\n",
      "[26413 | 80.24] loss=0.77 avg=1.18\n",
      "[26414 | 80.38] loss=0.86 avg=1.18\n",
      "[26415 | 80.49] loss=1.52 avg=1.18\n",
      "[26416 | 80.62] loss=0.30 avg=1.17\n",
      "[26417 | 80.74] loss=0.93 avg=1.17\n",
      "[26418 | 80.87] loss=0.19 avg=1.16\n",
      "[26419 | 81.01] loss=1.03 avg=1.16\n",
      "[26420 | 81.12] loss=0.32 avg=1.15\n",
      "[26421 | 81.25] loss=2.62 avg=1.17\n",
      "[26422 | 81.37] loss=0.82 avg=1.16\n",
      "[26423 | 81.49] loss=2.14 avg=1.17\n",
      "[26424 | 81.63] loss=2.30 avg=1.18\n",
      "[26425 | 81.74] loss=0.82 avg=1.18\n",
      "[26426 | 81.85] loss=0.50 avg=1.17\n",
      "[26427 | 81.98] loss=1.12 avg=1.17\n",
      "[26428 | 82.08] loss=1.90 avg=1.18\n",
      "[26429 | 82.20] loss=0.10 avg=1.17\n",
      "[26430 | 82.32] loss=0.58 avg=1.16\n",
      "[26431 | 82.43] loss=4.48 avg=1.20\n",
      "[26432 | 82.55] loss=1.12 avg=1.20\n",
      "[26433 | 82.70] loss=0.65 avg=1.19\n",
      "[26434 | 82.84] loss=3.00 avg=1.21\n",
      "[26435 | 82.97] loss=2.25 avg=1.22\n",
      "[26436 | 83.09] loss=0.49 avg=1.21\n",
      "[26437 | 83.21] loss=0.12 avg=1.20\n",
      "[26438 | 83.33] loss=1.06 avg=1.20\n",
      "[26439 | 83.44] loss=2.34 avg=1.21\n",
      "[26440 | 83.55] loss=2.13 avg=1.22\n",
      "[26441 | 83.66] loss=0.96 avg=1.22\n",
      "[26442 | 83.77] loss=1.60 avg=1.22\n",
      "[26443 | 83.88] loss=0.93 avg=1.22\n",
      "[26444 | 84.00] loss=0.91 avg=1.22\n",
      "[26445 | 84.11] loss=1.90 avg=1.22\n",
      "[26446 | 84.22] loss=1.17 avg=1.22\n",
      "[26447 | 84.33] loss=1.70 avg=1.23\n",
      "[26448 | 84.46] loss=0.33 avg=1.22\n",
      "[26449 | 84.57] loss=0.95 avg=1.22\n",
      "[26450 | 84.70] loss=0.63 avg=1.21\n",
      "[26451 | 84.81] loss=0.91 avg=1.21\n",
      "[26452 | 84.92] loss=0.28 avg=1.20\n",
      "[26453 | 85.03] loss=1.41 avg=1.20\n",
      "[26454 | 85.14] loss=1.49 avg=1.20\n",
      "[26455 | 85.26] loss=0.60 avg=1.20\n",
      "[26456 | 85.37] loss=0.44 avg=1.19\n",
      "[26457 | 85.48] loss=1.24 avg=1.19\n",
      "[26458 | 85.59] loss=3.13 avg=1.21\n",
      "[26459 | 85.73] loss=1.70 avg=1.21\n",
      "[26460 | 85.84] loss=1.04 avg=1.21\n",
      "[26461 | 85.95] loss=0.72 avg=1.21\n",
      "[26462 | 86.07] loss=1.73 avg=1.21\n",
      "[26463 | 86.18] loss=2.08 avg=1.22\n",
      "[26464 | 86.31] loss=0.50 avg=1.21\n",
      "[26465 | 86.43] loss=0.38 avg=1.20\n",
      "[26466 | 86.54] loss=1.26 avg=1.21\n",
      "[26467 | 86.65] loss=0.94 avg=1.20\n",
      "[26468 | 86.78] loss=0.24 avg=1.19\n",
      "[26469 | 86.91] loss=2.25 avg=1.20\n",
      "[26470 | 87.02] loss=1.17 avg=1.20\n",
      "[26471 | 87.17] loss=2.94 avg=1.22\n",
      "[26472 | 87.29] loss=2.89 avg=1.24\n",
      "[26473 | 87.41] loss=1.01 avg=1.24\n",
      "[26474 | 87.55] loss=0.15 avg=1.22\n",
      "[26475 | 87.66] loss=5.00 avg=1.26\n",
      "[26476 | 87.77] loss=1.93 avg=1.27\n",
      "[26477 | 87.90] loss=0.11 avg=1.26\n",
      "[26478 | 88.02] loss=0.40 avg=1.25\n",
      "[26479 | 88.13] loss=1.65 avg=1.25\n",
      "[26480 | 88.24] loss=1.06 avg=1.25\n",
      "[26481 | 88.37] loss=0.49 avg=1.24\n",
      "[26482 | 88.49] loss=0.27 avg=1.23\n",
      "[26483 | 88.62] loss=1.41 avg=1.24\n",
      "[26484 | 88.74] loss=0.27 avg=1.23\n",
      "[26485 | 88.85] loss=1.23 avg=1.23\n",
      "[26486 | 88.98] loss=1.69 avg=1.23\n",
      "[26487 | 89.10] loss=1.23 avg=1.23\n",
      "[26488 | 89.23] loss=0.59 avg=1.22\n",
      "[26489 | 89.34] loss=0.46 avg=1.22\n",
      "[26490 | 89.45] loss=2.69 avg=1.23\n",
      "[26491 | 89.56] loss=1.36 avg=1.23\n",
      "[26492 | 89.70] loss=1.20 avg=1.23\n",
      "[26493 | 89.81] loss=1.39 avg=1.23\n",
      "[26494 | 89.92] loss=1.28 avg=1.23\n",
      "[26495 | 90.04] loss=4.80 avg=1.27\n",
      "[26496 | 90.17] loss=0.22 avg=1.26\n",
      "[26497 | 90.28] loss=1.90 avg=1.27\n",
      "[26498 | 90.40] loss=0.84 avg=1.26\n",
      "[26499 | 90.52] loss=4.86 avg=1.30\n",
      "Saving checkpoint/transfer3/model-26500\n",
      "Generating samples...\n",
      "Seed text : 헌법에서 제시하고 있는 국민의 권리 중 홍길동씨의 예와 관련된 국민의 권리에는 어떤 것들이 있는지 아는 대로 제시해 주세요.\n",
      "[10000]\n",
      "헌법에서 제시하고 있는 국민의 권리 중 홍길동씨의 예와 관련된 국민의 권리에는 어떤 것들이 있는지 아는 대로 제시해 주세요. 법 사람은 자유권 ⁇  차별받지않고 있다. 차별받지 않을 한다.. ⁇  일분권로 가지고있아 권리가있는 평등권\n",
      "헌법에서 제시하고 있는 국민의 권리 중 홍길동씨의 예와 관련된 국민의 권리에는 어떤 것들이 있는지 아는 대로 제시해 주세요. 차별받지 않을 권리 ⁇  차별을 차별수있다. ...  ⁇ 모르 권리 ⁇ 소 행복이 차별 방법성등한 권리 등은\n",
      "헌법에서 제시하고 있는 국민의 권리 중 홍길동씨의 예와 관련된 국민의 권리에는 어떤 것들이 있는지 아는 대로 제시해 주세요. 장애 인간 일을 일할 권리. 그리고 ⁇  자유권리 ⁇  일권 등제별권 권리이다. 또한 국민의 권리 기능은 다양한 직 불편한\n",
      "[26500 | 95.27] loss=3.08 avg=1.32\n",
      "[26501 | 95.38] loss=1.67 avg=1.32\n",
      "[26502 | 95.51] loss=0.46 avg=1.31\n",
      "[26503 | 95.63] loss=2.95 avg=1.33\n",
      "[26504 | 95.75] loss=0.98 avg=1.32\n",
      "[26505 | 95.87] loss=0.41 avg=1.31\n",
      "[26506 | 95.99] loss=0.80 avg=1.31\n",
      "[26507 | 96.11] loss=1.49 avg=1.31\n",
      "[26508 | 96.27] loss=3.18 avg=1.33\n",
      "[26509 | 96.40] loss=2.55 avg=1.34\n",
      "[26510 | 96.51] loss=1.13 avg=1.34\n",
      "[26511 | 96.64] loss=0.76 avg=1.33\n",
      "[26512 | 96.77] loss=0.81 avg=1.33\n",
      "[26513 | 96.89] loss=0.85 avg=1.32\n",
      "[26514 | 97.02] loss=0.75 avg=1.32\n",
      "[26515 | 97.17] loss=0.33 avg=1.31\n",
      "[26516 | 97.29] loss=0.73 avg=1.30\n",
      "[26517 | 97.41] loss=0.76 avg=1.30\n",
      "[26518 | 97.52] loss=0.96 avg=1.29\n",
      "[26519 | 97.63] loss=1.88 avg=1.30\n",
      "[26520 | 97.77] loss=1.21 avg=1.30\n",
      "[26521 | 97.88] loss=1.61 avg=1.30\n",
      "[26522 | 98.04] loss=3.31 avg=1.32\n",
      "[26523 | 98.15] loss=1.73 avg=1.33\n",
      "[26524 | 98.26] loss=1.89 avg=1.33\n",
      "[26525 | 98.37] loss=2.41 avg=1.34\n",
      "[26526 | 98.51] loss=0.84 avg=1.34\n",
      "[26527 | 98.63] loss=2.32 avg=1.35\n",
      "[26528 | 98.76] loss=0.47 avg=1.34\n",
      "[26529 | 98.87] loss=1.38 avg=1.34\n",
      "[26530 | 98.99] loss=0.71 avg=1.33\n",
      "[26531 | 99.12] loss=2.05 avg=1.34\n",
      "[26532 | 99.25] loss=2.29 avg=1.35\n",
      "[26533 | 99.37] loss=2.28 avg=1.36\n",
      "[26534 | 99.52] loss=0.45 avg=1.35\n",
      "[26535 | 99.65] loss=0.60 avg=1.34\n",
      "[26536 | 99.78] loss=0.33 avg=1.33\n",
      "[26537 | 99.90] loss=0.36 avg=1.32\n",
      "[26538 | 100.01] loss=3.29 avg=1.34\n",
      "[26539 | 100.13] loss=0.10 avg=1.33\n",
      "[26540 | 100.28] loss=0.26 avg=1.32\n",
      "[26541 | 100.46] loss=1.71 avg=1.32\n",
      "[26542 | 100.59] loss=2.28 avg=1.33\n",
      "[26543 | 100.70] loss=0.05 avg=1.32\n",
      "[26544 | 100.81] loss=1.38 avg=1.32\n",
      "[26545 | 100.94] loss=0.77 avg=1.31\n",
      "[26546 | 101.05] loss=0.75 avg=1.31\n",
      "[26547 | 101.16] loss=0.94 avg=1.31\n",
      "[26548 | 101.28] loss=0.40 avg=1.30\n",
      "[26549 | 101.39] loss=1.80 avg=1.30\n",
      "[26550 | 101.50] loss=1.87 avg=1.31\n",
      "[26551 | 101.61] loss=1.54 avg=1.31\n",
      "[26552 | 101.73] loss=1.51 avg=1.31\n",
      "[26553 | 101.83] loss=0.10 avg=1.30\n",
      "[26554 | 101.96] loss=0.40 avg=1.29\n",
      "[26555 | 102.08] loss=0.93 avg=1.29\n",
      "[26556 | 102.19] loss=0.62 avg=1.28\n",
      "[26557 | 102.31] loss=0.42 avg=1.27\n",
      "[26558 | 102.45] loss=1.84 avg=1.28\n",
      "[26559 | 102.56] loss=1.45 avg=1.28\n",
      "[26560 | 102.67] loss=1.33 avg=1.28\n",
      "[26561 | 102.79] loss=1.11 avg=1.28\n",
      "[26562 | 102.91] loss=0.84 avg=1.27\n",
      "[26563 | 103.04] loss=2.99 avg=1.29\n",
      "[26564 | 103.15] loss=0.64 avg=1.28\n",
      "[26565 | 103.27] loss=0.82 avg=1.28\n",
      "[26566 | 103.38] loss=0.76 avg=1.27\n",
      "[26567 | 103.50] loss=2.32 avg=1.28\n",
      "[26568 | 103.61] loss=1.54 avg=1.29\n",
      "[26569 | 103.72] loss=2.53 avg=1.30\n",
      "[26570 | 103.83] loss=1.08 avg=1.30\n",
      "[26571 | 103.94] loss=1.40 avg=1.30\n",
      "[26572 | 104.05] loss=1.15 avg=1.30\n",
      "[26573 | 104.18] loss=0.46 avg=1.29\n",
      "[26574 | 104.32] loss=3.76 avg=1.31\n",
      "[26575 | 104.43] loss=1.24 avg=1.31\n",
      "[26576 | 104.55] loss=2.39 avg=1.32\n",
      "[26577 | 104.67] loss=4.01 avg=1.35\n",
      "[26578 | 104.80] loss=0.18 avg=1.34\n",
      "[26579 | 104.91] loss=4.14 avg=1.37\n",
      "[26580 | 105.05] loss=0.31 avg=1.36\n",
      "[26581 | 105.16] loss=1.59 avg=1.36\n",
      "[26582 | 105.27] loss=0.99 avg=1.35\n",
      "[26583 | 105.40] loss=2.92 avg=1.37\n",
      "[26584 | 105.52] loss=2.65 avg=1.38\n",
      "[26585 | 105.65] loss=1.45 avg=1.38\n",
      "[26586 | 105.76] loss=1.07 avg=1.38\n",
      "[26587 | 105.88] loss=1.22 avg=1.38\n",
      "[26588 | 106.00] loss=0.53 avg=1.37\n",
      "[26589 | 106.13] loss=0.69 avg=1.36\n",
      "[26590 | 106.24] loss=1.38 avg=1.36\n",
      "[26591 | 106.35] loss=2.41 avg=1.37\n",
      "[26592 | 106.46] loss=0.27 avg=1.36\n",
      "[26593 | 106.58] loss=0.46 avg=1.35\n",
      "[26594 | 106.69] loss=2.90 avg=1.37\n",
      "[26595 | 106.82] loss=0.19 avg=1.36\n",
      "[26596 | 106.94] loss=0.51 avg=1.35\n",
      "[26597 | 107.05] loss=1.29 avg=1.35\n",
      "[26598 | 107.18] loss=0.72 avg=1.34\n",
      "[26599 | 107.29] loss=0.78 avg=1.34\n",
      "Generating samples...\n",
      "Seed text : 불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요?\n",
      "[10000]\n",
      "불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요? 물을 뿌린 후 불이 꺼진것이 물을 뿌리는 수 있는 물로 되어 열은 만나면 꺼진다. 그 물은 물을 뿌린 후\n",
      "불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요? 물의 산소를 차단되기 때문이다. 그러나 연소 ⁇  공기 중 물로 변한다............ 땅\n",
      "불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요? 물을 뿌린 후 불이 꺼 진공기를 계속 연소로 산소가 차단해서 물을 뿌린 후 불이 꺼진다. 그리고 물을 뿌린 후 불이\n",
      "[26600 | 109.23] loss=0.53 avg=1.33\n",
      "[26601 | 109.34] loss=1.41 avg=1.33\n",
      "[26602 | 109.47] loss=0.83 avg=1.32\n",
      "[26603 | 109.58] loss=1.54 avg=1.33\n",
      "[26604 | 109.70] loss=0.50 avg=1.32\n",
      "[26605 | 109.82] loss=1.73 avg=1.32\n",
      "[26606 | 109.95] loss=1.91 avg=1.33\n",
      "[26607 | 110.07] loss=0.78 avg=1.32\n",
      "[26608 | 110.18] loss=1.96 avg=1.33\n",
      "[26609 | 110.29] loss=1.76 avg=1.33\n",
      "[26610 | 110.40] loss=1.21 avg=1.33\n",
      "[26611 | 110.51] loss=0.81 avg=1.33\n",
      "[26612 | 110.63] loss=0.69 avg=1.32\n",
      "[26613 | 110.76] loss=0.38 avg=1.31\n",
      "[26614 | 110.88] loss=0.48 avg=1.30\n",
      "[26615 | 110.99] loss=1.67 avg=1.31\n",
      "[26616 | 111.12] loss=0.42 avg=1.30\n",
      "[26617 | 111.27] loss=0.78 avg=1.29\n",
      "[26618 | 111.40] loss=1.47 avg=1.29\n",
      "[26619 | 111.53] loss=1.79 avg=1.30\n",
      "[26620 | 111.66] loss=2.42 avg=1.31\n",
      "[26621 | 111.78] loss=0.87 avg=1.31\n",
      "[26622 | 111.91] loss=0.92 avg=1.30\n",
      "[26623 | 112.04] loss=2.40 avg=1.31\n",
      "[26624 | 112.15] loss=2.77 avg=1.33\n",
      "[26625 | 112.27] loss=0.21 avg=1.32\n",
      "[26626 | 112.40] loss=0.41 avg=1.31\n",
      "[26627 | 112.52] loss=2.25 avg=1.32\n",
      "[26628 | 112.65] loss=1.52 avg=1.32\n",
      "[26629 | 112.76] loss=0.71 avg=1.31\n",
      "[26630 | 112.89] loss=4.03 avg=1.34\n",
      "[26631 | 113.00] loss=2.25 avg=1.35\n",
      "[26632 | 113.11] loss=1.20 avg=1.35\n",
      "[26633 | 113.23] loss=2.39 avg=1.36\n",
      "[26634 | 113.35] loss=1.28 avg=1.36\n",
      "[26635 | 113.47] loss=2.73 avg=1.37\n",
      "[26636 | 113.58] loss=0.33 avg=1.36\n",
      "[26637 | 113.71] loss=0.39 avg=1.35\n",
      "[26638 | 113.82] loss=1.23 avg=1.35\n",
      "[26639 | 113.93] loss=0.68 avg=1.34\n",
      "[26640 | 114.05] loss=0.90 avg=1.34\n",
      "[26641 | 114.18] loss=0.44 avg=1.33\n",
      "[26642 | 114.29] loss=1.37 avg=1.33\n",
      "[26643 | 114.41] loss=0.72 avg=1.32\n",
      "[26644 | 114.58] loss=5.68 avg=1.37\n",
      "[26645 | 114.69] loss=0.57 avg=1.36\n",
      "[26646 | 114.80] loss=1.68 avg=1.36\n",
      "[26647 | 114.92] loss=0.58 avg=1.36\n",
      "[26648 | 115.04] loss=0.36 avg=1.35\n",
      "[26649 | 115.15] loss=0.75 avg=1.34\n",
      "[26650 | 115.26] loss=0.54 avg=1.33\n",
      "[26651 | 115.38] loss=0.62 avg=1.32\n",
      "[26652 | 115.50] loss=1.56 avg=1.33\n",
      "[26653 | 115.62] loss=0.21 avg=1.32\n",
      "[26654 | 115.73] loss=0.93 avg=1.31\n",
      "[26655 | 115.84] loss=0.62 avg=1.30\n",
      "[26656 | 115.97] loss=0.56 avg=1.30\n",
      "[26657 | 116.10] loss=1.38 avg=1.30\n",
      "[26658 | 116.22] loss=1.48 avg=1.30\n",
      "[26659 | 116.35] loss=0.39 avg=1.29\n",
      "[26660 | 116.46] loss=1.16 avg=1.29\n",
      "[26661 | 116.57] loss=2.56 avg=1.30\n",
      "[26662 | 116.69] loss=1.22 avg=1.30\n",
      "[26663 | 116.82] loss=0.24 avg=1.29\n",
      "[26664 | 116.94] loss=0.58 avg=1.28\n",
      "[26665 | 117.07] loss=0.53 avg=1.28\n",
      "[26666 | 117.18] loss=0.95 avg=1.27\n",
      "[26667 | 117.30] loss=0.12 avg=1.26\n",
      "[26668 | 117.42] loss=2.22 avg=1.27\n",
      "[26669 | 117.53] loss=3.39 avg=1.29\n",
      "[26670 | 117.66] loss=1.09 avg=1.29\n",
      "[26671 | 117.79] loss=2.43 avg=1.30\n",
      "[26672 | 117.91] loss=0.81 avg=1.30\n",
      "[26673 | 118.02] loss=1.50 avg=1.30\n",
      "[26674 | 118.13] loss=0.46 avg=1.29\n",
      "[26675 | 118.27] loss=1.74 avg=1.29\n",
      "[26676 | 118.38] loss=0.92 avg=1.29\n",
      "[26677 | 118.49] loss=1.56 avg=1.29\n",
      "[26678 | 118.61] loss=0.57 avg=1.29\n",
      "[26679 | 118.73] loss=0.45 avg=1.28\n",
      "[26680 | 118.84] loss=1.66 avg=1.28\n",
      "[26681 | 118.95] loss=1.66 avg=1.29\n",
      "[26682 | 119.08] loss=2.54 avg=1.30\n",
      "[26683 | 119.21] loss=1.05 avg=1.30\n",
      "[26684 | 119.33] loss=1.63 avg=1.30\n",
      "[26685 | 119.45] loss=0.88 avg=1.29\n",
      "[26686 | 119.56] loss=1.22 avg=1.29\n",
      "[26687 | 119.68] loss=0.61 avg=1.29\n",
      "[26688 | 119.80] loss=1.55 avg=1.29\n",
      "[26689 | 119.92] loss=1.21 avg=1.29\n",
      "[26690 | 120.04] loss=1.43 avg=1.29\n",
      "[26691 | 120.16] loss=2.03 avg=1.30\n",
      "[26692 | 120.31] loss=0.58 avg=1.29\n",
      "[26693 | 120.42] loss=0.77 avg=1.29\n",
      "[26694 | 120.55] loss=0.51 avg=1.28\n",
      "[26695 | 120.68] loss=0.12 avg=1.27\n",
      "[26696 | 120.83] loss=0.14 avg=1.26\n",
      "[26697 | 120.95] loss=0.81 avg=1.25\n",
      "[26698 | 121.08] loss=0.28 avg=1.24\n",
      "[26699 | 121.19] loss=0.76 avg=1.24\n",
      "Generating samples...\n",
      "Seed text : 실험하면서 관찰하고 측정한 자료를 모으고 난 뒤 그 다음 순서로 하는 탐구 과정을 설명해 볼까요?\n",
      "[10000 10001]\n",
      "실험하면서 관찰하고 측정한 자료를 모으고 난 뒤 그 다음 순서로 하는 탐구 과정을 설명해 볼까요? 자료를 조사하고 결론을 도출하고 가설을 할 수 없는 이상이 결론을 도출한다. 그리고 가설을 다시 실험하여 세과 정하고 ⁇  그\n",
      "실험하면서 관찰하고 측정한 자료를 모으고 난 뒤 그 다음 순서로 하는 탐구 과정을 설명해 볼까요? 그 자료를 자신이 자료에서 현상에 대해 그 자료를 관찰하고 결론을 도출하고 그 다음에는 대해 ⁇  그 현상에 도출하고 이화하기위\n",
      "실험하면서 관찰하고 측정한 자료를 모으고 난 뒤 그 다음 순서로 하는 탐구 과정을 설명해 볼까요? 관찰하고 자료에 대해 그 현 정하고 실험을한다.. 그리고 그리고 그 현리를 대해 새로운화하고 ⁇  이 가설을  ⁇   ⁇ \n",
      "[26700 | 123.20] loss=1.02 avg=1.23\n",
      "[26701 | 123.32] loss=0.74 avg=1.23\n",
      "[26702 | 123.44] loss=0.70 avg=1.22\n",
      "[26703 | 123.58] loss=2.26 avg=1.23\n",
      "[26704 | 123.71] loss=0.94 avg=1.23\n",
      "[26705 | 123.84] loss=2.79 avg=1.25\n",
      "[26706 | 123.95] loss=0.19 avg=1.24\n",
      "[26707 | 124.08] loss=0.82 avg=1.23\n",
      "[26708 | 124.21] loss=0.39 avg=1.22\n",
      "[26709 | 124.35] loss=0.91 avg=1.22\n",
      "[26710 | 124.47] loss=2.00 avg=1.23\n",
      "[26711 | 124.59] loss=1.02 avg=1.23\n",
      "[26712 | 124.71] loss=0.83 avg=1.22\n",
      "[26713 | 124.82] loss=1.86 avg=1.23\n",
      "[26714 | 124.96] loss=2.11 avg=1.24\n",
      "[26715 | 125.09] loss=1.19 avg=1.24\n",
      "[26716 | 125.20] loss=0.43 avg=1.23\n",
      "[26717 | 125.31] loss=1.11 avg=1.23\n",
      "[26718 | 125.45] loss=1.16 avg=1.23\n",
      "[26719 | 125.58] loss=0.26 avg=1.22\n",
      "[26720 | 125.72] loss=0.79 avg=1.21\n",
      "[26721 | 125.84] loss=0.97 avg=1.21\n",
      "[26722 | 125.98] loss=2.72 avg=1.23\n",
      "[26723 | 126.10] loss=1.88 avg=1.23\n",
      "[26724 | 126.21] loss=1.33 avg=1.23\n",
      "[26725 | 126.32] loss=0.73 avg=1.23\n",
      "[26726 | 126.45] loss=0.46 avg=1.22\n",
      "[26727 | 126.57] loss=0.93 avg=1.22\n",
      "[26728 | 126.71] loss=1.46 avg=1.22\n",
      "[26729 | 126.82] loss=2.68 avg=1.23\n",
      "[26730 | 126.94] loss=0.54 avg=1.23\n",
      "[26731 | 127.05] loss=3.82 avg=1.25\n",
      "[26732 | 127.18] loss=3.18 avg=1.27\n",
      "[26733 | 127.31] loss=0.82 avg=1.27\n",
      "[26734 | 127.42] loss=2.64 avg=1.28\n",
      "[26735 | 127.53] loss=0.62 avg=1.28\n",
      "[26736 | 127.66] loss=0.99 avg=1.27\n",
      "[26737 | 127.78] loss=0.28 avg=1.26\n",
      "[26738 | 127.89] loss=2.52 avg=1.28\n",
      "[26739 | 128.00] loss=0.59 avg=1.27\n",
      "[26740 | 128.11] loss=0.67 avg=1.26\n",
      "[26741 | 128.23] loss=1.32 avg=1.26\n",
      "[26742 | 128.36] loss=0.76 avg=1.26\n",
      "[26743 | 128.47] loss=0.53 avg=1.25\n",
      "[26744 | 128.61] loss=0.29 avg=1.24\n",
      "[26745 | 128.74] loss=0.08 avg=1.23\n",
      "[26746 | 128.86] loss=0.37 avg=1.22\n",
      "[26747 | 129.00] loss=0.92 avg=1.22\n",
      "[26748 | 129.13] loss=1.72 avg=1.22\n",
      "[26749 | 129.25] loss=0.08 avg=1.21\n",
      "[26750 | 129.37] loss=1.50 avg=1.21\n",
      "[26751 | 129.50] loss=0.90 avg=1.21\n",
      "[26752 | 129.62] loss=0.51 avg=1.20\n",
      "[26753 | 129.73] loss=1.33 avg=1.21\n",
      "[26754 | 129.85] loss=0.85 avg=1.20\n",
      "[26755 | 129.96] loss=1.05 avg=1.20\n",
      "[26756 | 130.07] loss=0.97 avg=1.20\n",
      "[26757 | 130.19] loss=0.85 avg=1.19\n",
      "[26758 | 130.31] loss=0.12 avg=1.18\n",
      "[26759 | 130.48] loss=3.09 avg=1.20\n",
      "[26760 | 130.60] loss=2.03 avg=1.21\n",
      "[26761 | 130.72] loss=1.81 avg=1.22\n",
      "[26762 | 130.85] loss=0.46 avg=1.21\n",
      "[26763 | 130.97] loss=2.43 avg=1.22\n",
      "[26764 | 131.09] loss=1.58 avg=1.23\n",
      "[26765 | 131.23] loss=0.58 avg=1.22\n",
      "[26766 | 131.37] loss=1.89 avg=1.23\n",
      "[26767 | 131.48] loss=1.50 avg=1.23\n",
      "[26768 | 131.62] loss=2.35 avg=1.24\n",
      "[26769 | 131.76] loss=1.46 avg=1.24\n",
      "[26770 | 131.89] loss=0.50 avg=1.23\n",
      "[26771 | 132.02] loss=0.56 avg=1.23\n",
      "[26772 | 132.13] loss=1.98 avg=1.23\n",
      "[26773 | 132.27] loss=0.42 avg=1.23\n",
      "[26774 | 132.40] loss=0.29 avg=1.22\n",
      "[26775 | 132.52] loss=1.25 avg=1.22\n",
      "[26776 | 132.67] loss=1.72 avg=1.22\n",
      "[26777 | 132.79] loss=0.63 avg=1.22\n",
      "[26778 | 132.98] loss=0.48 avg=1.21\n",
      "[26779 | 133.11] loss=0.42 avg=1.20\n",
      "[26780 | 133.22] loss=0.71 avg=1.20\n",
      "[26781 | 133.33] loss=0.73 avg=1.19\n",
      "[26782 | 133.46] loss=2.44 avg=1.20\n",
      "[26783 | 133.58] loss=1.61 avg=1.21\n",
      "[26784 | 133.70] loss=0.48 avg=1.20\n",
      "[26785 | 133.82] loss=1.79 avg=1.21\n",
      "[26786 | 133.95] loss=1.03 avg=1.21\n",
      "[26787 | 134.06] loss=3.86 avg=1.23\n",
      "[26788 | 134.18] loss=0.95 avg=1.23\n",
      "[26789 | 134.32] loss=2.14 avg=1.24\n",
      "[26790 | 134.42] loss=1.32 avg=1.24\n",
      "[26791 | 134.53] loss=1.65 avg=1.24\n",
      "[26792 | 134.65] loss=0.43 avg=1.24\n",
      "[26793 | 134.76] loss=0.95 avg=1.23\n",
      "[26794 | 134.87] loss=1.33 avg=1.23\n",
      "[26795 | 135.00] loss=1.97 avg=1.24\n",
      "[26796 | 135.12] loss=1.36 avg=1.24\n",
      "[26797 | 135.25] loss=0.60 avg=1.24\n",
      "[26798 | 135.35] loss=1.51 avg=1.24\n",
      "[26799 | 135.47] loss=1.23 avg=1.24\n",
      "Generating samples...\n",
      "Seed text : 보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다.\n",
      "[10001]\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 단열에있는 재로 빠져나가지 못하고 전도가 일정하게 해주기위해 보온병은 전도와 온도가 높인다. 열전\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 열을 빠져나가지않게 할 때 보온병의 전도가 유지하고 열이 식지않는 것을 막아주기때문이다. 열\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 전도가 잘해서 열이 빠져나간다.5 이동을 가져가있는다. ⁇  전도가 높기 때문에 따뜻하게 유지하고 그 벽 전달되어\n",
      "[26800 | 137.42] loss=1.81 avg=1.24\n",
      "[26801 | 137.56] loss=5.28 avg=1.28\n",
      "[26802 | 137.67] loss=1.40 avg=1.29\n",
      "[26803 | 137.79] loss=0.52 avg=1.28\n",
      "[26804 | 137.92] loss=1.41 avg=1.28\n",
      "[26805 | 138.03] loss=1.21 avg=1.28\n",
      "[26806 | 138.15] loss=0.42 avg=1.27\n",
      "[26807 | 138.27] loss=0.84 avg=1.27\n",
      "[26808 | 138.42] loss=3.74 avg=1.29\n",
      "[26809 | 138.54] loss=2.89 avg=1.31\n",
      "[26810 | 138.67] loss=0.63 avg=1.30\n",
      "[26811 | 138.78] loss=0.29 avg=1.29\n",
      "[26812 | 138.89] loss=2.64 avg=1.30\n",
      "[26813 | 139.03] loss=3.54 avg=1.32\n",
      "[26814 | 139.15] loss=0.57 avg=1.32\n",
      "[26815 | 139.27] loss=0.98 avg=1.31\n",
      "[26816 | 139.38] loss=1.26 avg=1.31\n",
      "[26817 | 139.50] loss=2.46 avg=1.32\n",
      "[26818 | 139.63] loss=0.37 avg=1.32\n",
      "[26819 | 139.77] loss=0.50 avg=1.31\n",
      "[26820 | 139.88] loss=1.00 avg=1.30\n",
      "[26821 | 139.99] loss=0.90 avg=1.30\n",
      "[26822 | 140.09] loss=1.12 avg=1.30\n",
      "[26823 | 140.20] loss=1.76 avg=1.30\n",
      "[26824 | 140.31] loss=1.01 avg=1.30\n",
      "[26825 | 140.44] loss=3.13 avg=1.32\n",
      "[26826 | 140.56] loss=1.31 avg=1.32\n",
      "[26827 | 140.67] loss=0.62 avg=1.31\n",
      "[26828 | 140.78] loss=1.93 avg=1.32\n",
      "[26829 | 140.90] loss=1.02 avg=1.31\n",
      "[26830 | 141.02] loss=2.37 avg=1.33\n",
      "[26831 | 141.13] loss=0.91 avg=1.32\n",
      "[26832 | 141.24] loss=1.14 avg=1.32\n",
      "[26833 | 141.35] loss=0.30 avg=1.31\n",
      "[26834 | 141.51] loss=1.51 avg=1.31\n",
      "[26835 | 141.64] loss=0.75 avg=1.31\n",
      "[26836 | 141.75] loss=1.89 avg=1.31\n",
      "[26837 | 141.90] loss=0.09 avg=1.30\n",
      "[26838 | 142.02] loss=0.31 avg=1.29\n",
      "[26839 | 142.13] loss=0.47 avg=1.28\n",
      "[26840 | 142.24] loss=0.48 avg=1.27\n",
      "[26841 | 142.35] loss=1.21 avg=1.27\n",
      "[26842 | 142.48] loss=1.32 avg=1.27\n",
      "[26843 | 142.60] loss=0.34 avg=1.26\n",
      "[26844 | 142.71] loss=0.25 avg=1.25\n",
      "[26845 | 142.82] loss=1.07 avg=1.25\n",
      "[26846 | 142.94] loss=0.92 avg=1.25\n",
      "[26847 | 143.06] loss=2.79 avg=1.26\n",
      "[26848 | 143.16] loss=0.86 avg=1.26\n",
      "[26849 | 143.29] loss=1.78 avg=1.27\n",
      "[26850 | 143.41] loss=3.26 avg=1.28\n",
      "[26851 | 143.55] loss=0.66 avg=1.28\n",
      "[26852 | 143.68] loss=0.24 avg=1.27\n",
      "[26853 | 143.79] loss=2.66 avg=1.28\n",
      "[26854 | 143.92] loss=1.16 avg=1.28\n",
      "[26855 | 144.03] loss=1.78 avg=1.29\n",
      "[26856 | 144.13] loss=1.59 avg=1.29\n",
      "[26857 | 144.24] loss=0.71 avg=1.28\n",
      "[26858 | 144.36] loss=1.71 avg=1.29\n",
      "[26859 | 144.49] loss=0.86 avg=1.28\n",
      "[26860 | 144.60] loss=0.83 avg=1.28\n",
      "[26861 | 144.71] loss=0.81 avg=1.27\n",
      "[26862 | 144.83] loss=0.23 avg=1.26\n",
      "[26863 | 144.94] loss=1.11 avg=1.26\n",
      "[26864 | 145.05] loss=2.81 avg=1.28\n",
      "[26865 | 145.16] loss=0.75 avg=1.27\n",
      "[26866 | 145.28] loss=0.88 avg=1.27\n",
      "[26867 | 145.40] loss=1.46 avg=1.27\n",
      "[26868 | 145.51] loss=0.53 avg=1.26\n",
      "[26869 | 145.63] loss=0.87 avg=1.26\n",
      "[26870 | 145.74] loss=0.68 avg=1.25\n",
      "[26871 | 145.86] loss=0.38 avg=1.24\n",
      "[26872 | 145.96] loss=0.73 avg=1.24\n",
      "[26873 | 146.07] loss=0.20 avg=1.23\n",
      "[26874 | 146.20] loss=0.15 avg=1.22\n",
      "[26875 | 146.31] loss=2.14 avg=1.23\n",
      "[26876 | 146.42] loss=1.76 avg=1.23\n",
      "[26877 | 146.52] loss=1.25 avg=1.23\n",
      "[26878 | 146.64] loss=0.94 avg=1.23\n",
      "[26879 | 146.77] loss=0.78 avg=1.23\n",
      "[26880 | 146.89] loss=1.03 avg=1.22\n",
      "[26881 | 147.02] loss=0.62 avg=1.22\n",
      "[26882 | 147.14] loss=0.66 avg=1.21\n",
      "[26883 | 147.26] loss=1.04 avg=1.21\n",
      "[26884 | 147.38] loss=1.27 avg=1.21\n",
      "[26885 | 147.49] loss=0.85 avg=1.21\n",
      "[26886 | 147.62] loss=0.27 avg=1.20\n",
      "[26887 | 147.74] loss=0.20 avg=1.19\n",
      "[26888 | 147.86] loss=0.46 avg=1.18\n",
      "[26889 | 147.97] loss=2.79 avg=1.20\n",
      "[26890 | 148.10] loss=2.65 avg=1.21\n",
      "[26891 | 148.21] loss=2.19 avg=1.22\n",
      "[26892 | 148.33] loss=0.35 avg=1.21\n",
      "[26893 | 148.44] loss=0.79 avg=1.21\n",
      "[26894 | 148.55] loss=1.25 avg=1.21\n",
      "[26895 | 148.65] loss=1.34 avg=1.21\n",
      "[26896 | 148.76] loss=1.06 avg=1.21\n",
      "[26897 | 148.88] loss=0.73 avg=1.20\n",
      "[26898 | 149.02] loss=1.59 avg=1.21\n",
      "[26899 | 149.14] loss=0.13 avg=1.20\n",
      "Generating samples...\n",
      "Seed text : 에어컨을 전력량 이상으로 사용하여 전류가 과하게 흘렀고 에어컨이 꺼졌습니다. 이 때 ⁇  같은 콘센트에 연결되어 있던 TV ⁇  냉장고 등의 다른 전자제품은 전원이 꺼지지 않았습니다. 에어컨이 꺼졌는데도 같은 코드에 연결된 다른 전자제품들이 꺼지지 않은 이유를 전기의 작용과 전자제품의 연결 방법에 관한 지식을 활용하여 자세하게 설명해 봅시다.\n",
      "[10000]\n",
      "에어컨을 전력량 이상으로 사용하여 전류가 과하게 흘렀고 에어컨이 꺼졌습니다. 이 때 ⁇  같은 콘센트에 연결되어 있던 TV ⁇  냉장고 등의 다른 전자제품은 전원이 꺼지지 않았습니다. 에어컨이 꺼졌는데도 같은 코드에 연결된 다른 전자제품들이 꺼지지 않은 이유를 전기의 작용과 전자제품의 연결 방법에 관한 지식을 활용하여 자세하게 설명해 봅시다. 전자제품들이 병렬연결해 봅시다. 병렬연결되어 병렬연결되어있기 때문이다. ⁇ 문이다. 따라서 다른\n",
      "에어컨을 전력량 이상으로 사용하여 전류가 과하게 흘렀고 에어컨이 꺼졌습니다. 이 때 ⁇  같은 콘센트에 연결되어 있던 TV ⁇  냉장고 등의 다른 전자제품은 전원이 꺼지지 않았습니다. 에어컨이 꺼졌는데도 같은 코드에 연결된 다른 전자제품들이 꺼지지 않은 이유를 전기의 작용과 전자제품의 연결 방법에 관한 지식을 활용하여 자세하게 설명해 봅시다. 병렬 ⁇ 문에 병렬연결이 에어컨의 전기의 직렬연결되있어서 회 에어컨이 꺼졌기때문에 다른\n",
      "에어컨을 전력량 이상으로 사용하여 전류가 과하게 흘렀고 에어컨이 꺼졌습니다. 이 때 ⁇  같은 콘센트에 연결되어 있던 TV ⁇  냉장고 등의 다른 전자제품은 전원이 꺼지지 않았습니다. 에어컨이 꺼졌는데도 같은 코드에 연결된 다른 전자제품들이 꺼지지 않은 이유를 전기의 작용과 전자제품의 연결 방법에 관한 지식을 활용하여 자세하게 설명해 봅시다. 전기를만 꺼져서 다른 전면서 한 전자제품으로 전류기를 병렬연결하게 흐른것이 다른 전자제품은 병렬연\n",
      "[26900 | 151.16] loss=3.60 avg=1.22\n",
      "[26901 | 151.29] loss=0.85 avg=1.22\n",
      "[26902 | 151.40] loss=2.02 avg=1.23\n",
      "[26903 | 151.52] loss=0.73 avg=1.22\n",
      "[26904 | 151.63] loss=3.02 avg=1.24\n",
      "[26905 | 151.74] loss=0.94 avg=1.24\n",
      "[26906 | 151.88] loss=1.85 avg=1.24\n",
      "[26907 | 151.99] loss=3.15 avg=1.26\n",
      "[26908 | 152.10] loss=1.91 avg=1.27\n",
      "[26909 | 152.21] loss=1.30 avg=1.27\n",
      "[26910 | 152.32] loss=0.94 avg=1.26\n",
      "[26911 | 152.44] loss=2.34 avg=1.27\n",
      "[26912 | 152.60] loss=3.10 avg=1.29\n",
      "[26913 | 152.72] loss=3.41 avg=1.31\n",
      "[26914 | 152.82] loss=1.31 avg=1.31\n",
      "[26915 | 152.95] loss=1.68 avg=1.32\n",
      "[26916 | 153.05] loss=1.57 avg=1.32\n",
      "[26917 | 153.18] loss=1.00 avg=1.32\n",
      "[26918 | 153.28] loss=0.68 avg=1.31\n",
      "[26919 | 153.45] loss=5.44 avg=1.35\n",
      "[26920 | 153.57] loss=0.70 avg=1.35\n",
      "[26921 | 153.70] loss=0.94 avg=1.34\n",
      "[26922 | 153.81] loss=1.83 avg=1.35\n",
      "[26923 | 153.94] loss=1.28 avg=1.35\n",
      "[26924 | 154.07] loss=0.94 avg=1.34\n",
      "[26925 | 154.19] loss=0.41 avg=1.33\n",
      "[26926 | 154.31] loss=0.89 avg=1.33\n",
      "[26927 | 154.42] loss=1.55 avg=1.33\n",
      "[26928 | 154.54] loss=0.46 avg=1.32\n",
      "[26929 | 154.65] loss=1.51 avg=1.32\n",
      "[26930 | 154.75] loss=0.85 avg=1.32\n",
      "[26931 | 154.86] loss=0.97 avg=1.32\n",
      "[26932 | 154.99] loss=0.88 avg=1.31\n",
      "[26933 | 155.11] loss=0.38 avg=1.30\n",
      "[26934 | 155.21] loss=2.91 avg=1.32\n",
      "[26935 | 155.34] loss=0.59 avg=1.31\n",
      "[26936 | 155.46] loss=0.70 avg=1.30\n",
      "[26937 | 155.58] loss=0.56 avg=1.30\n",
      "[26938 | 155.69] loss=1.15 avg=1.30\n",
      "[26939 | 155.80] loss=0.69 avg=1.29\n",
      "[26940 | 155.93] loss=5.37 avg=1.33\n",
      "[26941 | 156.05] loss=1.86 avg=1.34\n",
      "[26942 | 156.18] loss=0.52 avg=1.33\n",
      "[26943 | 156.29] loss=0.67 avg=1.32\n",
      "[26944 | 156.40] loss=0.63 avg=1.31\n",
      "[26945 | 156.58] loss=4.76 avg=1.35\n",
      "[26946 | 156.70] loss=0.10 avg=1.34\n",
      "[26947 | 156.85] loss=3.33 avg=1.36\n",
      "[26948 | 156.97] loss=0.38 avg=1.35\n",
      "[26949 | 157.07] loss=1.62 avg=1.35\n",
      "[26950 | 157.20] loss=1.07 avg=1.35\n",
      "[26951 | 157.32] loss=0.38 avg=1.34\n",
      "[26952 | 157.43] loss=1.24 avg=1.34\n",
      "[26953 | 157.54] loss=0.63 avg=1.33\n",
      "[26954 | 157.66] loss=1.20 avg=1.33\n",
      "[26955 | 157.78] loss=1.51 avg=1.33\n",
      "[26956 | 157.92] loss=0.87 avg=1.32\n",
      "[26957 | 158.05] loss=1.03 avg=1.32\n",
      "[26958 | 158.17] loss=0.25 avg=1.31\n",
      "[26959 | 158.28] loss=2.26 avg=1.32\n",
      "[26960 | 158.42] loss=0.97 avg=1.32\n",
      "[26961 | 158.54] loss=1.27 avg=1.32\n",
      "[26962 | 158.65] loss=1.60 avg=1.32\n",
      "[26963 | 158.78] loss=0.51 avg=1.31\n",
      "[26964 | 158.90] loss=0.19 avg=1.30\n",
      "[26965 | 159.01] loss=1.81 avg=1.30\n",
      "[26966 | 159.12] loss=1.18 avg=1.30\n",
      "[26967 | 159.24] loss=1.83 avg=1.31\n",
      "[26968 | 159.36] loss=0.87 avg=1.30\n",
      "[26969 | 159.49] loss=1.29 avg=1.30\n",
      "[26970 | 159.59] loss=2.31 avg=1.31\n",
      "[26971 | 159.70] loss=0.31 avg=1.30\n",
      "[26972 | 159.83] loss=0.45 avg=1.30\n",
      "[26973 | 159.95] loss=2.11 avg=1.30\n",
      "[26974 | 160.06] loss=0.34 avg=1.29\n",
      "[26975 | 160.17] loss=1.77 avg=1.30\n",
      "[26976 | 160.29] loss=0.56 avg=1.29\n",
      "[26977 | 160.42] loss=0.83 avg=1.29\n",
      "[26978 | 160.52] loss=2.83 avg=1.30\n",
      "[26979 | 160.65] loss=0.45 avg=1.29\n",
      "[26980 | 160.76] loss=1.42 avg=1.30\n",
      "[26981 | 160.89] loss=1.86 avg=1.30\n",
      "[26982 | 161.04] loss=0.59 avg=1.29\n",
      "[26983 | 161.15] loss=1.33 avg=1.29\n",
      "[26984 | 161.29] loss=0.41 avg=1.29\n",
      "[26985 | 161.43] loss=0.22 avg=1.27\n",
      "[26986 | 161.53] loss=1.45 avg=1.28\n",
      "[26987 | 161.67] loss=3.20 avg=1.30\n",
      "[26988 | 161.78] loss=0.80 avg=1.29\n",
      "[26989 | 161.89] loss=1.43 avg=1.29\n",
      "[26990 | 162.01] loss=2.74 avg=1.31\n",
      "[26991 | 162.13] loss=1.93 avg=1.31\n",
      "[26992 | 162.25] loss=0.34 avg=1.30\n",
      "[26993 | 162.37] loss=1.96 avg=1.31\n",
      "[26994 | 162.48] loss=1.57 avg=1.31\n",
      "[26995 | 162.60] loss=0.78 avg=1.31\n",
      "[26996 | 162.71] loss=2.02 avg=1.31\n",
      "[26997 | 162.82] loss=0.84 avg=1.31\n",
      "[26998 | 162.98] loss=3.47 avg=1.33\n",
      "[26999 | 163.09] loss=0.57 avg=1.32\n",
      "Saving checkpoint/transfer3/model-27000\n",
      "Generating samples...\n",
      "Seed text : 여름에 하늘에서 번개가 치면서 천둥소리가 들렸습니다. 번개가 먼저 친 뒤에 시간이 조금 흐른 뒤 천둥소리가 들렸습니다. 왜 번개가 먼저 생기고 천둥소리는 나중에 들릴까요? 빛과 소리에 관한 지식을 활용하여 자세하게 설명해 봅시다.\n",
      "[10000]\n",
      "여름에 하늘에서 번개가 치면서 천둥소리가 들렸습니다. 번개가 먼저 친 뒤에 시간이 조금 흐른 뒤 천둥소리가 들렸습니다. 왜 번개가 먼저 생기고 천둥소리는 나중에 들릴까요? 빛과 소리에 관한 지식을 활용하여 자세하게 설명해 봅시다. 빛이 소리보다 빠르기 때문입니다. 소리의 이동하는 소리가 들리기 때문이다. 빛이 먼저 치고 천둥소리가 들리는 것이다. 왜 빛이 먼저 여러\n",
      "여름에 하늘에서 번개가 치면서 천둥소리가 들렸습니다. 번개가 먼저 친 뒤에 시간이 조금 흐른 뒤 천둥소리가 들렸습니다. 왜 번개가 먼저 생기고 천둥소리는 나중에 들릴까요? 빛과 소리에 관한 지식을 활용하여 자세하게 설명해 봅시다. 빛은 소리보다 빠르기 때문이다. 소리의 먼저 ⁇  빠르기 때문이다. 빛의 소리의 속력이 더 빠르기 때문이다. 번개가 먼저 치고\n",
      "여름에 하늘에서 번개가 치면서 천둥소리가 들렸습니다. 번개가 먼저 친 뒤에 시간이 조금 흐른 뒤 천둥소리가 들렸습니다. 왜 번개가 먼저 생기고 천둥소리는 나중에 들릴까요? 빛과 소리에 관한 지식을 활용하여 자세하게 설명해 봅시다. 빛의 이동하면 천둥소리가 천둥 소리가 들리기때문에 번개가 먼저 소리의만들것이다. 물에입니다. 빛이 먼저 생기고\n",
      "[27000 | 167.82] loss=1.13 avg=1.32\n",
      "[27001 | 167.93] loss=1.28 avg=1.32\n",
      "[27002 | 168.11] loss=2.88 avg=1.34\n",
      "[27003 | 168.24] loss=0.96 avg=1.33\n",
      "[27004 | 168.36] loss=1.78 avg=1.34\n",
      "[27005 | 168.49] loss=1.23 avg=1.34\n",
      "[27006 | 168.60] loss=1.99 avg=1.34\n",
      "[27007 | 168.74] loss=1.94 avg=1.35\n",
      "[27008 | 168.87] loss=2.11 avg=1.36\n",
      "[27009 | 168.97] loss=1.21 avg=1.35\n",
      "[27010 | 169.10] loss=1.21 avg=1.35\n",
      "[27011 | 169.21] loss=2.16 avg=1.36\n",
      "[27012 | 169.32] loss=2.18 avg=1.37\n",
      "[27013 | 169.43] loss=1.27 avg=1.37\n",
      "[27014 | 169.53] loss=1.35 avg=1.37\n",
      "[27015 | 169.65] loss=0.49 avg=1.36\n",
      "[27016 | 169.76] loss=1.16 avg=1.36\n",
      "[27017 | 169.88] loss=0.76 avg=1.35\n",
      "[27018 | 170.01] loss=2.68 avg=1.37\n",
      "[27019 | 170.13] loss=0.71 avg=1.36\n",
      "[27020 | 170.24] loss=1.53 avg=1.36\n",
      "[27021 | 170.36] loss=0.39 avg=1.35\n",
      "[27022 | 170.48] loss=0.69 avg=1.34\n",
      "[27023 | 170.58] loss=1.49 avg=1.35\n",
      "[27024 | 170.71] loss=0.52 avg=1.34\n",
      "[27025 | 170.83] loss=0.69 avg=1.33\n",
      "[27026 | 170.97] loss=1.25 avg=1.33\n",
      "[27027 | 171.08] loss=1.44 avg=1.33\n",
      "[27028 | 171.22] loss=0.78 avg=1.33\n",
      "[27029 | 171.41] loss=2.66 avg=1.34\n",
      "[27030 | 171.53] loss=1.36 avg=1.34\n",
      "[27031 | 171.65] loss=1.20 avg=1.34\n",
      "[27032 | 171.79] loss=0.61 avg=1.33\n",
      "[27033 | 171.90] loss=2.24 avg=1.34\n",
      "[27034 | 172.01] loss=1.55 avg=1.34\n",
      "[27035 | 172.12] loss=0.88 avg=1.34\n",
      "[27036 | 172.23] loss=1.30 avg=1.34\n",
      "[27037 | 172.34] loss=1.22 avg=1.34\n",
      "[27038 | 172.45] loss=1.01 avg=1.33\n",
      "[27039 | 172.57] loss=0.87 avg=1.33\n",
      "[27040 | 172.69] loss=0.77 avg=1.32\n",
      "[27041 | 172.80] loss=2.12 avg=1.33\n",
      "[27042 | 172.91] loss=1.32 avg=1.33\n",
      "[27043 | 173.02] loss=1.37 avg=1.33\n",
      "[27044 | 173.16] loss=0.66 avg=1.32\n",
      "[27045 | 173.27] loss=0.49 avg=1.32\n",
      "[27046 | 173.38] loss=1.20 avg=1.31\n",
      "[27047 | 173.49] loss=1.42 avg=1.31\n",
      "[27048 | 173.61] loss=0.62 avg=1.31\n",
      "[27049 | 173.72] loss=2.93 avg=1.32\n",
      "[27050 | 173.82] loss=0.43 avg=1.32\n",
      "[27051 | 173.93] loss=0.46 avg=1.31\n",
      "[27052 | 174.04] loss=0.69 avg=1.30\n",
      "[27053 | 174.15] loss=0.48 avg=1.29\n",
      "[27054 | 174.28] loss=0.38 avg=1.28\n",
      "[27055 | 174.39] loss=1.05 avg=1.28\n",
      "[27056 | 174.51] loss=0.35 avg=1.27\n",
      "[27057 | 174.64] loss=0.39 avg=1.26\n",
      "[27058 | 174.76] loss=0.57 avg=1.26\n",
      "[27059 | 174.90] loss=0.19 avg=1.25\n",
      "[27060 | 175.03] loss=1.29 avg=1.25\n",
      "[27061 | 175.13] loss=0.42 avg=1.24\n",
      "[27062 | 175.27] loss=3.52 avg=1.26\n",
      "[27063 | 175.39] loss=0.58 avg=1.25\n",
      "[27064 | 175.52] loss=0.60 avg=1.25\n",
      "[27065 | 175.65] loss=0.35 avg=1.24\n",
      "[27066 | 175.77] loss=0.61 avg=1.23\n",
      "[27067 | 175.88] loss=1.48 avg=1.23\n",
      "[27068 | 176.01] loss=4.01 avg=1.26\n",
      "[27069 | 176.13] loss=1.14 avg=1.26\n",
      "[27070 | 176.28] loss=0.35 avg=1.25\n",
      "[27071 | 176.39] loss=0.64 avg=1.25\n",
      "[27072 | 176.50] loss=1.52 avg=1.25\n",
      "[27073 | 176.62] loss=0.34 avg=1.24\n",
      "[27074 | 176.74] loss=0.24 avg=1.23\n",
      "[27075 | 176.85] loss=0.25 avg=1.22\n",
      "[27076 | 176.97] loss=1.19 avg=1.22\n",
      "[27077 | 177.10] loss=0.45 avg=1.21\n",
      "[27078 | 177.22] loss=1.28 avg=1.21\n",
      "[27079 | 177.36] loss=0.18 avg=1.20\n",
      "[27080 | 177.48] loss=1.15 avg=1.20\n",
      "[27081 | 177.60] loss=0.19 avg=1.19\n",
      "[27082 | 177.72] loss=1.37 avg=1.19\n",
      "[27083 | 177.84] loss=1.38 avg=1.19\n",
      "[27084 | 177.96] loss=0.39 avg=1.19\n",
      "[27085 | 178.10] loss=3.42 avg=1.21\n",
      "[27086 | 178.23] loss=1.23 avg=1.21\n",
      "[27087 | 178.34] loss=3.31 avg=1.23\n",
      "[27088 | 178.45] loss=0.73 avg=1.23\n",
      "[27089 | 178.57] loss=0.42 avg=1.22\n",
      "[27090 | 178.69] loss=1.22 avg=1.22\n",
      "[27091 | 178.79] loss=0.58 avg=1.21\n",
      "[27092 | 178.92] loss=2.85 avg=1.23\n",
      "[27093 | 179.04] loss=0.78 avg=1.22\n",
      "[27094 | 179.17] loss=1.68 avg=1.23\n",
      "[27095 | 179.30] loss=1.64 avg=1.23\n",
      "[27096 | 179.43] loss=0.79 avg=1.23\n",
      "[27097 | 179.54] loss=1.21 avg=1.23\n",
      "[27098 | 179.65] loss=0.65 avg=1.22\n",
      "[27099 | 179.76] loss=1.14 avg=1.22\n",
      "Generating samples...\n",
      "Seed text : 식물의 뿌리는 땅 아래에 있어 잘 보이지 않지만 여러 가지 기능을 가지고 있습니다. 식물의 뿌리는 어떤 일들을 할까요? 식물의 구조와 기능에 관한 지식을 활용하여 자세하게 설명하여 봅시다.\n",
      "[]\n",
      "식물의 뿌리는 땅 아래에 있어 잘 보이지 않지만 여러 가지 기능을 가지고 있습니다. 식물의 뿌리는 어떤 일들을 할까요? 식물의 구조와 기능에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 뿌리를 통해 영양분을 흡수 ⁇  영양분 ⁇  식물을 지탱한다.  ⁇ .............\n",
      "식물의 뿌리는 땅 아래에 있어 잘 보이지 않지만 여러 가지 기능을 가지고 있습니다. 식물의 뿌리는 어떤 일들을 할까요? 식물의 구조와 기능에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 뿌리를 통해 지탱 ⁇  흙가 물이 흡수 ⁇  영양분을 빨면 식물의 지지해준다. 그래서 식물이 만들어 양분을 흡수 ⁇  뿌리를\n",
      "식물의 뿌리는 땅 아래에 있어 잘 보이지 않지만 여러 가지 기능을 가지고 있습니다. 식물의 뿌리는 어떤 일들을 할까요? 식물의 구조와 기능에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 수분을 흡수해준다..........................\n",
      "[27100 | 181.65] loss=3.11 avg=1.24\n",
      "[27101 | 181.76] loss=0.52 avg=1.23\n",
      "[27102 | 181.87] loss=2.41 avg=1.24\n",
      "[27103 | 181.97] loss=1.07 avg=1.24\n",
      "[27104 | 182.10] loss=0.13 avg=1.23\n",
      "[27105 | 182.21] loss=1.60 avg=1.23\n",
      "[27106 | 182.34] loss=1.92 avg=1.24\n",
      "[27107 | 182.46] loss=1.44 avg=1.24\n",
      "[27108 | 182.56] loss=1.06 avg=1.24\n",
      "[27109 | 182.67] loss=0.56 avg=1.24\n",
      "[27110 | 182.79] loss=0.39 avg=1.23\n",
      "[27111 | 182.90] loss=0.77 avg=1.22\n",
      "[27112 | 183.03] loss=2.58 avg=1.24\n",
      "[27113 | 183.15] loss=0.29 avg=1.23\n",
      "[27114 | 183.28] loss=2.59 avg=1.24\n",
      "[27115 | 183.40] loss=1.21 avg=1.24\n",
      "[27116 | 183.52] loss=1.66 avg=1.24\n",
      "[27117 | 183.65] loss=1.45 avg=1.25\n",
      "[27118 | 183.75] loss=1.69 avg=1.25\n",
      "[27119 | 183.88] loss=0.47 avg=1.24\n",
      "[27120 | 184.01] loss=2.43 avg=1.25\n",
      "[27121 | 184.13] loss=0.80 avg=1.25\n",
      "[27122 | 184.24] loss=1.04 avg=1.25\n",
      "[27123 | 184.36] loss=1.17 avg=1.25\n",
      "[27124 | 184.46] loss=2.68 avg=1.26\n",
      "[27125 | 184.57] loss=0.47 avg=1.25\n",
      "[27126 | 184.69] loss=1.02 avg=1.25\n",
      "[27127 | 184.81] loss=0.67 avg=1.25\n",
      "[27128 | 184.93] loss=2.38 avg=1.26\n",
      "[27129 | 185.04] loss=2.21 avg=1.27\n",
      "[27130 | 185.15] loss=0.71 avg=1.26\n",
      "[27131 | 185.26] loss=0.73 avg=1.26\n",
      "[27132 | 185.37] loss=0.60 avg=1.25\n",
      "[27133 | 185.50] loss=1.50 avg=1.25\n",
      "[27134 | 185.63] loss=2.09 avg=1.26\n",
      "[27135 | 185.75] loss=0.56 avg=1.25\n",
      "[27136 | 185.85] loss=1.02 avg=1.25\n",
      "[27137 | 185.98] loss=0.61 avg=1.24\n",
      "[27138 | 186.10] loss=1.44 avg=1.25\n",
      "[27139 | 186.23] loss=3.05 avg=1.26\n",
      "[27140 | 186.36] loss=1.39 avg=1.27\n",
      "[27141 | 186.47] loss=1.12 avg=1.26\n",
      "[27142 | 186.57] loss=1.61 avg=1.27\n",
      "[27143 | 186.68] loss=0.65 avg=1.26\n",
      "[27144 | 186.79] loss=0.60 avg=1.25\n",
      "[27145 | 186.90] loss=1.17 avg=1.25\n",
      "[27146 | 187.02] loss=0.96 avg=1.25\n",
      "[27147 | 187.13] loss=1.75 avg=1.26\n",
      "[27148 | 187.23] loss=1.09 avg=1.25\n",
      "[27149 | 187.34] loss=3.61 avg=1.28\n",
      "[27150 | 187.46] loss=0.27 avg=1.27\n",
      "[27151 | 187.58] loss=0.37 avg=1.26\n",
      "[27152 | 187.70] loss=0.49 avg=1.25\n",
      "[27153 | 187.81] loss=0.42 avg=1.24\n",
      "[27154 | 187.93] loss=0.79 avg=1.24\n",
      "[27155 | 188.07] loss=0.90 avg=1.23\n",
      "[27156 | 188.19] loss=0.19 avg=1.22\n",
      "[27157 | 188.31] loss=0.37 avg=1.22\n",
      "[27158 | 188.43] loss=0.99 avg=1.21\n",
      "[27159 | 188.56] loss=1.35 avg=1.21\n",
      "[27160 | 188.67] loss=0.68 avg=1.21\n",
      "[27161 | 188.79] loss=0.37 avg=1.20\n",
      "[27162 | 189.18] loss=0.26 avg=1.19\n",
      "[27163 | 189.28] loss=1.21 avg=1.19\n",
      "[27164 | 189.39] loss=2.12 avg=1.20\n",
      "[27165 | 189.51] loss=2.48 avg=1.21\n",
      "[27166 | 189.61] loss=1.28 avg=1.21\n",
      "[27167 | 189.72] loss=1.30 avg=1.22\n",
      "[27168 | 189.85] loss=0.62 avg=1.21\n",
      "[27169 | 190.00] loss=3.62 avg=1.23\n",
      "[27170 | 190.14] loss=0.21 avg=1.22\n",
      "[27171 | 190.25] loss=2.73 avg=1.24\n",
      "[27172 | 190.36] loss=1.24 avg=1.24\n",
      "[27173 | 190.46] loss=1.25 avg=1.24\n",
      "[27174 | 190.59] loss=2.98 avg=1.26\n",
      "[27175 | 190.70] loss=1.99 avg=1.26\n",
      "[27176 | 190.81] loss=1.17 avg=1.26\n",
      "[27177 | 190.93] loss=0.50 avg=1.25\n",
      "[27178 | 191.08] loss=1.38 avg=1.26\n",
      "[27179 | 191.20] loss=0.44 avg=1.25\n",
      "[27180 | 191.32] loss=1.02 avg=1.25\n",
      "[27181 | 191.44] loss=0.14 avg=1.23\n",
      "[27182 | 191.56] loss=2.72 avg=1.25\n",
      "[27183 | 191.67] loss=0.38 avg=1.24\n",
      "[27184 | 191.77] loss=1.63 avg=1.24\n",
      "[27185 | 191.90] loss=3.01 avg=1.26\n",
      "[27186 | 192.01] loss=2.62 avg=1.28\n",
      "[27187 | 192.13] loss=1.58 avg=1.28\n",
      "[27188 | 192.27] loss=1.05 avg=1.28\n",
      "[27189 | 192.42] loss=0.94 avg=1.27\n",
      "[27190 | 192.52] loss=1.50 avg=1.28\n",
      "[27191 | 192.64] loss=0.71 avg=1.27\n",
      "[27192 | 192.75] loss=1.49 avg=1.27\n",
      "[27193 | 192.86] loss=0.71 avg=1.27\n",
      "[27194 | 192.96] loss=1.35 avg=1.27\n",
      "[27195 | 193.09] loss=0.33 avg=1.26\n",
      "[27196 | 193.21] loss=0.32 avg=1.25\n",
      "[27197 | 193.33] loss=0.55 avg=1.24\n",
      "[27198 | 193.48] loss=2.17 avg=1.25\n",
      "[27199 | 193.59] loss=0.86 avg=1.25\n",
      "Generating samples...\n",
      "Seed text : 석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요?\n",
      "[10000]\n",
      "석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요? 자원이 사용하게해 태양박해서 우리에 녹아져 사용할수없게 됩니다. 그리고 미래에 어떤 비 에너지와 자원이 고갈하기\n",
      "석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요? 태양광 안 석유를너지를 사용하고 태양광 에너지가 고갈되지 않는 이산화 비 발전하는 것이다. 고갈물질을 많이 생길\n",
      "석탄이나 석유와 같은 화석에너지를 많이 사용하면 미래에 어떤 일이 생길까요? 태양광 발전이 석탄이나 석유가 발생한다. 무라지지 ⁇  양이나 석유는 사용하지않는다. 그리고 오염한 오염을 이용\n",
      "[27200 | 195.55] loss=0.66 avg=1.24\n",
      "[27201 | 195.66] loss=0.54 avg=1.23\n",
      "[27202 | 195.80] loss=2.04 avg=1.24\n",
      "[27203 | 195.91] loss=0.58 avg=1.24\n",
      "[27204 | 196.03] loss=2.74 avg=1.25\n",
      "[27205 | 196.14] loss=1.45 avg=1.25\n",
      "[27206 | 196.25] loss=1.20 avg=1.25\n",
      "[27207 | 196.38] loss=3.38 avg=1.27\n",
      "[27208 | 196.50] loss=2.38 avg=1.28\n",
      "[27209 | 196.62] loss=0.84 avg=1.28\n",
      "[27210 | 196.73] loss=1.54 avg=1.28\n",
      "[27211 | 196.85] loss=0.23 avg=1.27\n",
      "[27212 | 196.98] loss=1.78 avg=1.28\n",
      "[27213 | 197.09] loss=1.84 avg=1.28\n",
      "[27214 | 197.21] loss=0.89 avg=1.28\n",
      "[27215 | 197.33] loss=0.16 avg=1.27\n",
      "[27216 | 197.43] loss=1.75 avg=1.27\n",
      "[27217 | 197.56] loss=0.73 avg=1.27\n",
      "[27218 | 197.66] loss=1.39 avg=1.27\n",
      "[27219 | 197.78] loss=1.63 avg=1.27\n",
      "[27220 | 197.91] loss=1.83 avg=1.28\n",
      "[27221 | 198.02] loss=1.08 avg=1.28\n",
      "[27222 | 198.13] loss=1.92 avg=1.28\n",
      "[27223 | 198.25] loss=0.90 avg=1.28\n",
      "[27224 | 198.36] loss=2.84 avg=1.29\n",
      "[27225 | 198.47] loss=1.92 avg=1.30\n",
      "[27226 | 198.58] loss=2.40 avg=1.31\n",
      "[27227 | 198.69] loss=0.42 avg=1.30\n",
      "[27228 | 198.79] loss=2.52 avg=1.31\n",
      "[27229 | 198.90] loss=1.26 avg=1.31\n",
      "[27230 | 199.02] loss=1.31 avg=1.31\n",
      "[27231 | 199.13] loss=1.00 avg=1.31\n",
      "[27232 | 199.23] loss=1.34 avg=1.31\n",
      "[27233 | 199.34] loss=2.22 avg=1.32\n",
      "[27234 | 199.46] loss=1.28 avg=1.32\n",
      "[27235 | 199.57] loss=1.83 avg=1.32\n",
      "[27236 | 199.69] loss=0.15 avg=1.31\n",
      "[27237 | 199.81] loss=0.56 avg=1.31\n",
      "[27238 | 199.92] loss=4.28 avg=1.34\n",
      "[27239 | 200.06] loss=1.83 avg=1.34\n",
      "[27240 | 200.16] loss=1.00 avg=1.34\n",
      "[27241 | 200.28] loss=1.26 avg=1.34\n",
      "[27242 | 200.38] loss=0.83 avg=1.33\n",
      "[27243 | 200.51] loss=1.40 avg=1.33\n",
      "[27244 | 200.62] loss=2.88 avg=1.35\n",
      "[27245 | 200.75] loss=1.16 avg=1.35\n",
      "[27246 | 200.86] loss=0.53 avg=1.34\n",
      "[27247 | 200.98] loss=0.48 avg=1.33\n",
      "[27248 | 201.11] loss=0.58 avg=1.32\n",
      "[27249 | 201.24] loss=1.54 avg=1.32\n",
      "[27250 | 201.35] loss=1.54 avg=1.33\n",
      "[27251 | 201.48] loss=1.50 avg=1.33\n",
      "[27252 | 201.63] loss=3.61 avg=1.35\n",
      "[27253 | 201.75] loss=0.57 avg=1.34\n",
      "[27254 | 201.86] loss=0.92 avg=1.34\n",
      "[27255 | 201.99] loss=0.82 avg=1.33\n",
      "[27256 | 202.10] loss=2.43 avg=1.34\n",
      "[27257 | 202.21] loss=0.81 avg=1.34\n",
      "[27258 | 202.31] loss=0.08 avg=1.33\n",
      "[27259 | 202.43] loss=0.16 avg=1.31\n",
      "[27260 | 202.56] loss=3.13 avg=1.33\n",
      "[27261 | 202.66] loss=3.46 avg=1.35\n",
      "[27262 | 202.77] loss=0.88 avg=1.35\n",
      "[27263 | 202.90] loss=0.50 avg=1.34\n",
      "[27264 | 203.02] loss=0.65 avg=1.33\n",
      "[27265 | 203.14] loss=0.10 avg=1.32\n",
      "[27266 | 203.25] loss=1.50 avg=1.32\n",
      "[27267 | 203.38] loss=0.19 avg=1.31\n",
      "[27268 | 203.49] loss=1.20 avg=1.31\n",
      "[27269 | 203.61] loss=2.58 avg=1.32\n",
      "[27270 | 203.73] loss=2.59 avg=1.34\n",
      "[27271 | 203.83] loss=1.17 avg=1.33\n",
      "[27272 | 203.95] loss=1.42 avg=1.33\n",
      "[27273 | 204.08] loss=1.40 avg=1.34\n",
      "[27274 | 204.19] loss=0.66 avg=1.33\n",
      "[27275 | 204.31] loss=0.60 avg=1.32\n",
      "[27276 | 204.41] loss=0.84 avg=1.32\n",
      "[27277 | 204.54] loss=2.23 avg=1.33\n",
      "[27278 | 204.66] loss=3.10 avg=1.34\n",
      "[27279 | 204.78] loss=0.76 avg=1.34\n",
      "[27280 | 204.89] loss=0.69 avg=1.33\n",
      "[27281 | 205.01] loss=0.81 avg=1.33\n",
      "[27282 | 205.12] loss=0.87 avg=1.32\n",
      "[27283 | 205.23] loss=3.12 avg=1.34\n",
      "[27284 | 205.37] loss=0.54 avg=1.33\n",
      "[27285 | 205.48] loss=1.56 avg=1.33\n",
      "[27286 | 205.60] loss=1.06 avg=1.33\n",
      "[27287 | 205.72] loss=0.44 avg=1.32\n",
      "[27288 | 205.83] loss=0.25 avg=1.31\n",
      "[27289 | 205.96] loss=0.12 avg=1.30\n",
      "[27290 | 206.09] loss=1.66 avg=1.30\n",
      "[27291 | 206.20] loss=0.43 avg=1.29\n",
      "[27292 | 206.32] loss=0.70 avg=1.29\n",
      "[27293 | 206.43] loss=2.26 avg=1.30\n",
      "[27294 | 206.55] loss=2.36 avg=1.31\n",
      "[27295 | 206.66] loss=0.38 avg=1.30\n",
      "[27296 | 206.78] loss=0.10 avg=1.29\n",
      "[27297 | 206.89] loss=1.67 avg=1.29\n",
      "[27298 | 206.99] loss=1.22 avg=1.29\n",
      "[27299 | 207.10] loss=3.37 avg=1.31\n",
      "Generating samples...\n",
      "Seed text : 20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요?\n",
      "[]\n",
      "20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요? 과학기술의 발달이 더 편리하게 해준다.. ⁇  과학기술의 발달은 우리 생활에 도움을 주었게할 수 있다.....\n",
      "20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요? 우리의 생활에 편리하게 해주고 다른 등등과 정보를 못 해주었다................\n",
      "20년 전에는 상상도 할 수 없었던 손 안에 작은 컴퓨터인 스마트폰을 지금은 대부분의 사람들이 사용하고 있습니다. 과학기술이 발달하여 많은 변화들이 생겼습니다. 과학기술의 발달은 우리 생활에 어떤 변화를 주었을까요? 편하고 편한 스마트폰보다 과학에 큰 편리하게 만들어주는 기능을를 할수있게 만들수있다.  ⁇  편하게 해준다\n",
      "[27300 | 209.01] loss=0.72 avg=1.31\n",
      "[27301 | 209.12] loss=0.86 avg=1.30\n",
      "[27302 | 209.22] loss=1.03 avg=1.30\n",
      "[27303 | 209.35] loss=1.13 avg=1.30\n",
      "[27304 | 209.47] loss=0.54 avg=1.29\n",
      "[27305 | 209.58] loss=3.44 avg=1.31\n",
      "[27306 | 209.68] loss=0.93 avg=1.31\n",
      "[27307 | 209.82] loss=4.49 avg=1.34\n",
      "[27308 | 209.93] loss=0.98 avg=1.33\n",
      "[27309 | 210.05] loss=1.76 avg=1.34\n",
      "[27310 | 210.15] loss=3.41 avg=1.36\n",
      "[27311 | 210.27] loss=1.49 avg=1.36\n",
      "[27312 | 210.38] loss=1.81 avg=1.37\n",
      "[27313 | 210.50] loss=0.51 avg=1.36\n",
      "[27314 | 210.61] loss=0.97 avg=1.35\n",
      "[27315 | 210.71] loss=1.06 avg=1.35\n",
      "[27316 | 210.83] loss=0.48 avg=1.34\n",
      "[27317 | 210.95] loss=0.12 avg=1.33\n",
      "[27318 | 211.07] loss=1.42 avg=1.33\n",
      "[27319 | 211.18] loss=4.15 avg=1.36\n",
      "[27320 | 211.29] loss=1.26 avg=1.36\n",
      "[27321 | 211.41] loss=2.85 avg=1.37\n",
      "[27322 | 211.53] loss=0.12 avg=1.36\n",
      "[27323 | 211.64] loss=1.05 avg=1.36\n",
      "[27324 | 211.74] loss=1.10 avg=1.35\n",
      "[27325 | 211.85] loss=1.74 avg=1.36\n",
      "[27326 | 211.96] loss=2.24 avg=1.37\n",
      "[27327 | 212.09] loss=0.99 avg=1.36\n",
      "[27328 | 212.24] loss=0.52 avg=1.35\n",
      "[27329 | 212.34] loss=0.70 avg=1.35\n",
      "[27330 | 212.45] loss=2.20 avg=1.36\n",
      "[27331 | 212.56] loss=1.11 avg=1.35\n",
      "[27332 | 212.69] loss=0.91 avg=1.35\n",
      "[27333 | 212.81] loss=0.37 avg=1.34\n",
      "[27334 | 212.92] loss=0.67 avg=1.33\n",
      "[27335 | 213.05] loss=0.61 avg=1.33\n",
      "[27336 | 213.16] loss=0.42 avg=1.32\n",
      "[27337 | 213.28] loss=0.47 avg=1.31\n",
      "[27338 | 213.38] loss=2.74 avg=1.32\n",
      "[27339 | 213.49] loss=2.83 avg=1.34\n",
      "[27340 | 213.60] loss=0.45 avg=1.33\n",
      "[27341 | 213.72] loss=2.20 avg=1.34\n",
      "[27342 | 213.84] loss=1.08 avg=1.34\n",
      "[27343 | 213.96] loss=2.97 avg=1.35\n",
      "[27344 | 214.07] loss=0.29 avg=1.34\n",
      "[27345 | 214.18] loss=3.60 avg=1.36\n",
      "[27346 | 214.29] loss=2.79 avg=1.38\n",
      "[27347 | 214.39] loss=1.64 avg=1.38\n",
      "[27348 | 214.50] loss=3.71 avg=1.40\n",
      "[27349 | 214.61] loss=0.07 avg=1.39\n",
      "[27350 | 214.72] loss=2.12 avg=1.40\n",
      "[27351 | 214.84] loss=0.62 avg=1.39\n",
      "[27352 | 214.95] loss=0.66 avg=1.38\n",
      "[27353 | 215.05] loss=0.45 avg=1.37\n",
      "[27354 | 215.17] loss=0.28 avg=1.36\n",
      "[27355 | 215.29] loss=0.67 avg=1.36\n",
      "[27356 | 215.41] loss=1.07 avg=1.35\n",
      "[27357 | 215.53] loss=0.70 avg=1.35\n",
      "[27358 | 215.63] loss=0.40 avg=1.34\n",
      "[27359 | 215.75] loss=0.80 avg=1.33\n",
      "[27360 | 215.86] loss=2.99 avg=1.35\n",
      "[27361 | 215.97] loss=2.19 avg=1.36\n",
      "[27362 | 216.09] loss=0.42 avg=1.35\n",
      "[27363 | 216.21] loss=0.25 avg=1.34\n",
      "[27364 | 216.33] loss=0.70 avg=1.33\n",
      "[27365 | 216.44] loss=1.43 avg=1.33\n",
      "[27366 | 216.55] loss=0.50 avg=1.32\n",
      "[27367 | 216.65] loss=0.82 avg=1.32\n",
      "[27368 | 216.76] loss=1.92 avg=1.32\n",
      "[27369 | 216.89] loss=0.55 avg=1.32\n",
      "[27370 | 217.00] loss=2.35 avg=1.33\n",
      "[27371 | 217.12] loss=0.33 avg=1.32\n",
      "[27372 | 217.23] loss=0.92 avg=1.31\n",
      "[27373 | 217.36] loss=3.15 avg=1.33\n",
      "[27374 | 217.49] loss=0.87 avg=1.33\n",
      "[27375 | 217.61] loss=2.99 avg=1.34\n",
      "[27376 | 217.73] loss=0.62 avg=1.33\n",
      "[27377 | 217.84] loss=2.69 avg=1.35\n",
      "[27378 | 217.95] loss=0.39 avg=1.34\n",
      "[27379 | 218.08] loss=1.27 avg=1.34\n",
      "[27380 | 218.20] loss=0.11 avg=1.33\n",
      "[27381 | 218.32] loss=1.44 avg=1.33\n",
      "[27382 | 218.42] loss=1.12 avg=1.33\n",
      "[27383 | 218.54] loss=0.54 avg=1.32\n",
      "[27384 | 218.68] loss=1.57 avg=1.32\n",
      "[27385 | 218.78] loss=1.21 avg=1.32\n",
      "[27386 | 218.90] loss=3.82 avg=1.34\n",
      "[27387 | 219.01] loss=3.03 avg=1.36\n",
      "[27388 | 219.12] loss=1.54 avg=1.36\n",
      "[27389 | 219.23] loss=1.70 avg=1.37\n",
      "[27390 | 219.33] loss=1.07 avg=1.36\n",
      "[27391 | 219.46] loss=1.12 avg=1.36\n",
      "[27392 | 219.57] loss=0.93 avg=1.36\n",
      "[27393 | 219.68] loss=1.41 avg=1.36\n",
      "[27394 | 219.78] loss=0.62 avg=1.35\n",
      "[27395 | 219.89] loss=2.45 avg=1.36\n",
      "[27396 | 219.99] loss=0.22 avg=1.35\n",
      "[27397 | 220.12] loss=0.26 avg=1.34\n",
      "[27398 | 220.23] loss=1.86 avg=1.34\n",
      "[27399 | 220.35] loss=0.63 avg=1.34\n",
      "Generating samples...\n",
      "Seed text : 비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요.\n",
      "[10002]\n",
      "비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요. 열을수록 온도가 높여 ⁇   ⁇ 다 ⁇ 다 ⁇ 실 저게 중 ⁇ 이 더 낮아지고 ⁇ 다. ⁇ 를 더 더 낮\n",
      "비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요. 물을 더 넣는다. 온도를 더 높을 경우 저어준다.................\n",
      "비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요. 물을 더 넣는다 ⁇  가을수록 물을 더 가열하는 전기를 더 섞어서 더 많이 저어주고 소금의 증가로올르기 때문에 가\n",
      "[27400 | 222.23] loss=0.64 avg=1.33\n",
      "[27401 | 222.34] loss=0.76 avg=1.32\n",
      "[27402 | 222.51] loss=1.78 avg=1.33\n",
      "[27403 | 222.62] loss=1.03 avg=1.32\n",
      "[27404 | 222.73] loss=1.32 avg=1.32\n",
      "[27405 | 222.83] loss=2.92 avg=1.34\n",
      "[27406 | 222.94] loss=2.79 avg=1.36\n",
      "[27407 | 223.06] loss=1.40 avg=1.36\n",
      "[27408 | 223.16] loss=2.12 avg=1.36\n",
      "[27409 | 223.29] loss=1.58 avg=1.37\n",
      "[27410 | 223.39] loss=0.93 avg=1.36\n",
      "[27411 | 223.52] loss=0.22 avg=1.35\n",
      "[27412 | 223.65] loss=0.51 avg=1.34\n",
      "[27413 | 223.76] loss=0.48 avg=1.33\n",
      "[27414 | 223.89] loss=1.22 avg=1.33\n",
      "[27415 | 224.00] loss=2.24 avg=1.34\n",
      "[27416 | 224.12] loss=1.49 avg=1.34\n",
      "[27417 | 224.24] loss=0.68 avg=1.34\n",
      "[27418 | 224.36] loss=1.29 avg=1.34\n",
      "[27419 | 224.47] loss=1.32 avg=1.33\n",
      "[27420 | 224.58] loss=2.08 avg=1.34\n",
      "[27421 | 224.70] loss=0.88 avg=1.34\n",
      "[27422 | 224.84] loss=2.11 avg=1.35\n",
      "[27423 | 224.96] loss=0.21 avg=1.33\n",
      "[27424 | 225.10] loss=0.45 avg=1.33\n",
      "[27425 | 225.23] loss=1.96 avg=1.33\n",
      "[27426 | 225.35] loss=1.00 avg=1.33\n",
      "[27427 | 225.47] loss=0.47 avg=1.32\n",
      "[27428 | 225.59] loss=0.68 avg=1.31\n",
      "[27429 | 225.71] loss=0.95 avg=1.31\n",
      "[27430 | 225.84] loss=1.04 avg=1.31\n",
      "[27431 | 225.95] loss=1.07 avg=1.30\n",
      "[27432 | 226.09] loss=1.29 avg=1.30\n",
      "[27433 | 226.21] loss=1.53 avg=1.31\n",
      "[27434 | 226.32] loss=0.47 avg=1.30\n",
      "[27435 | 226.45] loss=0.23 avg=1.29\n",
      "[27436 | 226.56] loss=0.97 avg=1.28\n",
      "[27437 | 226.68] loss=0.20 avg=1.27\n",
      "[27438 | 226.79] loss=0.53 avg=1.27\n",
      "[27439 | 226.93] loss=3.71 avg=1.29\n",
      "[27440 | 227.06] loss=0.59 avg=1.28\n",
      "[27441 | 227.18] loss=0.95 avg=1.28\n",
      "[27442 | 227.28] loss=0.50 avg=1.27\n",
      "[27443 | 227.39] loss=0.43 avg=1.26\n",
      "[27444 | 227.51] loss=0.77 avg=1.26\n",
      "[27445 | 227.65] loss=3.48 avg=1.28\n",
      "[27446 | 227.78] loss=0.10 avg=1.27\n",
      "[27447 | 227.88] loss=0.76 avg=1.26\n",
      "[27448 | 228.00] loss=0.72 avg=1.26\n",
      "[27449 | 228.11] loss=1.81 avg=1.26\n",
      "[27450 | 228.22] loss=1.26 avg=1.26\n",
      "[27451 | 228.32] loss=2.02 avg=1.27\n",
      "[27452 | 228.43] loss=0.76 avg=1.27\n",
      "[27453 | 228.55] loss=1.09 avg=1.27\n",
      "[27454 | 228.69] loss=0.76 avg=1.26\n",
      "[27455 | 228.79] loss=1.71 avg=1.26\n",
      "[27456 | 228.92] loss=0.51 avg=1.26\n",
      "[27457 | 229.03] loss=1.83 avg=1.26\n",
      "[27458 | 229.14] loss=0.96 avg=1.26\n",
      "[27459 | 229.25] loss=1.44 avg=1.26\n",
      "[27460 | 229.37] loss=1.41 avg=1.26\n",
      "[27461 | 229.50] loss=4.92 avg=1.30\n",
      "[27462 | 229.62] loss=0.05 avg=1.29\n",
      "[27463 | 229.75] loss=0.87 avg=1.28\n",
      "[27464 | 229.85] loss=0.99 avg=1.28\n",
      "[27465 | 229.96] loss=0.48 avg=1.27\n",
      "[27466 | 230.07] loss=1.73 avg=1.28\n",
      "[27467 | 230.18] loss=0.84 avg=1.27\n",
      "[27468 | 230.31] loss=0.56 avg=1.27\n",
      "[27469 | 230.41] loss=1.76 avg=1.27\n",
      "[27470 | 230.52] loss=0.90 avg=1.27\n",
      "[27471 | 230.64] loss=1.07 avg=1.26\n",
      "[27472 | 230.77] loss=1.82 avg=1.27\n",
      "[27473 | 230.90] loss=2.46 avg=1.28\n",
      "[27474 | 231.03] loss=2.64 avg=1.30\n",
      "[27475 | 231.15] loss=0.68 avg=1.29\n",
      "[27476 | 231.27] loss=1.31 avg=1.29\n",
      "[27477 | 231.39] loss=0.17 avg=1.28\n",
      "[27478 | 231.50] loss=0.66 avg=1.27\n",
      "[27479 | 231.60] loss=0.77 avg=1.27\n",
      "[27480 | 231.73] loss=2.04 avg=1.27\n",
      "[27481 | 231.85] loss=0.70 avg=1.27\n",
      "[27482 | 231.97] loss=0.57 avg=1.26\n",
      "[27483 | 232.09] loss=1.32 avg=1.26\n",
      "[27484 | 232.20] loss=2.82 avg=1.28\n",
      "[27485 | 232.34] loss=2.74 avg=1.29\n",
      "[27486 | 232.45] loss=0.51 avg=1.28\n",
      "[27487 | 232.56] loss=1.36 avg=1.29\n",
      "[27488 | 232.68] loss=0.71 avg=1.28\n",
      "[27489 | 232.81] loss=0.70 avg=1.27\n",
      "[27490 | 232.93] loss=0.38 avg=1.27\n",
      "[27491 | 233.05] loss=0.43 avg=1.26\n",
      "[27492 | 233.17] loss=1.03 avg=1.25\n",
      "[27493 | 233.28] loss=2.26 avg=1.26\n",
      "[27494 | 233.38] loss=2.38 avg=1.28\n",
      "[27495 | 233.50] loss=0.95 avg=1.27\n",
      "[27496 | 233.62] loss=0.49 avg=1.26\n",
      "[27497 | 233.77] loss=2.86 avg=1.28\n",
      "[27498 | 233.87] loss=0.97 avg=1.28\n",
      "[27499 | 233.98] loss=1.55 avg=1.28\n",
      "Saving checkpoint/transfer3/model-27500\n",
      "WARNING:tensorflow:From /home/jin-lab/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Generating samples...\n",
      "Seed text : 약 2천 년 전 한반도에는 신라 ⁇  백제 ⁇  고구려의 세 개의 나라였습니다. 약 700년간 세 개의 나라는 한강 등을 차지하기 위하여 많은 전쟁을 하였습니다. 결국 신라는 삼국을 통일하였습니다. 신라가 삼국을 통일하는 과정을 설명하여 봅시다.\n",
      "[10001]\n",
      "약 2천 년 전 한반도에는 신라 ⁇  백제 ⁇  고구려의 세 개의 나라였습니다. 약 700년간 세 개의 나라는 한강 등을 차지하기 위하여 많은 전쟁을 하였습니다. 결국 신라는 삼국을 통일하였습니다. 신라가 삼국을 통일하는 과정을 설명하여 봅시다. 신라는 신라가 연합을함. .. 숨백제 멸망시키고 고구려 멸망시키고 측라당나라에서 멸망시키고당전쟁당라는당나\n",
      "약 2천 년 전 한반도에는 신라 ⁇  백제 ⁇  고구려의 세 개의 나라였습니다. 약 700년간 세 개의 나라는 한강 등을 차지하기 위하여 많은 전쟁을 하였습니다. 결국 신라는 삼국을 통일하였습니다. 신라가 삼국을 통일하는 과정을 설명하여 봅시다. 신라가 멸망하고 백제와 고구려과 고구려에서 고구려의 뒤 나당연합을 차지함..당나라와 당나라가 당나라가 멸망\n",
      "약 2천 년 전 한반도에는 신라 ⁇  백제 ⁇  고구려의 세 개의 나라였습니다. 약 700년간 세 개의 나라는 한강 등을 차지하기 위하여 많은 전쟁을 하였습니다. 결국 신라는 삼국을 통일하였습니다. 신라가 삼국을 통일하는 과정을 설명하여 봅시다. 중국과 김 같다. 즉 저 백제로 인해 고구려의 승리되어서 통일했다.5 신라가 멸망해서 신라는당연합으로 이동을 만든다\n",
      "[27500 | 238.82] loss=0.28 avg=1.27\n",
      "[27501 | 238.94] loss=0.57 avg=1.26\n",
      "[27502 | 239.05] loss=1.17 avg=1.26\n",
      "[27503 | 239.17] loss=0.69 avg=1.26\n",
      "[27504 | 239.28] loss=0.57 avg=1.25\n",
      "[27505 | 239.39] loss=1.37 avg=1.25\n",
      "[27506 | 239.50] loss=3.02 avg=1.27\n",
      "[27507 | 239.61] loss=1.14 avg=1.27\n",
      "[27508 | 239.72] loss=1.04 avg=1.26\n",
      "[27509 | 239.85] loss=0.78 avg=1.26\n",
      "[27510 | 239.96] loss=0.48 avg=1.25\n",
      "[27511 | 240.08] loss=1.23 avg=1.25\n",
      "[27512 | 240.20] loss=2.42 avg=1.26\n",
      "[27513 | 240.33] loss=0.24 avg=1.25\n",
      "[27514 | 240.44] loss=0.98 avg=1.25\n",
      "[27515 | 240.56] loss=1.11 avg=1.25\n",
      "[27516 | 240.69] loss=0.23 avg=1.24\n",
      "[27517 | 240.79] loss=0.50 avg=1.23\n",
      "[27518 | 240.91] loss=0.10 avg=1.22\n",
      "[27519 | 241.03] loss=0.39 avg=1.21\n",
      "[27520 | 241.15] loss=0.56 avg=1.21\n",
      "[27521 | 241.25] loss=2.52 avg=1.22\n",
      "[27522 | 241.39] loss=0.56 avg=1.21\n",
      "[27523 | 241.50] loss=1.27 avg=1.21\n",
      "[27524 | 241.62] loss=1.08 avg=1.21\n",
      "[27525 | 241.73] loss=1.47 avg=1.21\n",
      "[27526 | 241.85] loss=0.62 avg=1.21\n",
      "[27527 | 241.96] loss=0.40 avg=1.20\n",
      "[27528 | 242.06] loss=0.70 avg=1.19\n",
      "[27529 | 242.19] loss=3.32 avg=1.22\n",
      "[27530 | 242.30] loss=0.63 avg=1.21\n",
      "[27531 | 242.45] loss=0.36 avg=1.20\n",
      "[27532 | 242.60] loss=3.45 avg=1.22\n",
      "[27533 | 242.75] loss=2.31 avg=1.24\n",
      "[27534 | 242.85] loss=2.77 avg=1.25\n",
      "[27535 | 242.98] loss=5.96 avg=1.30\n",
      "[27536 | 243.12] loss=0.24 avg=1.29\n",
      "[27537 | 243.24] loss=1.09 avg=1.29\n",
      "[27538 | 243.36] loss=0.51 avg=1.28\n",
      "[27539 | 243.48] loss=1.94 avg=1.28\n",
      "[27540 | 243.59] loss=0.85 avg=1.28\n",
      "[27541 | 243.70] loss=1.85 avg=1.29\n",
      "[27542 | 243.82] loss=0.14 avg=1.27\n",
      "[27543 | 243.95] loss=0.41 avg=1.27\n",
      "[27544 | 244.09] loss=2.51 avg=1.28\n",
      "[27545 | 244.20] loss=0.86 avg=1.27\n",
      "[27546 | 244.32] loss=2.43 avg=1.28\n",
      "[27547 | 244.43] loss=0.36 avg=1.28\n",
      "[27548 | 244.55] loss=2.41 avg=1.29\n",
      "[27549 | 244.69] loss=2.26 avg=1.30\n",
      "[27550 | 244.80] loss=1.75 avg=1.30\n",
      "[27551 | 244.91] loss=3.26 avg=1.32\n",
      "[27552 | 245.03] loss=0.61 avg=1.31\n",
      "[27553 | 245.15] loss=0.24 avg=1.30\n",
      "[27554 | 245.26] loss=1.47 avg=1.30\n",
      "[27555 | 245.40] loss=0.51 avg=1.30\n",
      "[27556 | 245.51] loss=1.09 avg=1.29\n",
      "[27557 | 245.62] loss=0.86 avg=1.29\n",
      "[27558 | 245.77] loss=1.00 avg=1.29\n",
      "[27559 | 245.88] loss=1.29 avg=1.29\n",
      "[27560 | 246.00] loss=0.43 avg=1.28\n",
      "[27561 | 246.17] loss=2.69 avg=1.29\n",
      "[27562 | 246.30] loss=1.13 avg=1.29\n",
      "[27563 | 246.43] loss=1.73 avg=1.30\n",
      "[27564 | 246.56] loss=1.76 avg=1.30\n",
      "[27565 | 246.67] loss=1.09 avg=1.30\n",
      "[27566 | 246.80] loss=2.19 avg=1.31\n",
      "[27567 | 246.91] loss=0.92 avg=1.30\n",
      "[27568 | 247.03] loss=0.57 avg=1.30\n",
      "[27569 | 247.16] loss=0.23 avg=1.29\n",
      "[27570 | 247.27] loss=1.19 avg=1.28\n",
      "[27571 | 247.38] loss=1.94 avg=1.29\n",
      "[27572 | 247.48] loss=3.24 avg=1.31\n",
      "[27573 | 247.60] loss=1.15 avg=1.31\n",
      "[27574 | 247.73] loss=1.15 avg=1.31\n",
      "[27575 | 247.85] loss=0.55 avg=1.30\n",
      "[27576 | 247.96] loss=1.58 avg=1.30\n",
      "[27577 | 248.07] loss=1.72 avg=1.31\n",
      "[27578 | 248.18] loss=1.26 avg=1.31\n",
      "[27579 | 248.28] loss=1.07 avg=1.30\n",
      "[27580 | 248.41] loss=1.34 avg=1.30\n",
      "[27581 | 248.52] loss=1.01 avg=1.30\n",
      "[27582 | 248.62] loss=0.96 avg=1.30\n",
      "[27583 | 248.75] loss=1.05 avg=1.30\n",
      "[27584 | 248.86] loss=1.69 avg=1.30\n",
      "[27585 | 248.97] loss=0.37 avg=1.29\n",
      "[27586 | 249.08] loss=1.79 avg=1.30\n",
      "[27587 | 249.20] loss=0.40 avg=1.29\n",
      "[27588 | 249.31] loss=0.55 avg=1.28\n",
      "[27589 | 249.42] loss=1.48 avg=1.28\n",
      "[27590 | 249.54] loss=1.86 avg=1.29\n",
      "[27591 | 249.65] loss=1.13 avg=1.29\n",
      "[27592 | 249.78] loss=0.72 avg=1.28\n",
      "[27593 | 249.93] loss=4.53 avg=1.31\n",
      "[27594 | 250.07] loss=0.32 avg=1.30\n",
      "[27595 | 250.18] loss=1.59 avg=1.30\n",
      "[27596 | 250.29] loss=0.75 avg=1.30\n",
      "[27597 | 250.40] loss=0.72 avg=1.29\n",
      "[27598 | 250.52] loss=0.97 avg=1.29\n",
      "[27599 | 250.64] loss=0.75 avg=1.28\n",
      "Generating samples...\n",
      "Seed text : 정부는 생물 다양성을 높이기 위하여 멸종된 생물들을 복원하는 작업 등 여러 가지 일들을 합니다. 이처럼 생물 다양성을 높이는 일은 중요한데 ⁇  왜 생물 다양성을 높여야 하는 것일까요? 생태계와 생물 다양성에 관한 지식을 활용하여 자세하게 설명하여 봅시다.\n",
      "[10002]\n",
      "정부는 생물 다양성을 높이기 위하여 멸종된 생물들을 복원하는 작업 등 여러 가지 일들을 합니다. 이처럼 생물 다양성을 높이는 일은 중요한데 ⁇  왜 생물 다양성을 높여야 하는 것일까요? 생태계와 생물 다양성에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 생물들을 더 많이 생성하게 유지하며 균형을 유지되기 때문이다. 그렇기 때문에 생태계 평형이 무너질것이다. 따라서 생물 다양성을 좋은 생물\n",
      "정부는 생물 다양성을 높이기 위하여 멸종된 생물들을 복원하는 작업 등 여러 가지 일들을 합니다. 이처럼 생물 다양성을 높이는 일은 중요한데 ⁇  왜 생물 다양성을 높여야 하는 것일까요? 생태계와 생물 다양성에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 생태계 다양성을 높아야하면 먹이사슬이 되기 때문이다. 또 균열이 균형을 유지되지않는다. 또 ⁇  그 생태계에 생태\n",
      "정부는 생물 다양성을 높이기 위하여 멸종된 생물들을 복원하는 작업 등 여러 가지 일들을 합니다. 이처럼 생물 다양성을 높이는 일은 중요한데 ⁇  왜 생물 다양성을 높여야 하는 것일까요? 생태계와 생물 다양성에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 동물들은 생물들을 사슬이 유지되기 때문이다. 그 생물을 잘 먹이사슬이 유지하게 되고 ⁇  생물 다양성을 높이기 때문에 먹이\n",
      "[27600 | 252.59] loss=3.42 avg=1.31\n",
      "[27601 | 252.70] loss=2.24 avg=1.32\n",
      "[27602 | 252.83] loss=0.85 avg=1.31\n",
      "[27603 | 252.93] loss=1.35 avg=1.31\n",
      "[27604 | 253.07] loss=0.28 avg=1.30\n",
      "[27605 | 253.20] loss=1.30 avg=1.30\n",
      "[27606 | 253.32] loss=0.82 avg=1.30\n",
      "[27607 | 253.43] loss=1.72 avg=1.30\n",
      "[27608 | 253.54] loss=1.20 avg=1.30\n",
      "[27609 | 253.65] loss=1.73 avg=1.30\n",
      "[27610 | 253.78] loss=0.32 avg=1.29\n",
      "[27611 | 253.93] loss=0.64 avg=1.29\n",
      "[27612 | 254.06] loss=0.75 avg=1.28\n",
      "[27613 | 254.19] loss=1.39 avg=1.28\n",
      "[27614 | 254.30] loss=1.02 avg=1.28\n",
      "[27615 | 254.47] loss=2.97 avg=1.30\n",
      "[27616 | 254.58] loss=0.41 avg=1.29\n",
      "[27617 | 254.71] loss=2.10 avg=1.30\n",
      "[27618 | 254.81] loss=0.63 avg=1.29\n",
      "[27619 | 254.92] loss=2.27 avg=1.30\n",
      "[27620 | 255.03] loss=0.76 avg=1.29\n",
      "[27621 | 255.18] loss=2.88 avg=1.31\n",
      "[27622 | 255.30] loss=1.81 avg=1.32\n",
      "[27623 | 255.40] loss=0.56 avg=1.31\n",
      "[27624 | 255.51] loss=0.66 avg=1.30\n",
      "[27625 | 255.62] loss=1.90 avg=1.31\n",
      "[27626 | 255.74] loss=0.79 avg=1.30\n",
      "[27627 | 255.85] loss=2.05 avg=1.31\n",
      "[27628 | 255.97] loss=1.72 avg=1.31\n",
      "[27629 | 256.11] loss=1.96 avg=1.32\n",
      "[27630 | 256.22] loss=1.41 avg=1.32\n",
      "[27631 | 256.33] loss=0.86 avg=1.32\n",
      "[27632 | 256.44] loss=0.46 avg=1.31\n",
      "[27633 | 256.54] loss=2.20 avg=1.32\n",
      "[27634 | 256.66] loss=0.30 avg=1.31\n",
      "[27635 | 256.77] loss=3.19 avg=1.33\n",
      "[27636 | 256.88] loss=1.15 avg=1.32\n",
      "[27637 | 256.98] loss=1.39 avg=1.32\n",
      "[27638 | 257.10] loss=3.46 avg=1.35\n",
      "[27639 | 257.21] loss=3.32 avg=1.37\n",
      "[27640 | 257.32] loss=2.25 avg=1.37\n",
      "[27641 | 257.46] loss=1.59 avg=1.38\n",
      "[27642 | 257.59] loss=0.50 avg=1.37\n",
      "[27643 | 257.71] loss=1.21 avg=1.37\n",
      "[27644 | 257.85] loss=0.31 avg=1.36\n",
      "[27645 | 257.97] loss=2.05 avg=1.36\n",
      "[27646 | 258.08] loss=1.36 avg=1.36\n",
      "[27647 | 258.19] loss=1.56 avg=1.36\n",
      "[27648 | 258.31] loss=1.69 avg=1.37\n",
      "[27649 | 258.46] loss=0.91 avg=1.36\n",
      "[27650 | 258.60] loss=4.66 avg=1.40\n",
      "[27651 | 258.71] loss=0.61 avg=1.39\n",
      "[27652 | 258.83] loss=1.11 avg=1.39\n",
      "[27653 | 258.96] loss=1.53 avg=1.39\n",
      "[27654 | 259.09] loss=1.29 avg=1.39\n",
      "[27655 | 259.20] loss=3.56 avg=1.41\n",
      "[27656 | 259.32] loss=0.46 avg=1.40\n",
      "[27657 | 259.43] loss=2.98 avg=1.41\n",
      "[27658 | 259.55] loss=4.58 avg=1.45\n",
      "[27659 | 259.68] loss=1.12 avg=1.44\n",
      "[27660 | 259.79] loss=0.37 avg=1.43\n",
      "[27661 | 259.90] loss=3.21 avg=1.45\n",
      "[27662 | 260.01] loss=2.45 avg=1.46\n",
      "[27663 | 260.12] loss=0.59 avg=1.45\n",
      "[27664 | 260.25] loss=0.79 avg=1.44\n",
      "[27665 | 260.37] loss=1.00 avg=1.44\n",
      "[27666 | 260.50] loss=3.62 avg=1.46\n",
      "[27667 | 260.62] loss=0.17 avg=1.45\n",
      "[27668 | 260.73] loss=0.82 avg=1.44\n",
      "[27669 | 260.83] loss=2.02 avg=1.45\n",
      "[27670 | 260.97] loss=0.99 avg=1.44\n",
      "[27671 | 261.10] loss=0.49 avg=1.43\n",
      "[27672 | 261.22] loss=1.98 avg=1.44\n",
      "[27673 | 261.39] loss=4.72 avg=1.47\n",
      "[27674 | 261.49] loss=0.33 avg=1.46\n",
      "[27675 | 261.61] loss=0.94 avg=1.46\n",
      "[27676 | 261.74] loss=2.28 avg=1.46\n",
      "[27677 | 261.87] loss=1.27 avg=1.46\n",
      "[27678 | 261.98] loss=0.40 avg=1.45\n",
      "[27679 | 262.10] loss=1.08 avg=1.45\n",
      "[27680 | 262.22] loss=0.24 avg=1.44\n",
      "[27681 | 262.32] loss=1.30 avg=1.43\n",
      "[27682 | 262.43] loss=1.66 avg=1.44\n",
      "[27683 | 262.56] loss=0.14 avg=1.42\n",
      "[27684 | 262.67] loss=0.56 avg=1.42\n",
      "[27685 | 262.81] loss=2.48 avg=1.43\n",
      "[27686 | 262.92] loss=0.97 avg=1.42\n",
      "[27687 | 263.07] loss=0.27 avg=1.41\n",
      "[27688 | 263.17] loss=3.27 avg=1.43\n",
      "[27689 | 263.28] loss=1.35 avg=1.43\n",
      "[27690 | 263.41] loss=0.91 avg=1.42\n",
      "[27691 | 263.53] loss=1.37 avg=1.42\n",
      "[27692 | 263.63] loss=1.44 avg=1.42\n",
      "[27693 | 263.74] loss=1.20 avg=1.42\n",
      "[27694 | 263.86] loss=1.67 avg=1.42\n",
      "[27695 | 263.98] loss=2.34 avg=1.43\n",
      "[27696 | 264.10] loss=0.29 avg=1.42\n",
      "[27697 | 264.21] loss=1.19 avg=1.42\n",
      "[27698 | 264.34] loss=0.23 avg=1.41\n",
      "[27699 | 264.45] loss=1.49 avg=1.41\n",
      "Generating samples...\n",
      "Seed text : 무더운 여름날 체온이 높아지면 자연스럽게 땀이 생깁니다. 땀이 난 뒤에는 체온이 낮아지데 그 이유는 무엇일까요? 물질의 상태 변화와 열에너지에 관한 지식을 활용하여 자세하게 설명하여 봅시다.\n",
      "[10001]\n",
      "무더운 여름날 체온이 높아지면 자연스럽게 땀이 생깁니다. 땀이 난 뒤에는 체온이 낮아지데 그 이유는 무엇일까요? 물질의 상태 변화와 열에너지에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 땀이 열을 낮추고 몸의 열을 흡수하여 낮아진다.. 땀이 기체는 열에너지를 가지고 생기면서 ⁇ 문에 체\n",
      "무더운 여름날 체온이 높아지면 자연스럽게 땀이 생깁니다. 땀이 난 뒤에는 체온이 낮아지데 그 이유는 무엇일까요? 물질의 상태 변화와 열에너지에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 액체상태의 열이 식어주는 상태의 우리몸은 액체 상태의 열을 흡수하는 것이 변화 흡수하기 때문에 체온이 낮아\n",
      "무더운 여름날 체온이 높아지면 자연스럽게 땀이 생깁니다. 땀이 난 뒤에는 체온이 낮아지데 그 이유는 무엇일까요? 물질의 상태 변화와 열에너지에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 땀이 수증으로 열을 흡수한다. 체온을 배출하고 온도가 낮아지게 되는면 체온이나 비가 기체의 땀이 낮아져\n",
      "[27700 | 266.36] loss=2.15 avg=1.41\n",
      "[27701 | 266.47] loss=0.60 avg=1.41\n",
      "[27702 | 266.57] loss=0.66 avg=1.40\n",
      "[27703 | 266.70] loss=0.26 avg=1.39\n",
      "[27704 | 266.81] loss=1.68 avg=1.39\n",
      "[27705 | 266.93] loss=0.94 avg=1.39\n",
      "[27706 | 267.03] loss=0.70 avg=1.38\n",
      "[27707 | 267.15] loss=0.85 avg=1.37\n",
      "[27708 | 267.26] loss=0.54 avg=1.37\n",
      "[27709 | 267.40] loss=0.31 avg=1.35\n",
      "[27710 | 267.54] loss=0.06 avg=1.34\n",
      "[27711 | 267.65] loss=0.84 avg=1.34\n",
      "[27712 | 267.77] loss=1.57 avg=1.34\n",
      "[27713 | 267.88] loss=2.79 avg=1.35\n",
      "[27714 | 268.01] loss=0.66 avg=1.35\n",
      "[27715 | 268.11] loss=0.48 avg=1.34\n",
      "[27716 | 268.23] loss=0.60 avg=1.33\n",
      "[27717 | 268.35] loss=0.85 avg=1.33\n",
      "[27718 | 268.47] loss=0.45 avg=1.32\n",
      "[27719 | 268.59] loss=0.67 avg=1.31\n",
      "[27720 | 268.71] loss=0.70 avg=1.30\n",
      "[27721 | 268.84] loss=0.62 avg=1.30\n",
      "[27722 | 268.96] loss=0.16 avg=1.29\n",
      "[27723 | 269.08] loss=0.82 avg=1.28\n",
      "[27724 | 269.21] loss=0.68 avg=1.28\n",
      "[27725 | 269.34] loss=0.38 avg=1.27\n",
      "[27726 | 269.45] loss=1.60 avg=1.27\n",
      "[27727 | 269.56] loss=0.62 avg=1.26\n",
      "[27728 | 269.67] loss=0.62 avg=1.26\n",
      "[27729 | 269.79] loss=2.86 avg=1.27\n",
      "[27730 | 269.90] loss=0.33 avg=1.26\n",
      "[27731 | 270.01] loss=0.81 avg=1.26\n",
      "[27732 | 270.12] loss=0.97 avg=1.26\n",
      "[27733 | 270.22] loss=0.64 avg=1.25\n",
      "[27734 | 270.33] loss=0.74 avg=1.24\n",
      "[27735 | 270.44] loss=0.87 avg=1.24\n",
      "[27736 | 270.58] loss=0.21 avg=1.23\n",
      "[27737 | 270.69] loss=1.31 avg=1.23\n",
      "[27738 | 270.81] loss=2.64 avg=1.25\n",
      "[27739 | 270.92] loss=0.92 avg=1.24\n",
      "[27740 | 271.04] loss=0.58 avg=1.24\n",
      "[27741 | 271.16] loss=0.66 avg=1.23\n",
      "[27742 | 271.27] loss=0.56 avg=1.22\n",
      "[27743 | 271.39] loss=0.59 avg=1.22\n",
      "[27744 | 271.51] loss=0.77 avg=1.21\n",
      "[27745 | 271.63] loss=1.89 avg=1.22\n",
      "[27746 | 271.77] loss=0.15 avg=1.21\n",
      "[27747 | 271.88] loss=0.51 avg=1.20\n",
      "[27748 | 272.01] loss=0.84 avg=1.20\n",
      "[27749 | 272.12] loss=0.82 avg=1.19\n",
      "[27750 | 272.23] loss=2.77 avg=1.21\n",
      "[27751 | 272.36] loss=0.42 avg=1.20\n",
      "[27752 | 272.47] loss=3.37 avg=1.22\n",
      "[27753 | 272.60] loss=1.04 avg=1.22\n",
      "[27754 | 272.72] loss=0.32 avg=1.21\n",
      "[27755 | 272.84] loss=2.06 avg=1.22\n",
      "[27756 | 272.97] loss=0.85 avg=1.22\n",
      "[27757 | 273.07] loss=1.17 avg=1.22\n",
      "[27758 | 273.20] loss=0.99 avg=1.21\n",
      "[27759 | 273.35] loss=0.73 avg=1.21\n",
      "[27760 | 273.47] loss=0.84 avg=1.21\n",
      "[27761 | 273.58] loss=0.99 avg=1.20\n",
      "[27762 | 273.72] loss=0.13 avg=1.19\n",
      "[27763 | 273.86] loss=1.05 avg=1.19\n",
      "[27764 | 273.97] loss=1.04 avg=1.19\n",
      "[27765 | 274.09] loss=1.27 avg=1.19\n",
      "[27766 | 274.20] loss=1.14 avg=1.19\n",
      "[27767 | 274.33] loss=0.98 avg=1.19\n",
      "[27768 | 274.44] loss=1.28 avg=1.19\n",
      "[27769 | 274.56] loss=0.69 avg=1.18\n",
      "[27770 | 274.67] loss=0.71 avg=1.18\n",
      "[27771 | 274.78] loss=2.87 avg=1.20\n",
      "[27772 | 274.90] loss=0.95 avg=1.19\n",
      "[27773 | 275.00] loss=0.59 avg=1.19\n",
      "[27774 | 275.13] loss=0.78 avg=1.18\n",
      "[27775 | 275.26] loss=1.05 avg=1.18\n",
      "[27776 | 275.37] loss=1.80 avg=1.19\n",
      "[27777 | 275.47] loss=2.24 avg=1.20\n",
      "[27778 | 275.58] loss=0.59 avg=1.19\n",
      "[27779 | 275.71] loss=0.77 avg=1.19\n",
      "[27780 | 275.83] loss=1.11 avg=1.19\n",
      "[27781 | 275.96] loss=1.65 avg=1.19\n",
      "[27782 | 276.10] loss=4.86 avg=1.23\n",
      "[27783 | 276.23] loss=0.70 avg=1.22\n",
      "[27784 | 276.34] loss=1.45 avg=1.23\n",
      "[27785 | 276.44] loss=0.38 avg=1.22\n",
      "[27786 | 276.56] loss=0.60 avg=1.21\n",
      "[27787 | 276.68] loss=0.23 avg=1.20\n",
      "[27788 | 276.79] loss=1.02 avg=1.20\n",
      "[27789 | 276.91] loss=1.22 avg=1.20\n",
      "[27790 | 277.02] loss=0.92 avg=1.20\n",
      "[27791 | 277.13] loss=1.77 avg=1.20\n",
      "[27792 | 277.25] loss=1.59 avg=1.21\n",
      "[27793 | 277.38] loss=1.91 avg=1.21\n",
      "[27794 | 277.49] loss=2.04 avg=1.22\n",
      "[27795 | 277.61] loss=0.40 avg=1.21\n",
      "[27796 | 277.72] loss=0.57 avg=1.21\n",
      "[27797 | 277.83] loss=0.58 avg=1.20\n",
      "[27798 | 277.96] loss=2.66 avg=1.22\n",
      "[27799 | 278.07] loss=3.73 avg=1.24\n",
      "Generating samples...\n",
      "Seed text : 보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다.\n",
      "[]\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 보온병에너지가 잘 빠져나가지 못하게 한다....................\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 보온병이 열이 유지될수있다......................\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 따뜻한 바람을 보온병은 열이 유지될 때 열이 유지된다. 그래서 열이 따뜻하여 단열이 잘 흡수하기 때문이다..\n",
      "[27800 | 280.08] loss=1.93 avg=1.25\n",
      "[27801 | 280.20] loss=1.42 avg=1.25\n",
      "[27802 | 280.32] loss=0.14 avg=1.24\n",
      "[27803 | 280.45] loss=1.82 avg=1.24\n",
      "[27804 | 280.57] loss=3.21 avg=1.26\n",
      "[27805 | 280.69] loss=0.70 avg=1.26\n",
      "[27806 | 280.80] loss=1.15 avg=1.26\n",
      "[27807 | 280.90] loss=1.82 avg=1.26\n",
      "[27808 | 281.04] loss=1.18 avg=1.26\n",
      "[27809 | 281.14] loss=3.67 avg=1.29\n",
      "[27810 | 281.25] loss=0.35 avg=1.28\n",
      "[27811 | 281.39] loss=0.58 avg=1.27\n",
      "[27812 | 281.51] loss=3.41 avg=1.29\n",
      "[27813 | 281.63] loss=3.62 avg=1.31\n",
      "[27814 | 281.75] loss=1.42 avg=1.32\n",
      "[27815 | 281.87] loss=0.60 avg=1.31\n",
      "[27816 | 281.97] loss=3.83 avg=1.33\n",
      "[27817 | 282.10] loss=0.11 avg=1.32\n",
      "[27818 | 282.21] loss=1.48 avg=1.32\n",
      "[27819 | 282.33] loss=1.28 avg=1.32\n",
      "[27820 | 282.44] loss=1.98 avg=1.33\n",
      "[27821 | 282.56] loss=0.31 avg=1.32\n",
      "[27822 | 282.69] loss=0.59 avg=1.31\n",
      "[27823 | 282.79] loss=0.57 avg=1.30\n",
      "[27824 | 282.92] loss=1.60 avg=1.31\n",
      "[27825 | 283.03] loss=0.87 avg=1.30\n",
      "[27826 | 283.14] loss=0.91 avg=1.30\n",
      "[27827 | 283.25] loss=1.49 avg=1.30\n",
      "[27828 | 283.36] loss=0.35 avg=1.29\n",
      "[27829 | 283.49] loss=0.57 avg=1.28\n",
      "[27830 | 283.60] loss=2.49 avg=1.30\n",
      "[27831 | 283.73] loss=4.04 avg=1.32\n",
      "[27832 | 283.85] loss=1.00 avg=1.32\n",
      "[27833 | 283.98] loss=1.18 avg=1.32\n",
      "[27834 | 284.10] loss=1.02 avg=1.32\n",
      "[27835 | 284.21] loss=1.07 avg=1.31\n",
      "[27836 | 284.31] loss=1.97 avg=1.32\n",
      "[27837 | 284.42] loss=2.64 avg=1.33\n",
      "[27838 | 284.53] loss=0.52 avg=1.33\n",
      "[27839 | 284.63] loss=1.08 avg=1.32\n",
      "[27840 | 284.74] loss=1.47 avg=1.32\n",
      "[27841 | 284.87] loss=0.53 avg=1.32\n",
      "[27842 | 285.00] loss=2.25 avg=1.33\n",
      "[27843 | 285.10] loss=2.15 avg=1.33\n",
      "[27844 | 285.22] loss=0.52 avg=1.33\n",
      "[27845 | 285.34] loss=1.68 avg=1.33\n",
      "[27846 | 285.46] loss=0.14 avg=1.32\n",
      "[27847 | 285.59] loss=3.19 avg=1.34\n",
      "[27848 | 285.69] loss=0.95 avg=1.33\n",
      "[27849 | 285.80] loss=0.59 avg=1.32\n",
      "[27850 | 285.92] loss=1.36 avg=1.33\n",
      "[27851 | 286.05] loss=1.14 avg=1.32\n",
      "[27852 | 286.16] loss=0.79 avg=1.32\n",
      "[27853 | 286.28] loss=0.37 avg=1.31\n",
      "[27854 | 286.39] loss=1.25 avg=1.31\n",
      "[27855 | 286.50] loss=0.97 avg=1.30\n",
      "[27856 | 286.63] loss=1.86 avg=1.31\n",
      "[27857 | 286.75] loss=0.30 avg=1.30\n",
      "[27858 | 286.88] loss=1.40 avg=1.30\n",
      "[27859 | 287.03] loss=0.26 avg=1.29\n",
      "[27860 | 287.14] loss=2.34 avg=1.30\n",
      "[27861 | 287.25] loss=0.71 avg=1.30\n",
      "[27862 | 287.36] loss=0.41 avg=1.29\n",
      "[27863 | 287.48] loss=0.65 avg=1.28\n",
      "[27864 | 287.60] loss=0.34 avg=1.27\n",
      "[27865 | 287.96] loss=0.08 avg=1.26\n",
      "[27866 | 288.07] loss=0.64 avg=1.25\n",
      "[27867 | 288.20] loss=0.83 avg=1.25\n",
      "[27868 | 288.31] loss=1.14 avg=1.25\n",
      "[27869 | 288.41] loss=0.30 avg=1.24\n",
      "[27870 | 288.54] loss=1.01 avg=1.24\n",
      "[27871 | 288.68] loss=1.73 avg=1.24\n",
      "[27872 | 288.78] loss=1.14 avg=1.24\n",
      "[27873 | 288.89] loss=1.43 avg=1.24\n",
      "[27874 | 289.00] loss=1.30 avg=1.24\n",
      "[27875 | 289.11] loss=3.53 avg=1.26\n",
      "[27876 | 289.23] loss=0.89 avg=1.26\n",
      "[27877 | 289.36] loss=1.87 avg=1.27\n",
      "[27878 | 289.47] loss=0.94 avg=1.26\n",
      "[27879 | 289.58] loss=2.04 avg=1.27\n",
      "[27880 | 289.68] loss=2.21 avg=1.28\n",
      "[27881 | 289.81] loss=2.72 avg=1.30\n",
      "[27882 | 289.93] loss=0.42 avg=1.29\n",
      "[27883 | 290.04] loss=1.20 avg=1.29\n",
      "[27884 | 290.16] loss=0.93 avg=1.28\n",
      "[27885 | 290.29] loss=1.43 avg=1.28\n",
      "[27886 | 290.40] loss=2.95 avg=1.30\n",
      "[27887 | 290.52] loss=0.66 avg=1.29\n",
      "[27888 | 290.63] loss=2.70 avg=1.31\n",
      "[27889 | 290.73] loss=1.22 avg=1.31\n",
      "[27890 | 290.84] loss=1.06 avg=1.30\n",
      "[27891 | 290.95] loss=1.02 avg=1.30\n",
      "[27892 | 291.07] loss=0.61 avg=1.29\n",
      "[27893 | 291.19] loss=0.55 avg=1.29\n",
      "[27894 | 291.31] loss=1.06 avg=1.29\n",
      "[27895 | 291.43] loss=2.49 avg=1.30\n",
      "[27896 | 291.55] loss=0.63 avg=1.29\n",
      "[27897 | 291.67] loss=1.00 avg=1.29\n",
      "[27898 | 291.77] loss=2.52 avg=1.30\n",
      "[27899 | 291.90] loss=0.74 avg=1.29\n",
      "Generating samples...\n",
      "Seed text : 뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요?\n",
      "[10001]\n",
      "뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요? 다리미가 뜨거워졌기때문이다. 결국 다리미판은 물로 물에 다리미가 그 불은 전향 온도 다리미판에\n",
      "뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요? 다리미의 온도가 높은 온도가 타게 되면 천에 온도가 타기구를 다리미로 인해 불이 붙는다. 다리미판이 타게 된\n",
      "뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요? 온도가 되어서 다리미판은 타게 되었다. 따라서 열에너지가 탈 물질과 나무 국자운동이 다었기 때문에. 그런데 다리\n",
      "[27900 | 293.80] loss=1.38 avg=1.30\n",
      "[27901 | 293.92] loss=0.32 avg=1.29\n",
      "[27902 | 294.03] loss=1.83 avg=1.29\n",
      "[27903 | 294.17] loss=0.38 avg=1.28\n",
      "[27904 | 294.29] loss=1.07 avg=1.28\n",
      "[27905 | 294.40] loss=2.87 avg=1.30\n",
      "[27906 | 294.54] loss=1.51 avg=1.30\n",
      "[27907 | 294.65] loss=0.67 avg=1.29\n",
      "[27908 | 294.77] loss=1.12 avg=1.29\n",
      "[27909 | 294.89] loss=1.60 avg=1.29\n",
      "[27910 | 295.02] loss=3.02 avg=1.31\n",
      "[27911 | 295.19] loss=0.37 avg=1.30\n",
      "[27912 | 295.30] loss=0.79 avg=1.30\n",
      "[27913 | 295.43] loss=3.15 avg=1.31\n",
      "[27914 | 295.54] loss=1.08 avg=1.31\n",
      "[27915 | 295.66] loss=0.65 avg=1.31\n",
      "[27916 | 295.79] loss=0.68 avg=1.30\n",
      "[27917 | 295.94] loss=0.29 avg=1.29\n",
      "[27918 | 296.06] loss=0.34 avg=1.28\n",
      "[27919 | 296.19] loss=0.57 avg=1.27\n",
      "[27920 | 296.30] loss=1.56 avg=1.28\n",
      "[27921 | 296.42] loss=1.06 avg=1.27\n",
      "[27922 | 296.52] loss=1.64 avg=1.28\n",
      "[27923 | 296.72] loss=3.69 avg=1.30\n",
      "[27924 | 296.84] loss=0.26 avg=1.29\n",
      "[27925 | 296.96] loss=1.58 avg=1.29\n",
      "[27926 | 297.08] loss=1.09 avg=1.29\n",
      "[27927 | 297.20] loss=1.35 avg=1.29\n",
      "[27928 | 297.31] loss=1.05 avg=1.29\n",
      "[27929 | 297.42] loss=1.19 avg=1.29\n",
      "[27930 | 297.54] loss=0.47 avg=1.28\n",
      "[27931 | 297.66] loss=1.24 avg=1.28\n",
      "[27932 | 297.79] loss=2.01 avg=1.29\n",
      "[27933 | 297.91] loss=1.48 avg=1.29\n",
      "[27934 | 298.03] loss=0.38 avg=1.28\n",
      "[27935 | 298.14] loss=0.50 avg=1.27\n",
      "[27936 | 298.27] loss=0.34 avg=1.26\n",
      "[27937 | 298.38] loss=2.01 avg=1.27\n",
      "[27938 | 298.50] loss=0.85 avg=1.27\n",
      "[27939 | 298.61] loss=1.75 avg=1.27\n",
      "[27940 | 298.75] loss=0.19 avg=1.26\n",
      "[27941 | 298.85] loss=0.90 avg=1.26\n",
      "[27942 | 298.97] loss=0.62 avg=1.25\n",
      "[27943 | 299.08] loss=0.74 avg=1.25\n",
      "[27944 | 299.21] loss=3.08 avg=1.26\n",
      "[27945 | 299.36] loss=0.07 avg=1.25\n",
      "[27946 | 299.47] loss=1.44 avg=1.25\n",
      "[27947 | 299.59] loss=0.89 avg=1.25\n",
      "[27948 | 299.72] loss=2.33 avg=1.26\n",
      "[27949 | 299.82] loss=0.72 avg=1.26\n",
      "[27950 | 299.95] loss=0.27 avg=1.25\n",
      "[27951 | 300.07] loss=0.31 avg=1.24\n",
      "[27952 | 300.18] loss=0.80 avg=1.23\n",
      "[27953 | 300.32] loss=1.70 avg=1.24\n",
      "[27954 | 300.45] loss=1.29 avg=1.24\n",
      "[27955 | 300.58] loss=1.01 avg=1.23\n",
      "[27956 | 300.69] loss=1.38 avg=1.24\n",
      "[27957 | 300.83] loss=1.41 avg=1.24\n",
      "[27958 | 300.96] loss=1.43 avg=1.24\n",
      "[27959 | 301.08] loss=0.53 avg=1.23\n",
      "[27960 | 301.21] loss=0.30 avg=1.22\n",
      "[27961 | 301.35] loss=0.43 avg=1.22\n",
      "[27962 | 301.46] loss=0.84 avg=1.21\n",
      "[27963 | 301.58] loss=0.54 avg=1.20\n",
      "[27964 | 301.71] loss=1.61 avg=1.21\n",
      "[27965 | 301.88] loss=0.93 avg=1.21\n",
      "[27966 | 301.99] loss=1.92 avg=1.21\n",
      "[27967 | 302.10] loss=0.77 avg=1.21\n",
      "[27968 | 302.24] loss=0.56 avg=1.20\n",
      "[27969 | 302.37] loss=0.09 avg=1.19\n",
      "[27970 | 302.49] loss=1.02 avg=1.19\n",
      "[27971 | 302.62] loss=0.83 avg=1.19\n",
      "[27972 | 302.74] loss=1.10 avg=1.19\n",
      "[27973 | 302.86] loss=0.64 avg=1.18\n",
      "[27974 | 302.98] loss=0.07 avg=1.17\n",
      "[27975 | 303.09] loss=2.05 avg=1.18\n",
      "[27976 | 303.20] loss=0.11 avg=1.17\n",
      "[27977 | 303.31] loss=0.27 avg=1.16\n",
      "[27978 | 303.45] loss=0.12 avg=1.15\n",
      "[27979 | 303.57] loss=1.14 avg=1.15\n",
      "[27980 | 303.70] loss=1.76 avg=1.15\n",
      "[27981 | 303.82] loss=1.02 avg=1.15\n",
      "[27982 | 303.93] loss=1.00 avg=1.15\n",
      "[27983 | 304.05] loss=0.58 avg=1.14\n",
      "[27984 | 304.16] loss=0.45 avg=1.14\n",
      "[27985 | 304.27] loss=1.06 avg=1.14\n",
      "[27986 | 304.38] loss=0.89 avg=1.13\n",
      "[27987 | 304.50] loss=0.44 avg=1.13\n",
      "[27988 | 304.61] loss=0.89 avg=1.13\n",
      "[27989 | 304.74] loss=1.41 avg=1.13\n",
      "[27990 | 304.85] loss=1.06 avg=1.13\n",
      "[27991 | 304.97] loss=0.41 avg=1.12\n",
      "[27992 | 305.08] loss=2.24 avg=1.13\n",
      "[27993 | 305.20] loss=0.70 avg=1.13\n",
      "[27994 | 305.36] loss=1.10 avg=1.13\n",
      "[27995 | 305.48] loss=0.60 avg=1.12\n",
      "[27996 | 305.62] loss=3.01 avg=1.14\n",
      "[27997 | 305.74] loss=0.45 avg=1.13\n",
      "[27998 | 305.85] loss=0.70 avg=1.13\n",
      "[27999 | 305.97] loss=1.37 avg=1.13\n",
      "Saving checkpoint/transfer3/model-28000\n",
      "Generating samples...\n",
      "Seed text : 여름철 밖에 둔 플라스틱 병을 냉장고에 넣었을 때 찌그러지는 이유는 무엇일까요?\n",
      "[10000]\n",
      "여름철 밖에 둔 플라스틱 병을 냉장고에 넣었을 때 찌그러지는 이유는 무엇일까요? 냉장고보다 더 높은곳의 온도가 높은 곳에 냉장고 안의 공기의 부피가 커지기 때문에 부피가 점점 커지게 된다\n",
      "여름철 밖에 둔 플라스틱 병을 냉장고에 넣었을 때 찌그러지는 이유는 무엇일까요? 온도보다 온도가 낮아지고 냉장리보다 커져서 차가워져서 플라스틱병의 공기는 온도가 낮아지기 때문이다. 지구 안의 공\n",
      "여름철 밖에 둔 플라스틱 병을 냉장고에 넣었을 때 찌그러지는 이유는 무엇일까요? 온도가 급격히 작아지기 때문이다. 수축된다. 기온들이 냉장고 안의 온도가 높은쪽으로 더 높아져 수축한다.\n",
      "[28000 | 310.78] loss=2.05 avg=1.14\n",
      "[28001 | 310.91] loss=1.35 avg=1.14\n",
      "[28002 | 311.03] loss=0.44 avg=1.14\n",
      "[28003 | 311.16] loss=0.16 avg=1.13\n",
      "[28004 | 311.31] loss=1.43 avg=1.13\n",
      "[28005 | 311.44] loss=0.79 avg=1.13\n",
      "[28006 | 311.56] loss=2.46 avg=1.14\n",
      "[28007 | 311.70] loss=0.32 avg=1.13\n",
      "[28008 | 311.82] loss=4.23 avg=1.16\n",
      "[28009 | 311.93] loss=0.91 avg=1.16\n",
      "[28010 | 312.03] loss=1.63 avg=1.16\n",
      "[28011 | 312.14] loss=2.13 avg=1.17\n",
      "[28012 | 312.27] loss=0.24 avg=1.16\n",
      "[28013 | 312.39] loss=2.76 avg=1.18\n",
      "[28014 | 312.50] loss=0.07 avg=1.17\n",
      "[28015 | 312.63] loss=0.36 avg=1.16\n",
      "[28016 | 312.74] loss=2.36 avg=1.17\n",
      "[28017 | 312.86] loss=0.68 avg=1.17\n",
      "[28018 | 312.98] loss=0.21 avg=1.16\n",
      "[28019 | 313.12] loss=0.24 avg=1.15\n",
      "[28020 | 313.26] loss=0.76 avg=1.15\n",
      "[28021 | 313.37] loss=0.33 avg=1.14\n",
      "[28022 | 313.50] loss=1.80 avg=1.14\n",
      "[28023 | 313.64] loss=0.52 avg=1.14\n",
      "[28024 | 313.75] loss=1.32 avg=1.14\n",
      "[28025 | 313.87] loss=0.56 avg=1.13\n",
      "[28026 | 314.01] loss=0.44 avg=1.13\n",
      "[28027 | 314.12] loss=1.50 avg=1.13\n",
      "[28028 | 314.25] loss=1.01 avg=1.13\n",
      "[28029 | 314.37] loss=0.78 avg=1.13\n",
      "[28030 | 314.51] loss=0.45 avg=1.12\n",
      "[28031 | 314.64] loss=1.62 avg=1.12\n",
      "[28032 | 314.74] loss=0.36 avg=1.12\n",
      "[28033 | 314.86] loss=0.28 avg=1.11\n",
      "[28034 | 314.98] loss=2.67 avg=1.12\n",
      "[28035 | 315.10] loss=3.15 avg=1.14\n",
      "[28036 | 315.22] loss=0.91 avg=1.14\n",
      "[28037 | 315.35] loss=0.69 avg=1.14\n",
      "[28038 | 315.48] loss=0.62 avg=1.13\n",
      "[28039 | 315.60] loss=1.12 avg=1.13\n",
      "[28040 | 315.71] loss=1.69 avg=1.14\n",
      "[28041 | 315.83] loss=0.83 avg=1.13\n",
      "[28042 | 315.94] loss=0.97 avg=1.13\n",
      "[28043 | 316.06] loss=1.11 avg=1.13\n",
      "[28044 | 316.19] loss=0.80 avg=1.13\n",
      "[28045 | 316.31] loss=0.60 avg=1.12\n",
      "[28046 | 316.41] loss=2.14 avg=1.13\n",
      "[28047 | 316.52] loss=2.22 avg=1.15\n",
      "[28048 | 316.64] loss=2.02 avg=1.15\n",
      "[28049 | 316.77] loss=0.47 avg=1.15\n",
      "[28050 | 316.91] loss=3.32 avg=1.17\n",
      "[28051 | 317.03] loss=1.83 avg=1.18\n",
      "[28052 | 317.15] loss=3.51 avg=1.20\n",
      "[28053 | 317.28] loss=1.25 avg=1.20\n",
      "[28054 | 317.40] loss=0.89 avg=1.20\n",
      "[28055 | 317.52] loss=2.51 avg=1.21\n",
      "[28056 | 317.65] loss=0.23 avg=1.20\n",
      "[28057 | 317.79] loss=0.36 avg=1.19\n",
      "[28058 | 317.90] loss=1.10 avg=1.19\n",
      "[28059 | 318.03] loss=0.38 avg=1.18\n",
      "[28060 | 318.15] loss=1.19 avg=1.18\n",
      "[28061 | 318.26] loss=0.76 avg=1.18\n",
      "[28062 | 318.37] loss=1.44 avg=1.18\n",
      "[28063 | 318.51] loss=2.38 avg=1.19\n",
      "[28064 | 318.64] loss=1.99 avg=1.20\n",
      "[28065 | 318.78] loss=0.36 avg=1.19\n",
      "[28066 | 318.89] loss=1.95 avg=1.20\n",
      "[28067 | 319.00] loss=2.37 avg=1.21\n",
      "[28068 | 319.13] loss=1.16 avg=1.21\n",
      "[28069 | 319.24] loss=2.22 avg=1.22\n",
      "[28070 | 319.36] loss=0.44 avg=1.21\n",
      "[28071 | 319.48] loss=0.47 avg=1.21\n",
      "[28072 | 319.60] loss=2.63 avg=1.22\n",
      "[28073 | 319.72] loss=0.31 avg=1.21\n",
      "[28074 | 319.84] loss=1.56 avg=1.21\n",
      "[28075 | 319.95] loss=1.48 avg=1.22\n",
      "[28076 | 320.07] loss=0.57 avg=1.21\n",
      "[28077 | 320.18] loss=0.78 avg=1.21\n",
      "[28078 | 320.30] loss=0.26 avg=1.20\n",
      "[28079 | 320.42] loss=0.57 avg=1.19\n",
      "[28080 | 320.54] loss=2.11 avg=1.20\n",
      "[28081 | 320.66] loss=0.53 avg=1.19\n",
      "[28082 | 320.77] loss=1.21 avg=1.19\n",
      "[28083 | 320.90] loss=1.68 avg=1.20\n",
      "[28084 | 321.02] loss=1.14 avg=1.20\n",
      "[28085 | 321.13] loss=0.71 avg=1.19\n",
      "[28086 | 321.26] loss=0.93 avg=1.19\n",
      "[28087 | 321.39] loss=1.57 avg=1.19\n",
      "[28088 | 321.50] loss=0.86 avg=1.19\n",
      "[28089 | 321.61] loss=1.00 avg=1.19\n",
      "[28090 | 321.73] loss=0.36 avg=1.18\n",
      "[28091 | 321.84] loss=1.15 avg=1.18\n",
      "[28092 | 321.97] loss=0.43 avg=1.17\n",
      "[28093 | 322.09] loss=0.54 avg=1.17\n",
      "[28094 | 322.19] loss=0.63 avg=1.16\n",
      "[28095 | 322.30] loss=2.51 avg=1.17\n",
      "[28096 | 322.41] loss=0.32 avg=1.17\n",
      "[28097 | 322.53] loss=2.27 avg=1.18\n",
      "[28098 | 322.66] loss=3.89 avg=1.20\n",
      "[28099 | 322.76] loss=2.53 avg=1.22\n",
      "Generating samples...\n",
      "Seed text : 분해자 버섯 곰팡이가 지구에서 사라지면 어떤 일이 생길까요?\n",
      "[10000 10002]\n",
      "분해자 버섯 곰팡이가 지구에서 사라지면 어떤 일이 생길까요? 동물과 생태계에서는 생태계가 파괴나나 생물들이 없어지게 된다. 분해를 분해되지 않는다. 생산자이 부족해주는 분해자가\n",
      "분해자 버섯 곰팡이가 지구에서 사라지면 어떤 일이 생길까요? 시체나 생산으로 인해 생태계가 파괴해주는 생물들이 죽게 된다면 지구의 동물도 없다면 생물들은 분해되지 않는다. 분해할수\n",
      "분해자 버섯 곰팡이가 지구에서 사라지면 어떤 일이 생길까요? 생물이 사체들이 사라져 시체는 풀이 파괴될 것이다. 또 없어질 것 같다. 지구의 먹이 일어나지 않기 때문이다. 동\n",
      "[28100 | 324.65] loss=0.94 avg=1.21\n",
      "[28101 | 324.78] loss=1.40 avg=1.22\n",
      "[28102 | 324.90] loss=0.78 avg=1.21\n",
      "[28103 | 325.00] loss=2.43 avg=1.22\n",
      "[28104 | 325.11] loss=0.68 avg=1.22\n",
      "[28105 | 325.22] loss=1.87 avg=1.23\n",
      "[28106 | 325.36] loss=0.51 avg=1.22\n",
      "[28107 | 325.50] loss=3.15 avg=1.24\n",
      "[28108 | 325.62] loss=0.86 avg=1.23\n",
      "[28109 | 325.74] loss=0.08 avg=1.22\n",
      "[28110 | 325.85] loss=0.70 avg=1.22\n",
      "[28111 | 325.96] loss=1.43 avg=1.22\n",
      "[28112 | 326.08] loss=1.14 avg=1.22\n",
      "[28113 | 326.18] loss=0.49 avg=1.21\n",
      "[28114 | 326.30] loss=0.54 avg=1.20\n",
      "[28115 | 326.43] loss=2.73 avg=1.22\n",
      "[28116 | 326.55] loss=1.05 avg=1.22\n",
      "[28117 | 326.67] loss=0.48 avg=1.21\n",
      "[28118 | 326.78] loss=1.99 avg=1.22\n",
      "[28119 | 326.93] loss=2.25 avg=1.23\n",
      "[28120 | 327.05] loss=0.41 avg=1.22\n",
      "[28121 | 327.16] loss=1.54 avg=1.22\n",
      "[28122 | 327.26] loss=1.96 avg=1.23\n",
      "[28123 | 327.38] loss=0.57 avg=1.22\n",
      "[28124 | 327.51] loss=0.74 avg=1.22\n",
      "[28125 | 327.63] loss=0.46 avg=1.21\n",
      "[28126 | 327.75] loss=1.79 avg=1.22\n",
      "[28127 | 327.85] loss=0.72 avg=1.21\n",
      "[28128 | 327.96] loss=0.63 avg=1.21\n",
      "[28129 | 328.09] loss=0.46 avg=1.20\n",
      "[28130 | 328.30] loss=4.85 avg=1.24\n",
      "[28131 | 328.41] loss=1.20 avg=1.24\n",
      "[28132 | 328.52] loss=0.82 avg=1.23\n",
      "[28133 | 328.63] loss=1.91 avg=1.24\n",
      "[28134 | 328.75] loss=1.63 avg=1.24\n",
      "[28135 | 328.88] loss=0.73 avg=1.24\n",
      "[28136 | 329.02] loss=0.93 avg=1.23\n",
      "[28137 | 329.19] loss=0.50 avg=1.23\n",
      "[28138 | 329.32] loss=1.15 avg=1.23\n",
      "[28139 | 329.43] loss=1.36 avg=1.23\n",
      "[28140 | 329.53] loss=1.54 avg=1.23\n",
      "[28141 | 329.65] loss=0.62 avg=1.22\n",
      "[28142 | 329.77] loss=0.55 avg=1.22\n",
      "[28143 | 329.90] loss=1.18 avg=1.22\n",
      "[28144 | 330.02] loss=1.26 avg=1.22\n",
      "[28145 | 330.13] loss=1.01 avg=1.22\n",
      "[28146 | 330.25] loss=0.78 avg=1.21\n",
      "[28147 | 330.37] loss=2.28 avg=1.22\n",
      "[28148 | 330.49] loss=0.96 avg=1.22\n",
      "[28149 | 330.61] loss=1.06 avg=1.22\n",
      "[28150 | 330.74] loss=0.32 avg=1.21\n",
      "[28151 | 330.87] loss=0.63 avg=1.20\n",
      "[28152 | 330.99] loss=1.20 avg=1.20\n",
      "[28153 | 331.10] loss=0.43 avg=1.19\n",
      "[28154 | 331.20] loss=1.14 avg=1.19\n",
      "[28155 | 331.32] loss=1.03 avg=1.19\n",
      "[28156 | 331.43] loss=2.25 avg=1.20\n",
      "[28157 | 331.53] loss=1.31 avg=1.20\n",
      "[28158 | 331.64] loss=2.89 avg=1.22\n",
      "[28159 | 331.78] loss=0.60 avg=1.22\n",
      "[28160 | 331.90] loss=1.84 avg=1.22\n",
      "[28161 | 332.02] loss=0.98 avg=1.22\n",
      "[28162 | 332.14] loss=0.39 avg=1.21\n",
      "[28163 | 332.26] loss=0.60 avg=1.20\n",
      "[28164 | 332.36] loss=1.57 avg=1.21\n",
      "[28165 | 332.49] loss=0.11 avg=1.20\n",
      "[28166 | 332.60] loss=1.03 avg=1.20\n",
      "[28167 | 332.71] loss=1.10 avg=1.19\n",
      "[28168 | 332.83] loss=0.34 avg=1.19\n",
      "[28169 | 332.94] loss=0.93 avg=1.18\n",
      "[28170 | 333.05] loss=1.39 avg=1.19\n",
      "[28171 | 333.18] loss=2.14 avg=1.20\n",
      "[28172 | 333.30] loss=1.26 avg=1.20\n",
      "[28173 | 333.41] loss=0.81 avg=1.19\n",
      "[28174 | 333.56] loss=2.44 avg=1.20\n",
      "[28175 | 333.67] loss=0.97 avg=1.20\n",
      "[28176 | 333.79] loss=1.48 avg=1.21\n",
      "[28177 | 333.90] loss=2.05 avg=1.21\n",
      "[28178 | 334.03] loss=1.22 avg=1.21\n",
      "[28179 | 334.13] loss=1.65 avg=1.22\n",
      "[28180 | 334.24] loss=3.05 avg=1.24\n",
      "[28181 | 334.38] loss=0.61 avg=1.23\n",
      "[28182 | 334.50] loss=0.83 avg=1.23\n",
      "[28183 | 334.61] loss=1.90 avg=1.23\n",
      "[28184 | 334.73] loss=1.58 avg=1.24\n",
      "[28185 | 334.84] loss=2.45 avg=1.25\n",
      "[28186 | 334.95] loss=3.94 avg=1.28\n",
      "[28187 | 335.08] loss=0.98 avg=1.27\n",
      "[28188 | 335.18] loss=1.73 avg=1.28\n",
      "[28189 | 335.29] loss=1.64 avg=1.28\n",
      "[28190 | 335.40] loss=1.09 avg=1.28\n",
      "[28191 | 335.52] loss=0.19 avg=1.27\n",
      "[28192 | 335.66] loss=0.62 avg=1.26\n",
      "[28193 | 335.78] loss=0.89 avg=1.26\n",
      "[28194 | 335.92] loss=0.97 avg=1.25\n",
      "[28195 | 336.05] loss=1.55 avg=1.26\n",
      "[28196 | 336.16] loss=2.27 avg=1.27\n",
      "[28197 | 336.27] loss=1.51 avg=1.27\n",
      "[28198 | 336.38] loss=0.47 avg=1.26\n",
      "[28199 | 336.49] loss=1.79 avg=1.27\n",
      "Generating samples...\n",
      "Seed text : 태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요?\n",
      "[]\n",
      "태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요? 고갈되지 않기 때문이다. 한정되어 있습니다......................\n",
      "태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요? 전기가 한정되어있다. 고갈되지 않기 때문에 자원이기때문에. 하지만 태양광광 발전 ⁇  그리고 석탄이나 석유와 같은 화\n",
      "태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요? 미래에너지를 더 높면서 고갈되지않고 ⁇  태양광 발전에너지 되는 이유는 무엇일까요?? 빛에너지를 비해\n",
      "[28200 | 338.42] loss=0.32 avg=1.26\n",
      "[28201 | 338.54] loss=0.38 avg=1.25\n",
      "[28202 | 338.65] loss=0.96 avg=1.25\n",
      "[28203 | 338.76] loss=1.20 avg=1.25\n",
      "[28204 | 338.87] loss=2.09 avg=1.25\n",
      "[28205 | 338.99] loss=0.29 avg=1.24\n",
      "[28206 | 339.11] loss=0.20 avg=1.23\n",
      "[28207 | 339.23] loss=0.64 avg=1.23\n",
      "[28208 | 339.33] loss=2.33 avg=1.24\n",
      "[28209 | 339.45] loss=0.48 avg=1.23\n",
      "[28210 | 339.57] loss=0.65 avg=1.23\n",
      "[28211 | 339.70] loss=0.54 avg=1.22\n",
      "[28212 | 339.82] loss=1.31 avg=1.22\n",
      "[28213 | 339.96] loss=3.77 avg=1.25\n",
      "[28214 | 340.06] loss=0.71 avg=1.24\n",
      "[28215 | 340.19] loss=0.91 avg=1.24\n",
      "[28216 | 340.32] loss=0.39 avg=1.23\n",
      "[28217 | 340.45] loss=0.39 avg=1.22\n",
      "[28218 | 340.56] loss=1.14 avg=1.22\n",
      "[28219 | 340.69] loss=1.87 avg=1.23\n",
      "[28220 | 340.81] loss=1.18 avg=1.23\n",
      "[28221 | 340.93] loss=0.73 avg=1.22\n",
      "[28222 | 341.03] loss=0.82 avg=1.22\n",
      "[28223 | 341.16] loss=0.31 avg=1.21\n",
      "[28224 | 341.27] loss=1.34 avg=1.21\n",
      "[28225 | 341.39] loss=1.00 avg=1.21\n",
      "[28226 | 341.51] loss=2.93 avg=1.22\n",
      "[28227 | 341.64] loss=0.36 avg=1.21\n",
      "[28228 | 341.74] loss=1.23 avg=1.21\n",
      "[28229 | 341.88] loss=0.31 avg=1.21\n",
      "[28230 | 342.01] loss=1.46 avg=1.21\n",
      "[28231 | 342.14] loss=0.35 avg=1.20\n",
      "[28232 | 342.25] loss=1.20 avg=1.20\n",
      "[28233 | 342.38] loss=0.24 avg=1.19\n",
      "[28234 | 342.50] loss=1.12 avg=1.19\n",
      "[28235 | 342.62] loss=0.87 avg=1.19\n",
      "[28236 | 342.74] loss=0.37 avg=1.18\n",
      "[28237 | 342.85] loss=0.63 avg=1.17\n",
      "[28238 | 342.95] loss=3.77 avg=1.20\n",
      "[28239 | 343.06] loss=1.43 avg=1.20\n",
      "[28240 | 343.18] loss=0.43 avg=1.19\n",
      "[28241 | 343.29] loss=0.98 avg=1.19\n",
      "[28242 | 343.42] loss=0.46 avg=1.18\n",
      "[28243 | 343.53] loss=2.84 avg=1.20\n",
      "[28244 | 343.65] loss=0.17 avg=1.19\n",
      "[28245 | 343.77] loss=0.40 avg=1.18\n",
      "[28246 | 343.89] loss=0.55 avg=1.18\n",
      "[28247 | 344.00] loss=0.47 avg=1.17\n",
      "[28248 | 344.12] loss=0.39 avg=1.16\n",
      "[28249 | 344.23] loss=2.93 avg=1.18\n",
      "[28250 | 344.35] loss=0.23 avg=1.17\n",
      "[28251 | 344.46] loss=1.23 avg=1.17\n",
      "[28252 | 344.57] loss=0.62 avg=1.16\n",
      "[28253 | 344.67] loss=1.41 avg=1.17\n",
      "[28254 | 344.80] loss=0.39 avg=1.16\n",
      "[28255 | 344.92] loss=1.32 avg=1.16\n",
      "[28256 | 345.03] loss=2.13 avg=1.17\n",
      "[28257 | 345.15] loss=0.38 avg=1.16\n",
      "[28258 | 345.27] loss=0.60 avg=1.16\n",
      "[28259 | 345.39] loss=1.86 avg=1.16\n",
      "[28260 | 345.54] loss=1.34 avg=1.17\n",
      "[28261 | 345.65] loss=1.98 avg=1.17\n",
      "[28262 | 345.77] loss=0.49 avg=1.17\n",
      "[28263 | 345.90] loss=1.26 avg=1.17\n",
      "[28264 | 346.03] loss=1.86 avg=1.17\n",
      "[28265 | 346.14] loss=1.14 avg=1.17\n",
      "[28266 | 346.24] loss=1.43 avg=1.18\n",
      "[28267 | 346.36] loss=1.76 avg=1.18\n",
      "[28268 | 346.50] loss=0.48 avg=1.18\n",
      "[28269 | 346.62] loss=0.81 avg=1.17\n",
      "[28270 | 346.74] loss=0.91 avg=1.17\n",
      "[28271 | 346.87] loss=0.92 avg=1.17\n",
      "[28272 | 346.99] loss=2.58 avg=1.18\n",
      "[28273 | 347.11] loss=0.99 avg=1.18\n",
      "[28274 | 347.22] loss=2.01 avg=1.19\n",
      "[28275 | 347.35] loss=5.27 avg=1.23\n",
      "[28276 | 347.46] loss=0.61 avg=1.22\n",
      "[28277 | 347.57] loss=2.04 avg=1.23\n",
      "[28278 | 347.67] loss=2.17 avg=1.24\n",
      "[28279 | 347.79] loss=0.90 avg=1.24\n",
      "[28280 | 347.90] loss=1.52 avg=1.24\n",
      "[28281 | 348.01] loss=0.80 avg=1.23\n",
      "[28282 | 348.12] loss=0.78 avg=1.23\n",
      "[28283 | 348.23] loss=0.87 avg=1.23\n",
      "[28284 | 348.39] loss=2.00 avg=1.23\n",
      "[28285 | 348.51] loss=2.70 avg=1.25\n",
      "[28286 | 348.61] loss=0.98 avg=1.25\n",
      "[28287 | 348.72] loss=0.92 avg=1.24\n",
      "[28288 | 348.82] loss=1.01 avg=1.24\n",
      "[28289 | 348.93] loss=1.58 avg=1.24\n",
      "[28290 | 349.05] loss=0.19 avg=1.23\n",
      "[28291 | 349.18] loss=1.03 avg=1.23\n",
      "[28292 | 349.29] loss=1.34 avg=1.23\n",
      "[28293 | 349.41] loss=1.22 avg=1.23\n",
      "[28294 | 349.52] loss=0.87 avg=1.23\n",
      "[28295 | 349.63] loss=1.14 avg=1.23\n",
      "[28296 | 349.74] loss=0.78 avg=1.22\n",
      "[28297 | 349.86] loss=1.38 avg=1.23\n",
      "[28298 | 349.99] loss=0.34 avg=1.22\n",
      "[28299 | 350.11] loss=4.40 avg=1.25\n",
      "Generating samples...\n",
      "Seed text : 여러 제과점이 서로 경쟁을 하면 소비자에게 어떤 점이 좋을까요?\n",
      "[10001]\n",
      "여러 제과점이 서로 경쟁을 하면 소비자에게 어떤 점이 좋을까요? 점점 많이 발전하는 제품이있는다. 서로이 낮은 경쟁을 통해 살 수 있다. 더 좋은 소비자에게 서로의 질에서 제공되있다.\n",
      "여러 제과점이 서로 경쟁을 하면 소비자에게 어떤 점이 좋을까요? 서로 이득을 받을수 있다. ⁇  공기를 얻고 있다. 서로 이득이 될 것이다. 그래서........\n",
      "여러 제과점이 서로 경쟁을 하면 소비자에게 어떤 점이 좋을까요? 더 이득을 통해 서로 더  ⁇ 을수있고 ⁇ 은 상품을 받을 수 있다.  ⁇ 을 제공하여 지구온난화가 없다\n",
      "[28300 | 352.07] loss=3.48 avg=1.27\n",
      "[28301 | 352.17] loss=0.34 avg=1.26\n",
      "[28302 | 352.29] loss=0.96 avg=1.26\n",
      "[28303 | 352.41] loss=1.28 avg=1.26\n",
      "[28304 | 352.53] loss=0.19 avg=1.25\n",
      "[28305 | 352.64] loss=2.81 avg=1.26\n",
      "[28306 | 352.76] loss=1.37 avg=1.26\n",
      "[28307 | 352.87] loss=2.59 avg=1.28\n",
      "[28308 | 353.02] loss=0.49 avg=1.27\n",
      "[28309 | 353.16] loss=3.74 avg=1.29\n",
      "[28310 | 353.27] loss=2.23 avg=1.30\n",
      "[28311 | 353.37] loss=1.10 avg=1.30\n",
      "[28312 | 353.48] loss=2.41 avg=1.31\n",
      "[28313 | 353.59] loss=2.09 avg=1.32\n",
      "[28314 | 353.71] loss=1.64 avg=1.32\n",
      "[28315 | 353.84] loss=1.29 avg=1.32\n",
      "[28316 | 353.95] loss=0.63 avg=1.32\n",
      "[28317 | 354.06] loss=1.48 avg=1.32\n",
      "[28318 | 354.17] loss=0.52 avg=1.31\n",
      "[28319 | 354.30] loss=1.28 avg=1.31\n",
      "[28320 | 354.42] loss=1.31 avg=1.31\n",
      "[28321 | 354.54] loss=2.14 avg=1.32\n",
      "[28322 | 354.66] loss=0.28 avg=1.31\n",
      "[28323 | 354.79] loss=1.48 avg=1.31\n",
      "[28324 | 354.89] loss=0.42 avg=1.30\n",
      "[28325 | 355.00] loss=2.73 avg=1.31\n",
      "[28326 | 355.12] loss=0.60 avg=1.31\n",
      "[28327 | 355.26] loss=1.09 avg=1.31\n",
      "[28328 | 355.39] loss=0.58 avg=1.30\n",
      "[28329 | 355.51] loss=1.09 avg=1.30\n",
      "[28330 | 355.66] loss=0.64 avg=1.29\n",
      "[28331 | 355.78] loss=1.52 avg=1.29\n",
      "[28332 | 355.90] loss=0.14 avg=1.28\n",
      "[28333 | 356.01] loss=0.86 avg=1.28\n",
      "[28334 | 356.13] loss=0.70 avg=1.27\n",
      "[28335 | 356.26] loss=0.64 avg=1.26\n",
      "[28336 | 356.40] loss=2.90 avg=1.28\n",
      "[28337 | 356.54] loss=0.54 avg=1.27\n",
      "[28338 | 356.65] loss=0.73 avg=1.27\n",
      "[28339 | 356.77] loss=1.04 avg=1.27\n",
      "[28340 | 356.88] loss=1.99 avg=1.27\n",
      "[28341 | 356.98] loss=1.22 avg=1.27\n",
      "[28342 | 357.09] loss=2.16 avg=1.28\n",
      "[28343 | 357.21] loss=0.58 avg=1.27\n",
      "[28344 | 357.36] loss=5.43 avg=1.32\n",
      "[28345 | 357.48] loss=2.20 avg=1.32\n",
      "[28346 | 357.61] loss=0.18 avg=1.31\n",
      "[28347 | 357.73] loss=0.33 avg=1.30\n",
      "[28348 | 357.83] loss=1.14 avg=1.30\n",
      "[28349 | 357.94] loss=0.86 avg=1.30\n",
      "[28350 | 358.05] loss=2.73 avg=1.31\n",
      "[28351 | 358.17] loss=1.59 avg=1.31\n",
      "[28352 | 358.29] loss=0.39 avg=1.31\n",
      "[28353 | 358.40] loss=1.06 avg=1.30\n",
      "[28354 | 358.51] loss=1.75 avg=1.31\n",
      "[28355 | 358.62] loss=0.84 avg=1.30\n",
      "[28356 | 358.72] loss=2.54 avg=1.31\n",
      "[28357 | 358.83] loss=0.69 avg=1.31\n",
      "[28358 | 358.93] loss=0.58 avg=1.30\n",
      "[28359 | 359.04] loss=1.26 avg=1.30\n",
      "[28360 | 359.16] loss=0.16 avg=1.29\n",
      "[28361 | 359.28] loss=0.59 avg=1.28\n",
      "[28362 | 359.40] loss=1.75 avg=1.29\n",
      "[28363 | 359.55] loss=3.20 avg=1.31\n",
      "[28364 | 359.68] loss=1.06 avg=1.30\n",
      "[28365 | 359.79] loss=1.62 avg=1.31\n",
      "[28366 | 359.92] loss=1.73 avg=1.31\n",
      "[28367 | 360.03] loss=2.40 avg=1.32\n",
      "[28368 | 360.18] loss=2.40 avg=1.33\n",
      "[28369 | 360.30] loss=1.35 avg=1.33\n",
      "[28370 | 360.43] loss=1.01 avg=1.33\n",
      "[28371 | 360.53] loss=0.64 avg=1.32\n",
      "[28372 | 360.65] loss=0.79 avg=1.32\n",
      "[28373 | 360.75] loss=0.84 avg=1.31\n",
      "[28374 | 360.86] loss=1.22 avg=1.31\n",
      "[28375 | 360.98] loss=0.68 avg=1.31\n",
      "[28376 | 361.09] loss=0.24 avg=1.30\n",
      "[28377 | 361.21] loss=1.09 avg=1.29\n",
      "[28378 | 361.39] loss=0.92 avg=1.29\n",
      "[28379 | 361.50] loss=1.03 avg=1.29\n",
      "[28380 | 361.61] loss=1.99 avg=1.29\n",
      "[28381 | 361.73] loss=0.85 avg=1.29\n",
      "[28382 | 361.86] loss=0.36 avg=1.28\n",
      "[28383 | 361.97] loss=1.35 avg=1.28\n",
      "[28384 | 362.09] loss=0.18 avg=1.27\n",
      "[28385 | 362.20] loss=0.46 avg=1.26\n",
      "[28386 | 362.30] loss=1.74 avg=1.27\n",
      "[28387 | 362.43] loss=0.40 avg=1.26\n",
      "[28388 | 362.58] loss=2.00 avg=1.27\n",
      "[28389 | 362.69] loss=1.23 avg=1.26\n",
      "[28390 | 362.82] loss=1.23 avg=1.26\n",
      "[28391 | 362.93] loss=3.79 avg=1.29\n",
      "[28392 | 363.05] loss=0.31 avg=1.28\n",
      "[28393 | 363.18] loss=1.55 avg=1.28\n",
      "[28394 | 363.30] loss=0.24 avg=1.27\n",
      "[28395 | 363.44] loss=0.26 avg=1.26\n",
      "[28396 | 363.56] loss=0.14 avg=1.25\n",
      "[28397 | 363.69] loss=0.69 avg=1.25\n",
      "[28398 | 363.83] loss=2.55 avg=1.26\n",
      "[28399 | 363.96] loss=1.61 avg=1.26\n",
      "Generating samples...\n",
      "Seed text : 태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요?\n",
      "[10000 10001]\n",
      "태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요? 무만석연료가 화석에너지는 나중에 자전을 할수있고 쓸수 있다. 자원에너지 않기때문이기 때문이다.\n",
      "태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요? 무한한 에너인 환경오염이 사라지기 때문이다. 또한 친력을 배출이 다가기 때문에 화이 없기 때문에 자원에너지 않기 때문에\n",
      "태양광 발전이 석탄이나 석유와 같은 화석에너지에 비해 미래를 위한 지속 가능한 발전에 도움이 되는 이유는 무엇일까요? 태양광 에너지는 자원이 무한해 에너지가 고갈되지 않는다. 고갈되지 않고 무 가능한 자원이 무한하기 때문에 지속 가능한 가능한 화석에\n",
      "[28400 | 365.86] loss=1.53 avg=1.26\n",
      "[28401 | 365.98] loss=2.97 avg=1.28\n",
      "[28402 | 366.09] loss=1.69 avg=1.29\n",
      "[28403 | 366.22] loss=0.43 avg=1.28\n",
      "[28404 | 366.32] loss=1.33 avg=1.28\n",
      "[28405 | 366.46] loss=3.21 avg=1.30\n",
      "[28406 | 366.59] loss=2.51 avg=1.31\n",
      "[28407 | 366.71] loss=0.98 avg=1.31\n",
      "[28408 | 366.83] loss=0.89 avg=1.30\n",
      "[28409 | 366.95] loss=0.90 avg=1.30\n",
      "[28410 | 367.06] loss=0.21 avg=1.29\n",
      "[28411 | 367.21] loss=4.87 avg=1.32\n",
      "[28412 | 367.34] loss=0.92 avg=1.32\n",
      "[28413 | 367.47] loss=0.41 avg=1.31\n",
      "[28414 | 367.61] loss=1.13 avg=1.31\n",
      "[28415 | 367.74] loss=0.25 avg=1.30\n",
      "[28416 | 367.87] loss=0.11 avg=1.29\n",
      "[28417 | 367.97] loss=0.49 avg=1.28\n",
      "[28418 | 368.10] loss=1.33 avg=1.28\n",
      "[28419 | 368.22] loss=4.96 avg=1.31\n",
      "[28420 | 368.33] loss=0.42 avg=1.31\n",
      "[28421 | 368.46] loss=0.40 avg=1.30\n",
      "[28422 | 368.57] loss=2.28 avg=1.31\n",
      "[28423 | 368.73] loss=3.41 avg=1.33\n",
      "[28424 | 368.83] loss=1.20 avg=1.33\n",
      "[28425 | 368.94] loss=0.64 avg=1.32\n",
      "[28426 | 369.05] loss=0.89 avg=1.31\n",
      "[28427 | 369.17] loss=0.90 avg=1.31\n",
      "[28428 | 369.30] loss=2.20 avg=1.32\n",
      "[28429 | 369.42] loss=0.83 avg=1.31\n",
      "[28430 | 369.52] loss=2.25 avg=1.32\n",
      "[28431 | 369.63] loss=1.32 avg=1.32\n",
      "[28432 | 369.74] loss=0.68 avg=1.32\n",
      "[28433 | 369.88] loss=1.00 avg=1.31\n",
      "[28434 | 370.00] loss=0.80 avg=1.31\n",
      "[28435 | 370.12] loss=2.35 avg=1.32\n",
      "[28436 | 370.26] loss=0.64 avg=1.31\n",
      "[28437 | 370.37] loss=0.14 avg=1.30\n",
      "[28438 | 370.48] loss=1.40 avg=1.30\n",
      "[28439 | 370.58] loss=1.37 avg=1.30\n",
      "[28440 | 370.69] loss=0.74 avg=1.30\n",
      "[28441 | 370.82] loss=2.28 avg=1.31\n",
      "[28442 | 370.93] loss=0.77 avg=1.30\n",
      "[28443 | 371.08] loss=2.61 avg=1.31\n",
      "[28444 | 371.21] loss=1.65 avg=1.32\n",
      "[28445 | 371.34] loss=1.73 avg=1.32\n",
      "[28446 | 371.45] loss=0.68 avg=1.32\n",
      "[28447 | 371.55] loss=0.34 avg=1.31\n",
      "[28448 | 371.68] loss=1.08 avg=1.30\n",
      "[28449 | 371.79] loss=2.28 avg=1.31\n",
      "[28450 | 371.90] loss=1.25 avg=1.31\n",
      "[28451 | 372.00] loss=0.97 avg=1.31\n",
      "[28452 | 372.14] loss=1.48 avg=1.31\n",
      "[28453 | 372.25] loss=2.10 avg=1.32\n",
      "[28454 | 372.37] loss=1.00 avg=1.32\n",
      "[28455 | 372.48] loss=0.85 avg=1.31\n",
      "[28456 | 372.58] loss=1.33 avg=1.31\n",
      "[28457 | 372.69] loss=0.79 avg=1.31\n",
      "[28458 | 372.80] loss=0.57 avg=1.30\n",
      "[28459 | 372.91] loss=0.39 avg=1.29\n",
      "[28460 | 373.02] loss=1.35 avg=1.29\n",
      "[28461 | 373.12] loss=0.34 avg=1.28\n",
      "[28462 | 373.23] loss=0.69 avg=1.27\n",
      "[28463 | 373.34] loss=0.80 avg=1.27\n",
      "[28464 | 373.46] loss=0.88 avg=1.27\n",
      "[28465 | 373.57] loss=0.99 avg=1.26\n",
      "[28466 | 373.69] loss=0.21 avg=1.25\n",
      "[28467 | 373.80] loss=0.50 avg=1.25\n",
      "[28468 | 373.93] loss=3.60 avg=1.27\n",
      "[28469 | 374.04] loss=0.70 avg=1.26\n",
      "[28470 | 374.18] loss=0.32 avg=1.25\n",
      "[28471 | 374.29] loss=0.73 avg=1.25\n",
      "[28472 | 374.39] loss=1.63 avg=1.25\n",
      "[28473 | 374.54] loss=2.94 avg=1.27\n",
      "[28474 | 374.68] loss=0.79 avg=1.26\n",
      "[28475 | 374.79] loss=2.26 avg=1.27\n",
      "[28476 | 374.91] loss=0.17 avg=1.26\n",
      "[28477 | 375.04] loss=4.36 avg=1.29\n",
      "[28478 | 375.15] loss=0.63 avg=1.29\n",
      "[28479 | 375.26] loss=0.64 avg=1.28\n",
      "[28480 | 375.37] loss=1.37 avg=1.28\n",
      "[28481 | 375.50] loss=1.11 avg=1.28\n",
      "[28482 | 375.63] loss=2.50 avg=1.29\n",
      "[28483 | 375.74] loss=1.13 avg=1.29\n",
      "[28484 | 375.87] loss=0.50 avg=1.28\n",
      "[28485 | 376.00] loss=3.63 avg=1.31\n",
      "[28486 | 376.13] loss=0.10 avg=1.29\n",
      "[28487 | 376.25] loss=2.37 avg=1.31\n",
      "[28488 | 376.37] loss=0.86 avg=1.30\n",
      "[28489 | 376.48] loss=0.62 avg=1.29\n",
      "[28490 | 376.59] loss=0.55 avg=1.29\n",
      "[28491 | 376.71] loss=6.58 avg=1.34\n",
      "[28492 | 376.83] loss=0.57 avg=1.33\n",
      "[28493 | 376.94] loss=0.82 avg=1.33\n",
      "[28494 | 377.06] loss=1.24 avg=1.33\n",
      "[28495 | 377.20] loss=1.94 avg=1.33\n",
      "[28496 | 377.31] loss=2.60 avg=1.34\n",
      "[28497 | 377.43] loss=2.66 avg=1.36\n",
      "[28498 | 377.55] loss=0.24 avg=1.35\n",
      "[28499 | 377.67] loss=1.24 avg=1.35\n",
      "Saving checkpoint/transfer3/model-28500\n",
      "Generating samples...\n",
      "Seed text : 비커에 있는 소금물의 진하기를 비교하려면 맛보기 이외에 어떤 방법을 사용할 수 있을까요?\n",
      "[10002]\n",
      "비커에 있는 소금물의 진하기를 비교하려면 맛보기 이외에 어떤 방법을 사용할 수 있을까요? 물의 농도를 용질을 측정하기 때문이다. 또 용액을 진하게되고 질에 따라 비교하기 쉴 수증기를 이용한다. 또한 소\n",
      "비커에 있는 소금물의 진하기를 비교하려면 맛보기 이외에 어떤 방법을 사용할 수 있을까요? 진하기 위해서는 염도성이 변하지 못하기 때문. ⁇  우리를 더 높은 소금물 ⁇  염로를 비교할수 없는 것이다. 나\n",
      "비커에 있는 소금물의 진하기를 비교하려면 맛보기 이외에 어떤 방법을 사용할 수 있을까요? 소금물의 진하기를 비커에 따라 비커에 따라 밀도가 비커가 커져서 밀도가 다르기 때문이다. 따라서 물을 넣어 빨리 소금을\n",
      "[28500 | 382.41] loss=2.13 avg=1.35\n",
      "[28501 | 382.53] loss=0.43 avg=1.34\n",
      "[28502 | 382.64] loss=0.28 avg=1.33\n",
      "[28503 | 382.75] loss=0.42 avg=1.32\n",
      "[28504 | 382.89] loss=0.35 avg=1.31\n",
      "[28505 | 383.04] loss=1.21 avg=1.31\n",
      "[28506 | 383.15] loss=2.21 avg=1.32\n",
      "[28507 | 383.26] loss=0.79 avg=1.32\n",
      "[28508 | 383.39] loss=1.32 avg=1.32\n",
      "[28509 | 383.50] loss=2.13 avg=1.33\n",
      "[28510 | 383.62] loss=1.45 avg=1.33\n",
      "[28511 | 383.73] loss=1.47 avg=1.33\n",
      "[28512 | 383.87] loss=3.59 avg=1.35\n",
      "[28513 | 383.98] loss=0.64 avg=1.34\n",
      "[28514 | 384.10] loss=0.46 avg=1.33\n",
      "[28515 | 384.21] loss=0.85 avg=1.33\n",
      "[28516 | 384.31] loss=2.01 avg=1.34\n",
      "[28517 | 384.43] loss=1.29 avg=1.34\n",
      "[28518 | 384.55] loss=2.07 avg=1.34\n",
      "[28519 | 384.66] loss=3.01 avg=1.36\n",
      "[28520 | 384.81] loss=2.05 avg=1.37\n",
      "[28521 | 384.92] loss=1.38 avg=1.37\n",
      "[28522 | 385.05] loss=1.84 avg=1.37\n",
      "[28523 | 385.16] loss=1.41 avg=1.37\n",
      "[28524 | 385.28] loss=0.77 avg=1.37\n",
      "[28525 | 385.40] loss=0.98 avg=1.36\n",
      "[28526 | 385.52] loss=0.09 avg=1.35\n",
      "[28527 | 385.65] loss=0.20 avg=1.34\n",
      "[28528 | 385.76] loss=0.43 avg=1.33\n",
      "[28529 | 385.87] loss=1.63 avg=1.33\n",
      "[28530 | 386.01] loss=0.15 avg=1.32\n",
      "[28531 | 386.13] loss=1.49 avg=1.32\n",
      "[28532 | 386.25] loss=3.09 avg=1.34\n",
      "[28533 | 386.37] loss=0.81 avg=1.33\n",
      "[28534 | 386.50] loss=1.63 avg=1.34\n",
      "[28535 | 386.61] loss=1.23 avg=1.34\n",
      "[28536 | 386.72] loss=2.33 avg=1.35\n",
      "[28537 | 386.86] loss=2.98 avg=1.36\n",
      "[28538 | 386.97] loss=0.91 avg=1.36\n",
      "[28539 | 387.08] loss=0.53 avg=1.35\n",
      "[28540 | 387.21] loss=1.01 avg=1.35\n",
      "[28541 | 387.32] loss=0.23 avg=1.34\n",
      "[28542 | 387.43] loss=1.22 avg=1.33\n",
      "[28543 | 387.54] loss=0.65 avg=1.33\n",
      "[28544 | 387.66] loss=1.72 avg=1.33\n",
      "[28545 | 387.78] loss=0.99 avg=1.33\n",
      "[28546 | 387.90] loss=0.26 avg=1.32\n",
      "[28547 | 388.02] loss=3.41 avg=1.34\n",
      "[28548 | 388.13] loss=0.19 avg=1.33\n",
      "[28549 | 388.25] loss=0.36 avg=1.32\n",
      "[28550 | 388.36] loss=0.91 avg=1.31\n",
      "[28551 | 388.48] loss=0.25 avg=1.30\n",
      "[28552 | 388.59] loss=0.93 avg=1.30\n",
      "[28553 | 388.70] loss=0.38 avg=1.29\n",
      "[28554 | 388.82] loss=0.64 avg=1.28\n",
      "[28555 | 388.93] loss=0.96 avg=1.28\n",
      "[28556 | 389.04] loss=2.38 avg=1.29\n",
      "[28557 | 389.16] loss=4.12 avg=1.32\n",
      "[28558 | 389.29] loss=2.03 avg=1.33\n",
      "[28559 | 389.41] loss=0.56 avg=1.32\n",
      "[28560 | 389.54] loss=1.38 avg=1.32\n",
      "[28561 | 389.65] loss=1.04 avg=1.32\n",
      "[28562 | 389.76] loss=1.25 avg=1.32\n",
      "[28563 | 389.87] loss=0.56 avg=1.31\n",
      "[28564 | 389.99] loss=1.32 avg=1.31\n",
      "[28565 | 390.10] loss=1.33 avg=1.31\n",
      "[28566 | 390.21] loss=1.14 avg=1.31\n",
      "[28567 | 390.31] loss=2.14 avg=1.31\n",
      "[28568 | 390.44] loss=0.46 avg=1.31\n",
      "[28569 | 390.55] loss=2.64 avg=1.32\n",
      "[28570 | 390.67] loss=0.35 avg=1.31\n",
      "[28571 | 390.78] loss=2.12 avg=1.32\n",
      "[28572 | 390.90] loss=4.42 avg=1.35\n",
      "[28573 | 391.02] loss=2.06 avg=1.36\n",
      "[28574 | 391.14] loss=2.56 avg=1.37\n",
      "[28575 | 391.25] loss=1.76 avg=1.37\n",
      "[28576 | 391.37] loss=3.03 avg=1.39\n",
      "[28577 | 391.50] loss=1.05 avg=1.39\n",
      "[28578 | 391.62] loss=0.76 avg=1.38\n",
      "[28579 | 391.73] loss=1.25 avg=1.38\n",
      "[28580 | 391.86] loss=1.31 avg=1.38\n",
      "[28581 | 392.02] loss=4.04 avg=1.40\n",
      "[28582 | 392.14] loss=0.52 avg=1.39\n",
      "[28583 | 392.25] loss=0.21 avg=1.38\n",
      "[28584 | 392.40] loss=2.63 avg=1.40\n",
      "[28585 | 392.51] loss=0.65 avg=1.39\n",
      "[28586 | 392.62] loss=2.26 avg=1.40\n",
      "[28587 | 392.73] loss=0.69 avg=1.39\n",
      "[28588 | 392.84] loss=2.24 avg=1.40\n",
      "[28589 | 392.94] loss=1.64 avg=1.40\n",
      "[28590 | 393.06] loss=0.51 avg=1.39\n",
      "[28591 | 393.17] loss=2.05 avg=1.40\n",
      "[28592 | 393.30] loss=0.80 avg=1.39\n",
      "[28593 | 393.40] loss=0.89 avg=1.39\n",
      "[28594 | 393.51] loss=0.94 avg=1.38\n",
      "[28595 | 393.62] loss=1.17 avg=1.38\n",
      "[28596 | 393.74] loss=0.49 avg=1.37\n",
      "[28597 | 393.87] loss=1.52 avg=1.37\n",
      "[28598 | 393.98] loss=2.03 avg=1.38\n",
      "[28599 | 394.09] loss=2.23 avg=1.39\n",
      "Generating samples...\n",
      "Seed text : 마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요?\n",
      "[10000 10002]\n",
      "마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요? 여러것을 하여 우리나라에 없는 물건이 다르기 때문에 이익을 얻을 수 있다. 산이 없다. 또한 ⁇ 것같거나 ⁇  종에게 우리나라가\n",
      "마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요? 각 나라에서 없는 물건을 교류를 통해 다른 나라와 얻는다?과 교환하여 외국의 양을 얻을 수 있다. 생산되지 않기 때문에 교류를 통해\n",
      "마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요? 각 나라마다 얻면 구하는 물질이 없 없는 물건을 수출 ⁇  교환한다. ⁇  환경이 발달되지 않는것은 발전에 따라 나라에서 고체\n",
      "[28600 | 395.97] loss=1.20 avg=1.39\n",
      "[28601 | 396.08] loss=1.36 avg=1.39\n",
      "[28602 | 396.20] loss=3.47 avg=1.41\n",
      "[28603 | 396.31] loss=1.55 avg=1.41\n",
      "[28604 | 396.42] loss=1.41 avg=1.41\n",
      "[28605 | 396.52] loss=1.81 avg=1.41\n",
      "[28606 | 396.66] loss=2.19 avg=1.42\n",
      "[28607 | 396.77] loss=0.16 avg=1.41\n",
      "[28608 | 396.89] loss=1.09 avg=1.40\n",
      "[28609 | 397.01] loss=0.51 avg=1.40\n",
      "[28610 | 397.13] loss=0.60 avg=1.39\n",
      "[28611 | 397.23] loss=1.04 avg=1.38\n",
      "[28612 | 397.34] loss=0.75 avg=1.38\n",
      "[28613 | 397.46] loss=0.45 avg=1.37\n",
      "[28614 | 397.57] loss=1.10 avg=1.37\n",
      "[28615 | 397.67] loss=1.05 avg=1.36\n",
      "[28616 | 397.81] loss=3.65 avg=1.39\n",
      "[28617 | 397.91] loss=0.31 avg=1.37\n",
      "[28618 | 398.04] loss=1.75 avg=1.38\n",
      "[28619 | 398.15] loss=0.55 avg=1.37\n",
      "[28620 | 398.26] loss=0.74 avg=1.36\n",
      "[28621 | 398.39] loss=2.22 avg=1.37\n",
      "[28622 | 398.50] loss=2.00 avg=1.38\n",
      "[28623 | 398.61] loss=2.77 avg=1.39\n",
      "[28624 | 398.73] loss=0.15 avg=1.38\n",
      "[28625 | 398.84] loss=0.50 avg=1.37\n",
      "[28626 | 398.96] loss=1.06 avg=1.37\n",
      "[28627 | 399.08] loss=0.53 avg=1.36\n",
      "[28628 | 399.19] loss=2.03 avg=1.37\n",
      "[28629 | 399.30] loss=0.55 avg=1.36\n",
      "[28630 | 399.41] loss=1.06 avg=1.36\n",
      "[28631 | 399.51] loss=0.46 avg=1.35\n",
      "[28632 | 399.62] loss=0.48 avg=1.34\n",
      "[28633 | 399.73] loss=1.17 avg=1.34\n",
      "[28634 | 399.83] loss=1.21 avg=1.33\n",
      "[28635 | 399.94] loss=1.51 avg=1.34\n",
      "[28636 | 400.06] loss=0.55 avg=1.33\n",
      "[28637 | 400.19] loss=0.89 avg=1.32\n",
      "[28638 | 400.32] loss=0.36 avg=1.31\n",
      "[28639 | 400.43] loss=1.23 avg=1.31\n",
      "[28640 | 400.55] loss=0.71 avg=1.31\n",
      "[28641 | 400.65] loss=0.72 avg=1.30\n",
      "[28642 | 400.76] loss=0.62 avg=1.30\n",
      "[28643 | 400.90] loss=3.13 avg=1.31\n",
      "[28644 | 401.02] loss=0.53 avg=1.31\n",
      "[28645 | 401.13] loss=2.38 avg=1.32\n",
      "[28646 | 401.25] loss=0.14 avg=1.30\n",
      "[28647 | 401.36] loss=1.52 avg=1.31\n",
      "[28648 | 401.46] loss=1.77 avg=1.31\n",
      "[28649 | 401.59] loss=2.19 avg=1.32\n",
      "[28650 | 401.70] loss=1.61 avg=1.32\n",
      "[28651 | 401.85] loss=2.94 avg=1.34\n",
      "[28652 | 402.01] loss=3.15 avg=1.36\n",
      "[28653 | 402.13] loss=0.69 avg=1.35\n",
      "[28654 | 402.23] loss=2.17 avg=1.36\n",
      "[28655 | 402.36] loss=1.48 avg=1.36\n",
      "[28656 | 402.49] loss=2.10 avg=1.37\n",
      "[28657 | 402.60] loss=0.89 avg=1.36\n",
      "[28658 | 402.72] loss=0.65 avg=1.36\n",
      "[28659 | 402.83] loss=1.79 avg=1.36\n",
      "[28660 | 402.94] loss=1.09 avg=1.36\n",
      "[28661 | 403.05] loss=2.31 avg=1.37\n",
      "[28662 | 403.16] loss=0.84 avg=1.36\n",
      "[28663 | 403.26] loss=0.67 avg=1.35\n",
      "[28664 | 403.37] loss=0.35 avg=1.34\n",
      "[28665 | 403.48] loss=0.81 avg=1.34\n",
      "[28666 | 403.59] loss=2.34 avg=1.35\n",
      "[28667 | 403.70] loss=0.83 avg=1.34\n",
      "[28668 | 403.82] loss=0.34 avg=1.33\n",
      "[28669 | 403.93] loss=3.07 avg=1.35\n",
      "[28670 | 404.05] loss=0.49 avg=1.34\n",
      "[28671 | 404.16] loss=1.90 avg=1.35\n",
      "[28672 | 404.27] loss=0.78 avg=1.34\n",
      "[28673 | 404.38] loss=0.86 avg=1.34\n",
      "[28674 | 404.54] loss=3.32 avg=1.36\n",
      "[28675 | 404.66] loss=0.36 avg=1.35\n",
      "[28676 | 404.77] loss=0.31 avg=1.34\n",
      "[28677 | 404.88] loss=4.43 avg=1.37\n",
      "[28678 | 404.99] loss=1.00 avg=1.36\n",
      "[28679 | 405.10] loss=1.01 avg=1.36\n",
      "[28680 | 405.23] loss=1.92 avg=1.37\n",
      "[28681 | 405.33] loss=0.66 avg=1.36\n",
      "[28682 | 405.44] loss=1.36 avg=1.36\n",
      "[28683 | 405.57] loss=2.34 avg=1.37\n",
      "[28684 | 405.76] loss=2.97 avg=1.39\n",
      "[28685 | 405.91] loss=1.94 avg=1.39\n",
      "[28686 | 406.05] loss=3.40 avg=1.41\n",
      "[28687 | 406.16] loss=1.59 avg=1.41\n",
      "[28688 | 406.27] loss=0.30 avg=1.40\n",
      "[28689 | 406.39] loss=1.51 avg=1.40\n",
      "[28690 | 406.52] loss=1.43 avg=1.40\n",
      "[28691 | 406.64] loss=0.27 avg=1.39\n",
      "[28692 | 406.76] loss=0.63 avg=1.38\n",
      "[28693 | 406.90] loss=0.24 avg=1.37\n",
      "[28694 | 407.03] loss=1.77 avg=1.38\n",
      "[28695 | 407.14] loss=0.39 avg=1.37\n",
      "[28696 | 407.25] loss=0.81 avg=1.36\n",
      "[28697 | 407.36] loss=0.35 avg=1.35\n",
      "[28698 | 407.46] loss=2.09 avg=1.36\n",
      "[28699 | 407.57] loss=0.87 avg=1.35\n",
      "Generating samples...\n",
      "Seed text : 높은 산을 올라가거나 ⁇  100미터 달리기를 하고 난 뒤에 우리 몸에는 다양한 변화가 나타납니다. 급격한 운동을 하고 난 뒤에 우리 몸에는 어떤 변화가 생길까요? 그리고 그런 변화는 어떤 역할을 할까요? 운동 후 몸의 변화에 관한 지식을 활용하여 자세하게 설명하여 봅시다.\n",
      "[10004 10008]\n",
      "높은 산을 올라가거나 ⁇  100미터 달리기를 하고 난 뒤에 우리 몸에는 다양한 변화가 나타납니다. 급격한 운동을 하고 난 뒤에 우리 몸에는 어떤 변화가 생길까요? 그리고 그런 변화는 어떤 역할을 할까요? 운동 후 몸의 변화에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 심장이 뛰고 호흡이 뛰고 호흡이 올라간다 ⁇  땀이 땀이 난다. 이 산소비자가 땀을 뛴다. 노\n",
      "높은 산을 올라가거나 ⁇  100미터 달리기를 하고 난 뒤에 우리 몸에는 다양한 변화가 나타납니다. 급격한 운동을 하고 난 뒤에 우리 몸에는 어떤 변화가 생길까요? 그리고 그런 변화는 어떤 역할을 할까요? 운동 후 몸의 변화에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 운동 후 심장이 빨리 뛰기 위해 땀이 많이뛴다. 땀이 배출한다.  ⁇ . 우리 몸에서 열이 난다. 그래서 땀\n",
      "높은 산을 올라가거나 ⁇  100미터 달리기를 하고 난 뒤에 우리 몸에는 다양한 변화가 나타납니다. 급격한 운동을 하고 난 뒤에 우리 몸에는 어떤 변화가 생길까요? 그리고 그런 변화는 어떤 역할을 할까요? 운동 후 몸의 변화에 관한 지식을 활용하여 자세하게 설명하여 봅시다. 심장박동이빠진다. 운동 후 산소를 공급해줌에서 노폐물을 뛰게된다. 또 ⁇  심장박동이 빨라지고 숨이\n",
      "[28700 | 409.46] loss=0.45 avg=1.34\n",
      "[28701 | 409.57] loss=0.78 avg=1.34\n",
      "[28702 | 409.70] loss=2.12 avg=1.35\n",
      "[28703 | 409.81] loss=1.59 avg=1.35\n",
      "[28704 | 409.91] loss=1.11 avg=1.35\n",
      "[28705 | 410.04] loss=1.95 avg=1.35\n",
      "[28706 | 410.14] loss=0.71 avg=1.35\n",
      "[28707 | 410.27] loss=2.11 avg=1.35\n",
      "[28708 | 410.40] loss=0.19 avg=1.34\n",
      "[28709 | 410.52] loss=0.42 avg=1.33\n",
      "[28710 | 410.71] loss=1.72 avg=1.34\n",
      "[28711 | 410.82] loss=1.50 avg=1.34\n",
      "[28712 | 410.95] loss=0.42 avg=1.33\n",
      "[28713 | 411.06] loss=0.54 avg=1.32\n",
      "[28714 | 411.18] loss=1.16 avg=1.32\n",
      "[28715 | 411.29] loss=1.16 avg=1.32\n",
      "[28716 | 411.41] loss=1.09 avg=1.32\n",
      "[28717 | 411.53] loss=1.26 avg=1.32\n",
      "[28718 | 411.65] loss=4.89 avg=1.35\n",
      "[28719 | 411.76] loss=2.01 avg=1.36\n",
      "[28720 | 411.87] loss=0.53 avg=1.35\n",
      "[28721 | 412.00] loss=3.06 avg=1.37\n",
      "[28722 | 412.11] loss=0.58 avg=1.36\n",
      "[28723 | 412.23] loss=0.25 avg=1.35\n",
      "[28724 | 412.33] loss=1.66 avg=1.35\n",
      "[28725 | 412.45] loss=0.65 avg=1.34\n",
      "[28726 | 412.55] loss=1.36 avg=1.34\n",
      "[28727 | 412.66] loss=0.45 avg=1.33\n",
      "[28728 | 412.78] loss=1.59 avg=1.34\n",
      "[28729 | 412.89] loss=0.71 avg=1.33\n",
      "[28730 | 413.04] loss=0.44 avg=1.32\n",
      "[28731 | 413.14] loss=0.38 avg=1.31\n",
      "[28732 | 413.27] loss=1.18 avg=1.31\n",
      "[28733 | 413.38] loss=0.70 avg=1.31\n",
      "[28734 | 413.48] loss=1.85 avg=1.31\n",
      "[28735 | 413.62] loss=0.34 avg=1.30\n",
      "[28736 | 413.75] loss=1.45 avg=1.30\n",
      "[28737 | 413.88] loss=1.70 avg=1.31\n",
      "[28738 | 414.00] loss=0.31 avg=1.30\n",
      "[28739 | 414.11] loss=0.45 avg=1.29\n",
      "[28740 | 414.23] loss=0.18 avg=1.28\n",
      "[28741 | 414.34] loss=1.70 avg=1.28\n",
      "[28742 | 414.44] loss=0.31 avg=1.27\n",
      "[28743 | 414.55] loss=0.31 avg=1.26\n",
      "[28744 | 414.68] loss=1.74 avg=1.27\n",
      "[28745 | 414.79] loss=1.27 avg=1.27\n",
      "[28746 | 414.90] loss=1.66 avg=1.27\n",
      "[28747 | 415.02] loss=0.29 avg=1.26\n",
      "[28748 | 415.14] loss=0.72 avg=1.26\n",
      "[28749 | 415.26] loss=0.82 avg=1.25\n",
      "[28750 | 415.38] loss=0.81 avg=1.25\n",
      "[28751 | 415.48] loss=0.24 avg=1.24\n",
      "[28752 | 415.60] loss=0.18 avg=1.23\n",
      "[28753 | 415.72] loss=1.09 avg=1.22\n",
      "[28754 | 415.83] loss=0.79 avg=1.22\n",
      "[28755 | 415.93] loss=0.54 avg=1.21\n",
      "[28756 | 416.05] loss=0.71 avg=1.21\n",
      "[28757 | 416.17] loss=0.23 avg=1.20\n",
      "[28758 | 416.32] loss=0.77 avg=1.19\n",
      "[28759 | 416.44] loss=0.45 avg=1.19\n",
      "[28760 | 416.54] loss=1.49 avg=1.19\n",
      "[28761 | 416.68] loss=1.87 avg=1.20\n",
      "[28762 | 416.80] loss=0.95 avg=1.19\n",
      "[28763 | 416.93] loss=1.46 avg=1.20\n",
      "[28764 | 417.04] loss=1.92 avg=1.20\n",
      "[28765 | 417.16] loss=0.55 avg=1.20\n",
      "[28766 | 417.27] loss=0.83 avg=1.19\n",
      "[28767 | 417.38] loss=0.70 avg=1.19\n",
      "[28768 | 417.49] loss=0.65 avg=1.18\n",
      "[28769 | 417.61] loss=1.38 avg=1.19\n",
      "[28770 | 417.74] loss=0.96 avg=1.18\n",
      "[28771 | 417.85] loss=2.66 avg=1.20\n",
      "[28772 | 417.96] loss=6.30 avg=1.25\n",
      "[28773 | 418.07] loss=0.62 avg=1.24\n",
      "[28774 | 418.17] loss=0.47 avg=1.24\n",
      "[28775 | 418.29] loss=0.92 avg=1.23\n",
      "[28776 | 418.40] loss=2.30 avg=1.24\n",
      "[28777 | 418.51] loss=0.69 avg=1.24\n",
      "[28778 | 418.62] loss=0.52 avg=1.23\n",
      "[28779 | 418.74] loss=0.46 avg=1.22\n",
      "[28780 | 418.85] loss=2.36 avg=1.23\n",
      "[28781 | 418.97] loss=0.37 avg=1.23\n",
      "[28782 | 419.08] loss=1.25 avg=1.23\n",
      "[28783 | 419.20] loss=0.11 avg=1.21\n",
      "[28784 | 419.30] loss=1.56 avg=1.22\n",
      "[28785 | 419.41] loss=1.57 avg=1.22\n",
      "[28786 | 419.53] loss=0.63 avg=1.22\n",
      "[28787 | 419.64] loss=0.52 avg=1.21\n",
      "[28788 | 419.75] loss=0.41 avg=1.20\n",
      "[28789 | 419.89] loss=3.62 avg=1.22\n",
      "[28790 | 420.01] loss=0.13 avg=1.21\n",
      "[28791 | 420.14] loss=0.54 avg=1.21\n",
      "[28792 | 420.26] loss=0.95 avg=1.20\n",
      "[28793 | 420.37] loss=1.57 avg=1.21\n",
      "[28794 | 420.48] loss=0.47 avg=1.20\n",
      "[28795 | 420.60] loss=0.86 avg=1.20\n",
      "[28796 | 420.72] loss=1.04 avg=1.20\n",
      "[28797 | 420.88] loss=4.05 avg=1.22\n",
      "[28798 | 420.99] loss=1.42 avg=1.23\n",
      "[28799 | 421.10] loss=1.59 avg=1.23\n",
      "Generating samples...\n",
      "Seed text : 비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요.\n",
      "[10000 10001 10002]\n",
      "비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요. 뜨거운 물을 부은다 ⁇  물의  ⁇ 는다. 물을 더 좋은화에기 때문이다. 용해본다. ⁇  소금이 녹이면 용해\n",
      "비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요. 소금을 녹을때 ⁇ 열로 녹인다. 물의 온도를 높이거나 온도를 높인다. ⁇  소금이 녹인다. ⁇  용액을\n",
      "비커에서 녹지 않은 소금을 다 녹이기 위해 할 수 있는 여러 가지 방법들을 설명해 보세요. 뜨거운 물을 부어서 소금이 녹을때 양이기 때문에 소금이 녹을 때문이다. 소금이 녹을때 용해본다. 물을 부은다\n",
      "[28800 | 422.94] loss=0.60 avg=1.22\n",
      "[28801 | 423.06] loss=0.85 avg=1.22\n",
      "[28802 | 423.17] loss=0.92 avg=1.22\n",
      "[28803 | 423.27] loss=1.56 avg=1.22\n",
      "[28804 | 423.39] loss=0.45 avg=1.21\n",
      "[28805 | 423.51] loss=1.05 avg=1.21\n",
      "[28806 | 423.62] loss=2.21 avg=1.22\n",
      "[28807 | 423.74] loss=0.51 avg=1.21\n",
      "[28808 | 423.84] loss=0.93 avg=1.21\n",
      "[28809 | 423.95] loss=0.83 avg=1.21\n",
      "[28810 | 424.07] loss=2.00 avg=1.22\n",
      "[28811 | 424.19] loss=0.57 avg=1.21\n",
      "[28812 | 424.31] loss=2.94 avg=1.23\n",
      "[28813 | 424.43] loss=0.97 avg=1.22\n",
      "[28814 | 424.57] loss=0.48 avg=1.22\n",
      "[28815 | 424.69] loss=2.80 avg=1.23\n",
      "[28816 | 424.79] loss=1.24 avg=1.23\n",
      "[28817 | 424.90] loss=0.83 avg=1.23\n",
      "[28818 | 425.03] loss=1.79 avg=1.23\n",
      "[28819 | 425.13] loss=0.39 avg=1.22\n",
      "[28820 | 425.24] loss=0.60 avg=1.22\n",
      "[28821 | 425.35] loss=0.62 avg=1.21\n",
      "[28822 | 425.45] loss=0.53 avg=1.21\n",
      "[28823 | 425.56] loss=0.74 avg=1.20\n",
      "[28824 | 425.66] loss=1.85 avg=1.21\n",
      "[28825 | 425.79] loss=1.12 avg=1.21\n",
      "[28826 | 425.90] loss=1.60 avg=1.21\n",
      "[28827 | 426.01] loss=0.91 avg=1.21\n",
      "[28828 | 426.11] loss=1.31 avg=1.21\n",
      "[28829 | 426.23] loss=0.38 avg=1.20\n",
      "[28830 | 426.38] loss=3.35 avg=1.22\n",
      "[28831 | 426.52] loss=1.75 avg=1.23\n",
      "[28832 | 426.64] loss=1.42 avg=1.23\n",
      "[28833 | 426.78] loss=1.83 avg=1.24\n",
      "[28834 | 426.89] loss=0.46 avg=1.23\n",
      "[28835 | 427.00] loss=0.96 avg=1.22\n",
      "[28836 | 427.11] loss=0.93 avg=1.22\n",
      "[28837 | 427.22] loss=3.00 avg=1.24\n",
      "[28838 | 427.34] loss=1.00 avg=1.24\n",
      "[28839 | 427.50] loss=1.05 avg=1.24\n",
      "[28840 | 427.62] loss=3.21 avg=1.25\n",
      "[28841 | 427.73] loss=1.48 avg=1.26\n",
      "[28842 | 427.84] loss=1.10 avg=1.26\n",
      "[28843 | 427.97] loss=0.84 avg=1.25\n",
      "[28844 | 428.07] loss=0.27 avg=1.24\n",
      "[28845 | 428.19] loss=0.17 avg=1.23\n",
      "[28846 | 428.31] loss=0.11 avg=1.22\n",
      "[28847 | 428.43] loss=0.83 avg=1.22\n",
      "[28848 | 428.54] loss=0.34 avg=1.21\n",
      "[28849 | 428.65] loss=0.54 avg=1.20\n",
      "[28850 | 428.77] loss=0.21 avg=1.19\n",
      "[28851 | 428.90] loss=1.73 avg=1.20\n",
      "[28852 | 429.02] loss=0.98 avg=1.19\n",
      "[28853 | 429.13] loss=0.37 avg=1.19\n",
      "[28854 | 429.25] loss=1.40 avg=1.19\n",
      "[28855 | 429.37] loss=0.86 avg=1.18\n",
      "[28856 | 429.47] loss=1.12 avg=1.18\n",
      "[28857 | 429.58] loss=0.85 avg=1.18\n",
      "[28858 | 429.70] loss=0.41 avg=1.17\n",
      "[28859 | 429.81] loss=0.60 avg=1.17\n",
      "[28860 | 429.93] loss=0.55 avg=1.16\n",
      "[28861 | 430.05] loss=0.32 avg=1.15\n",
      "[28862 | 430.16] loss=1.51 avg=1.16\n",
      "[28863 | 430.28] loss=0.22 avg=1.15\n",
      "[28864 | 430.39] loss=0.47 avg=1.14\n",
      "[28865 | 430.49] loss=0.37 avg=1.13\n",
      "[28866 | 430.60] loss=1.83 avg=1.14\n",
      "[28867 | 430.71] loss=0.08 avg=1.13\n",
      "[28868 | 430.82] loss=0.71 avg=1.12\n",
      "[28869 | 430.93] loss=2.80 avg=1.14\n",
      "[28870 | 431.05] loss=1.27 avg=1.14\n",
      "[28871 | 431.18] loss=3.56 avg=1.17\n",
      "[28872 | 431.29] loss=0.97 avg=1.16\n",
      "[28873 | 431.41] loss=0.17 avg=1.15\n",
      "[28874 | 431.53] loss=2.02 avg=1.16\n",
      "[28875 | 431.65] loss=1.89 avg=1.17\n",
      "[28876 | 431.76] loss=0.30 avg=1.16\n",
      "[28877 | 431.87] loss=1.56 avg=1.17\n",
      "[28878 | 431.98] loss=0.96 avg=1.16\n",
      "[28879 | 432.08] loss=2.38 avg=1.18\n",
      "[28880 | 432.19] loss=0.71 avg=1.17\n",
      "[28881 | 432.29] loss=2.10 avg=1.18\n",
      "[28882 | 432.40] loss=1.24 avg=1.18\n",
      "[28883 | 432.51] loss=2.39 avg=1.19\n",
      "[28884 | 432.63] loss=0.39 avg=1.19\n",
      "[28885 | 432.74] loss=1.98 avg=1.19\n",
      "[28886 | 432.85] loss=2.45 avg=1.21\n",
      "[28887 | 432.99] loss=2.22 avg=1.22\n",
      "[28888 | 433.11] loss=0.39 avg=1.21\n",
      "[28889 | 433.22] loss=1.20 avg=1.21\n",
      "[28890 | 433.32] loss=1.65 avg=1.21\n",
      "[28891 | 433.45] loss=0.20 avg=1.20\n",
      "[28892 | 433.56] loss=0.54 avg=1.20\n",
      "[28893 | 433.68] loss=0.69 avg=1.19\n",
      "[28894 | 433.79] loss=0.57 avg=1.18\n",
      "[28895 | 433.91] loss=0.14 avg=1.17\n",
      "[28896 | 434.02] loss=1.84 avg=1.18\n",
      "[28897 | 434.13] loss=0.65 avg=1.18\n",
      "[28898 | 434.23] loss=1.84 avg=1.18\n",
      "[28899 | 434.36] loss=0.69 avg=1.18\n",
      "Generating samples...\n",
      "Seed text : 마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요?\n",
      "[10000]\n",
      "마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요? 우리나라에서 얻을수있다. 그리고 생산되는 자 중화 것들이 다른 나라마다 자연환경이 다르기 때문에 제품을 잘식물도 다르기 때문에 우리나라\n",
      "마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요? 각 가게 나라의 환경에 비해 다른 나라에 따라 생산되는 물질들이 다르기 때문이다. ⁇  서로 교환을 할수있어서 ⁇ 문에 서로에 따라\n",
      "마트에 가면 외국에서 생산된 농산물들이 많이 있습니다. 미국에는 우리나라 김과 굴 등의 수산물들이 많이 수출됩니다. 여러 나라끼리 교류를 통해 물건을 교환하는 이유는 무엇일까요? 다른 교환하는것과 물건을 다른나라에서 얻을수 있기 때문에. 생산하기 때문이다.. ⁇ ???????????\n",
      "[28900 | 436.24] loss=1.16 avg=1.18\n",
      "[28901 | 436.37] loss=2.56 avg=1.19\n",
      "[28902 | 436.47] loss=1.81 avg=1.20\n",
      "[28903 | 436.58] loss=1.74 avg=1.20\n",
      "[28904 | 436.71] loss=1.17 avg=1.20\n",
      "[28905 | 436.84] loss=1.11 avg=1.20\n",
      "[28906 | 436.96] loss=2.72 avg=1.22\n",
      "[28907 | 437.08] loss=2.48 avg=1.23\n",
      "[28908 | 437.19] loss=1.33 avg=1.23\n",
      "[28909 | 437.31] loss=0.07 avg=1.22\n",
      "[28910 | 437.42] loss=0.95 avg=1.22\n",
      "[28911 | 437.53] loss=0.58 avg=1.21\n",
      "[28912 | 437.64] loss=0.40 avg=1.20\n",
      "[28913 | 437.78] loss=0.53 avg=1.19\n",
      "[28914 | 437.90] loss=0.35 avg=1.19\n",
      "[28915 | 438.02] loss=0.45 avg=1.18\n",
      "[28916 | 438.15] loss=1.34 avg=1.18\n",
      "[28917 | 438.27] loss=0.16 avg=1.17\n",
      "[28918 | 438.37] loss=2.37 avg=1.18\n",
      "[28919 | 438.50] loss=0.84 avg=1.18\n",
      "[28920 | 438.63] loss=0.34 avg=1.17\n",
      "[28921 | 438.75] loss=1.28 avg=1.17\n",
      "[28922 | 438.86] loss=2.72 avg=1.19\n",
      "[28923 | 438.98] loss=0.22 avg=1.18\n",
      "[28924 | 439.11] loss=0.15 avg=1.17\n",
      "[28925 | 439.22] loss=0.88 avg=1.16\n",
      "[28926 | 439.37] loss=0.93 avg=1.16\n",
      "[28927 | 439.49] loss=0.18 avg=1.15\n",
      "[28928 | 439.60] loss=1.83 avg=1.16\n",
      "[28929 | 439.73] loss=0.92 avg=1.16\n",
      "[28930 | 439.85] loss=0.67 avg=1.15\n",
      "[28931 | 439.98] loss=0.74 avg=1.15\n",
      "[28932 | 440.10] loss=1.00 avg=1.15\n",
      "[28933 | 440.21] loss=1.20 avg=1.15\n",
      "[28934 | 440.35] loss=1.19 avg=1.15\n",
      "[28935 | 440.46] loss=1.11 avg=1.15\n",
      "[28936 | 440.56] loss=1.37 avg=1.15\n",
      "[28937 | 440.67] loss=0.06 avg=1.14\n",
      "[28938 | 440.79] loss=1.09 avg=1.14\n",
      "[28939 | 440.90] loss=0.83 avg=1.13\n",
      "[28940 | 441.02] loss=1.01 avg=1.13\n",
      "[28941 | 441.13] loss=0.66 avg=1.13\n",
      "[28942 | 441.26] loss=0.34 avg=1.12\n",
      "[28943 | 441.37] loss=1.02 avg=1.12\n",
      "[28944 | 441.48] loss=0.92 avg=1.12\n",
      "[28945 | 441.60] loss=3.79 avg=1.14\n",
      "[28946 | 441.71] loss=1.80 avg=1.15\n",
      "[28947 | 441.83] loss=1.25 avg=1.15\n",
      "[28948 | 441.95] loss=0.64 avg=1.15\n",
      "[28949 | 442.07] loss=0.93 avg=1.14\n",
      "[28950 | 442.19] loss=0.89 avg=1.14\n",
      "[28951 | 442.30] loss=0.12 avg=1.13\n",
      "[28952 | 442.41] loss=1.50 avg=1.14\n",
      "[28953 | 442.52] loss=1.11 avg=1.14\n",
      "[28954 | 442.63] loss=3.33 avg=1.16\n",
      "[28955 | 442.75] loss=0.71 avg=1.15\n",
      "[28956 | 442.87] loss=1.10 avg=1.15\n",
      "[28957 | 443.00] loss=0.82 avg=1.15\n",
      "[28958 | 443.12] loss=1.26 avg=1.15\n",
      "[28959 | 443.23] loss=1.93 avg=1.16\n",
      "[28960 | 443.35] loss=0.32 avg=1.15\n",
      "[28961 | 443.46] loss=1.28 avg=1.15\n",
      "[28962 | 443.56] loss=0.69 avg=1.15\n",
      "[28963 | 443.67] loss=0.99 avg=1.14\n",
      "[28964 | 443.78] loss=0.41 avg=1.14\n",
      "[28965 | 443.90] loss=0.84 avg=1.13\n",
      "[28966 | 444.00] loss=1.07 avg=1.13\n",
      "[28967 | 444.11] loss=4.26 avg=1.16\n",
      "[28968 | 444.24] loss=1.21 avg=1.17\n",
      "[28969 | 444.36] loss=0.22 avg=1.16\n",
      "[28970 | 444.46] loss=1.06 avg=1.15\n",
      "[28971 | 444.59] loss=2.84 avg=1.17\n",
      "[28972 | 444.69] loss=1.04 avg=1.17\n",
      "[28973 | 444.80] loss=0.56 avg=1.16\n",
      "[28974 | 444.92] loss=3.58 avg=1.19\n",
      "[28975 | 445.04] loss=0.73 avg=1.18\n",
      "[28976 | 445.16] loss=0.14 avg=1.17\n",
      "[28977 | 445.28] loss=0.98 avg=1.17\n",
      "[28978 | 445.41] loss=1.98 avg=1.18\n",
      "[28979 | 445.53] loss=1.42 avg=1.18\n",
      "[28980 | 445.64] loss=1.00 avg=1.18\n",
      "[28981 | 445.76] loss=0.51 avg=1.17\n",
      "[28982 | 445.88] loss=0.31 avg=1.16\n",
      "[28983 | 446.00] loss=0.90 avg=1.16\n",
      "[28984 | 446.12] loss=0.75 avg=1.16\n",
      "[28985 | 446.25] loss=1.49 avg=1.16\n",
      "[28986 | 446.35] loss=3.11 avg=1.18\n",
      "[28987 | 446.48] loss=0.99 avg=1.18\n",
      "[28988 | 446.60] loss=0.57 avg=1.17\n",
      "[28989 | 446.72] loss=0.89 avg=1.17\n",
      "[28990 | 446.84] loss=2.65 avg=1.18\n",
      "[28991 | 446.95] loss=1.30 avg=1.19\n",
      "[28992 | 447.07] loss=2.06 avg=1.19\n",
      "[28993 | 447.19] loss=0.56 avg=1.19\n",
      "[28994 | 447.33] loss=0.41 avg=1.18\n",
      "[28995 | 447.44] loss=1.11 avg=1.18\n",
      "[28996 | 447.56] loss=0.78 avg=1.18\n",
      "[28997 | 447.70] loss=2.75 avg=1.19\n",
      "[28998 | 447.82] loss=0.85 avg=1.19\n",
      "[28999 | 447.94] loss=1.55 avg=1.19\n",
      "Saving checkpoint/transfer3/model-29000\n",
      "Generating samples...\n",
      "Seed text : 보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다.\n",
      "[10001]\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 따뜻하게 유지한다. 밀도가 안나서 열이 빠져나갈 수 있다. 액체 ⁇   ⁇ 어 가열되서 열은 열이\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 열이 빠져나가는 성질이 있어서 잘된다. 물이 따뜻하므로 열을 배출하는 양을 배출하기 위해서이다.. 그러므로 열이 잘 전달하는\n",
      "보온병은 물의 온도를 일정하게 유지하도록 만들어진 병입니다. 보온병에 따뜻한 물을 넣어 두면 추운 날씨에도 따뜻한 물을 마실 수 있습니다. 보온병이 추운 겨울에도 물을 따뜻하게 유지하는 이유는 무엇일까요? 열의 특징에 관한 지식을 활용하여 자세하게 설명해 봅시다. 열과 보온병에는 열은 단열되어 있습니다. 열에너지가 일어나지 않아 보온도 따뜻하게 유지 될 대류 ⁇  복\n",
      "[29000 | 452.77] loss=0.76 avg=1.19\n",
      "[29001 | 452.91] loss=3.31 avg=1.21\n",
      "[29002 | 453.01] loss=1.17 avg=1.21\n",
      "[29003 | 453.13] loss=1.76 avg=1.21\n",
      "[29004 | 453.24] loss=4.59 avg=1.25\n",
      "[29005 | 453.35] loss=1.02 avg=1.25\n",
      "[29006 | 453.45] loss=0.99 avg=1.24\n",
      "[29007 | 453.56] loss=0.44 avg=1.23\n",
      "[29008 | 453.69] loss=0.98 avg=1.23\n",
      "[29009 | 453.81] loss=0.76 avg=1.23\n",
      "[29010 | 453.93] loss=0.57 avg=1.22\n",
      "[29011 | 454.04] loss=1.17 avg=1.22\n",
      "[29012 | 454.15] loss=0.30 avg=1.21\n",
      "[29013 | 454.27] loss=1.13 avg=1.21\n",
      "[29014 | 454.40] loss=1.35 avg=1.21\n",
      "[29015 | 454.50] loss=0.31 avg=1.20\n",
      "[29016 | 454.62] loss=0.37 avg=1.19\n",
      "[29017 | 454.74] loss=0.63 avg=1.19\n",
      "[29018 | 454.85] loss=1.18 avg=1.19\n",
      "[29019 | 454.96] loss=2.32 avg=1.20\n",
      "[29020 | 455.08] loss=3.00 avg=1.22\n",
      "[29021 | 455.19] loss=0.67 avg=1.21\n",
      "[29022 | 455.31] loss=0.52 avg=1.21\n",
      "[29023 | 455.45] loss=0.50 avg=1.20\n",
      "[29024 | 455.56] loss=1.68 avg=1.20\n",
      "[29025 | 455.66] loss=1.35 avg=1.20\n",
      "[29026 | 455.80] loss=0.22 avg=1.19\n",
      "[29027 | 455.93] loss=0.47 avg=1.19\n",
      "[29028 | 456.05] loss=2.66 avg=1.20\n",
      "[29029 | 456.18] loss=0.33 avg=1.19\n",
      "[29030 | 456.29] loss=3.85 avg=1.22\n",
      "[29031 | 456.41] loss=1.75 avg=1.23\n",
      "[29032 | 456.53] loss=0.95 avg=1.22\n",
      "[29033 | 456.65] loss=1.80 avg=1.23\n",
      "[29034 | 456.75] loss=0.70 avg=1.22\n",
      "[29035 | 456.86] loss=1.44 avg=1.23\n",
      "[29036 | 456.97] loss=1.23 avg=1.23\n",
      "[29037 | 457.09] loss=0.13 avg=1.21\n",
      "[29038 | 457.21] loss=0.93 avg=1.21\n",
      "[29039 | 457.34] loss=2.58 avg=1.23\n",
      "[29040 | 457.45] loss=0.41 avg=1.22\n",
      "[29041 | 457.55] loss=0.50 avg=1.21\n",
      "[29042 | 457.68] loss=0.88 avg=1.21\n",
      "[29043 | 457.79] loss=0.85 avg=1.20\n",
      "[29044 | 457.90] loss=1.08 avg=1.20\n",
      "[29045 | 458.00] loss=0.64 avg=1.20\n",
      "[29046 | 458.11] loss=1.13 avg=1.20\n",
      "[29047 | 458.22] loss=0.50 avg=1.19\n",
      "[29048 | 458.33] loss=1.52 avg=1.19\n",
      "[29049 | 458.45] loss=1.49 avg=1.19\n",
      "[29050 | 458.58] loss=1.56 avg=1.20\n",
      "[29051 | 458.68] loss=1.46 avg=1.20\n",
      "[29052 | 458.79] loss=0.65 avg=1.20\n",
      "[29053 | 458.90] loss=1.11 avg=1.19\n",
      "[29054 | 459.03] loss=0.43 avg=1.19\n",
      "[29055 | 459.15] loss=0.57 avg=1.18\n",
      "[29056 | 459.26] loss=1.15 avg=1.18\n",
      "[29057 | 459.38] loss=0.74 avg=1.18\n",
      "[29058 | 459.52] loss=3.66 avg=1.20\n",
      "[29059 | 459.66] loss=1.43 avg=1.20\n",
      "[29060 | 459.78] loss=1.85 avg=1.21\n",
      "[29061 | 459.90] loss=0.50 avg=1.20\n",
      "[29062 | 460.05] loss=3.60 avg=1.23\n",
      "[29063 | 460.16] loss=0.64 avg=1.22\n",
      "[29064 | 460.27] loss=3.58 avg=1.24\n",
      "[29065 | 460.39] loss=0.19 avg=1.23\n",
      "[29066 | 460.52] loss=1.77 avg=1.24\n",
      "[29067 | 460.64] loss=0.78 avg=1.23\n",
      "[29068 | 460.76] loss=0.97 avg=1.23\n",
      "[29069 | 460.88] loss=1.00 avg=1.23\n",
      "[29070 | 461.03] loss=0.59 avg=1.22\n",
      "[29071 | 461.15] loss=0.95 avg=1.22\n",
      "[29072 | 461.26] loss=1.17 avg=1.22\n",
      "[29073 | 461.40] loss=3.84 avg=1.25\n",
      "[29074 | 461.53] loss=4.03 avg=1.27\n",
      "[29075 | 461.64] loss=0.30 avg=1.26\n",
      "[29076 | 461.74] loss=2.81 avg=1.28\n",
      "[29077 | 461.87] loss=1.12 avg=1.28\n",
      "[29078 | 461.97] loss=0.61 avg=1.27\n",
      "[29079 | 462.09] loss=0.89 avg=1.27\n",
      "[29080 | 462.28] loss=4.51 avg=1.30\n",
      "[29081 | 462.39] loss=0.85 avg=1.30\n",
      "[29082 | 462.49] loss=1.25 avg=1.30\n",
      "[29083 | 462.60] loss=1.13 avg=1.29\n",
      "[29084 | 462.71] loss=1.61 avg=1.30\n",
      "[29085 | 462.81] loss=2.13 avg=1.31\n",
      "[29086 | 462.92] loss=1.26 avg=1.30\n",
      "[29087 | 463.04] loss=0.46 avg=1.30\n",
      "[29088 | 463.15] loss=0.69 avg=1.29\n",
      "[29089 | 463.25] loss=0.67 avg=1.28\n",
      "[29090 | 463.38] loss=4.70 avg=1.32\n",
      "[29091 | 463.51] loss=0.98 avg=1.31\n",
      "[29092 | 463.62] loss=1.13 avg=1.31\n",
      "[29093 | 463.73] loss=1.17 avg=1.31\n",
      "[29094 | 463.83] loss=1.15 avg=1.31\n",
      "[29095 | 463.94] loss=0.82 avg=1.30\n",
      "[29096 | 464.05] loss=1.56 avg=1.31\n",
      "[29097 | 464.17] loss=0.94 avg=1.30\n",
      "[29098 | 464.28] loss=1.07 avg=1.30\n",
      "[29099 | 464.39] loss=1.08 avg=1.30\n",
      "Generating samples...\n",
      "Seed text : 불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요?\n",
      "[10000]\n",
      "불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요? 발화 탈 물질나면 불이 꺼져 불이 꺼지지않아져서 ⁇  온도로 불이 꺼지는 것이다... 그리고 물이 온도를 낮추\n",
      "불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요? 물이 온도를 낮 시간에게하였아서. 결국 불이 꺼져 불이 꺼져 불이 붙을 수 있다.. 지구는것에 물을 뿌린 중\n",
      "불이 붙은 다리미판에 물을 뿌린 후 불이 꺼진 이유는 무엇일까요? 열이 전달해서. 하지만 낮에는 다리미판에 물을 뿌린 후 불이 꺼지는데 불이 꺼 지기 때문이다. 이로 인해 한판의 온도를\n",
      "[29100 | 466.27] loss=0.81 avg=1.29\n",
      "[29101 | 466.39] loss=1.30 avg=1.29\n",
      "[29102 | 466.50] loss=1.62 avg=1.30\n",
      "[29103 | 466.60] loss=0.71 avg=1.29\n",
      "[29104 | 466.73] loss=0.96 avg=1.29\n",
      "[29105 | 466.86] loss=4.24 avg=1.32\n",
      "[29106 | 466.97] loss=1.38 avg=1.32\n",
      "[29107 | 467.08] loss=4.31 avg=1.35\n",
      "[29108 | 467.20] loss=1.33 avg=1.35\n",
      "[29109 | 467.30] loss=0.86 avg=1.34\n",
      "[29110 | 467.42] loss=0.13 avg=1.33\n",
      "[29111 | 467.55] loss=2.81 avg=1.35\n",
      "[29112 | 467.65] loss=2.11 avg=1.35\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config = config) as sess :\n",
    "    context = tf.placeholder(tf.int32, [args.batch_size, None])\n",
    "    labels = tf.placeholder(tf.int32, [args.batch_size, None])\n",
    "    context_in = randomize(context, hparams, args.noise)\n",
    "    context_in = tf.concat([labels, context_in], axis = 1)\n",
    "    slice_index = tf.shape(labels)[1]\n",
    "    output = model.model(hparams = hparams, X = context_in)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels = context[:, 1:], logits = output['logits'][:, slice_index:-1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if args.val_every > 0 :\n",
    "        val_context = tf.placeholder(tf.int32, [args.val_batch_size, None])\n",
    "        val_output = model.model(hparams = hparams, X = val_context)\n",
    "        val_loss = tf.redeuce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = val_context[:, 1:], logits = val_output['logits'][:, :-1]\n",
    "            )\n",
    "        )\n",
    "        val_loss_summary = tf.summary.scalar('val_loss', val_loss)\n",
    "\n",
    "    tf_sample = sample.sample_sequence(\n",
    "        hparams = hparams,\n",
    "        length = args.sample_length,\n",
    "        context = context,\n",
    "        labels = labels,\n",
    "        batch_size = args.batch_size,\n",
    "        temperature = 1.0,\n",
    "        top_k = args.top_k,\n",
    "        top_p = args.top_p\n",
    "    )\n",
    "\n",
    "    all_vars = [v for v in tf.all_variables() if 'model' in v.name]\n",
    "    if args.only_train_transformer_layers :\n",
    "        train_vars = [v for v in all_vars if '/h' in v.name]\n",
    "    else :\n",
    "        train_vars = all_vars\n",
    "\n",
    "    if args.optimizer == 'adam' :\n",
    "        opt = tf.train.AdamOptimizer(learning_rate = args.learning_rate)\n",
    "    elif args.optimizer == 'sgd' :\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate = args.learning_rate)\n",
    "    else :\n",
    "        exit('Bad optimizer : ', args.optimizer)\n",
    "\n",
    "    if args.accumulate_gradients > 1 :\n",
    "        if args.memory_saving_gradients:\n",
    "            exit(\"Memory saving gradients are not implemented for gradient accumulation yet.\")\n",
    "        opt = AccumulatingOptimizer(\n",
    "            opt=opt,\n",
    "            var_list=train_vars)\n",
    "        opt_reset = opt.reset()\n",
    "        opt_compute = opt.compute_gradients(loss)\n",
    "        opt_apply = opt.apply_gradients()\n",
    "        summary_loss = tf.summary.scalar('loss', opt_apply)\n",
    "    else:\n",
    "        if args.memory_saving_gradients:\n",
    "            opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n",
    "        else:\n",
    "            opt_grads = tf.gradients(loss, train_vars)\n",
    "        opt_grads = list(zip(opt_grads, train_vars))\n",
    "        opt_apply = opt.apply_gradients(opt_grads)\n",
    "        summary_loss = tf.summary.scalar('loss', loss)\n",
    "\n",
    "    summary_lr = tf.summary.scalar('learning_rate', args.learning_rate)\n",
    "    summaries = tf.summary.merge([summary_lr, summary_loss])\n",
    "    summary_log = tf.summary.FileWriter(\n",
    "        os.path.join(CHECKPOINT_DIR, args.run_name)\n",
    "    )\n",
    "    summary_log = tf.summary.FileWriter(\n",
    "        os.path.join(CHECKPOINT_DIR, args.run_name)\n",
    "    )\n",
    "\n",
    "    saver = tf.train.Saver(\n",
    "        var_list = all_vars,\n",
    "        max_to_keep = 3,\n",
    "        keep_checkpoint_every_n_hours = 2\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    ckpt = tf.train.latest_checkpoint(\n",
    "        os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "    if ckpt :\n",
    "        saver.restore(sess, ckpt)\n",
    "    \n",
    "    \n",
    "    with open('../merged_data_human_only_cls.pkl', 'rb') as f :\n",
    "        chunks = pickle.load(f)\n",
    "    data_sampler = Sampler(chunks, transfer = True, labels=True)\n",
    "    \n",
    "    print('dataset has', data_sampler.total_size, 'tokens')\n",
    "\n",
    "    print('Training...')\n",
    "\n",
    "    # if args.val_every > 0:\n",
    "    #     # Sample from validation set once with fixed seed to make\n",
    "    #     # it deterministic during training as well as across runs.\n",
    "    #     val_data_sampler = Sampler(val_chunks, seed=1)\n",
    "    #     val_batches = [[val_data_sampler.sample(1024) for _ in range(args.val_batch_size)]\n",
    "    #                    for _ in range(args.val_batch_count)]\n",
    "\n",
    "    counter = 1\n",
    "    counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')\n",
    "\n",
    "    if os.path.exists(counter_path):\n",
    "        # Load the step number if we're resuming a run\n",
    "        # Add 1 so we don't immediately try to save again\n",
    "        with open(counter_path, 'r') as fp:\n",
    "            counter = int(fp.read()) + 1\n",
    "\n",
    "    def save():\n",
    "        maketree(os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "        print(\n",
    "            'Saving',\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name,\n",
    "                         'model-{}').format(counter))\n",
    "        saver.save(\n",
    "            sess,\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),\n",
    "            global_step=counter)\n",
    "        with open(counter_path, 'w') as fp:\n",
    "            fp.write(str(counter) + '\\n')\n",
    "\n",
    "    def generate_samples(enc, label = False):\n",
    "        print('Generating samples...')\n",
    "        context_tokens, context_labels = data_sampler.sample(10, generate = True)\n",
    "        print('Seed text : {}'.format(enc.DecodeIds(context_tokens.tolist())))\n",
    "        print(context_labels)\n",
    "        all_text = []\n",
    "        index = 0\n",
    "        while index < args.sample_num :\n",
    "            \n",
    "            out = sess.run(\n",
    "                tf_sample,\n",
    "                feed_dict={context: args.batch_size * [context_tokens],\n",
    "                           labels : args.batch_size * [context_labels]\n",
    "                          })\n",
    "            out = out.tolist()\n",
    "            for i in range(min(args.sample_num - index, args.batch_size)):\n",
    "                tokens = out[i]\n",
    "                tokens = [idx for idx in tokens if idx not in [10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008]]\n",
    "                text = enc.DecodeIds(tokens)\n",
    "                all_text.append(text)\n",
    "                index += 1\n",
    "            print(text)\n",
    "        maketree(os.path.join(SAMPLE_DIR, args.run_name))\n",
    "        with open(\n",
    "                os.path.join(SAMPLE_DIR, args.run_name,\n",
    "                             'samples-{}').format(counter), 'w') as fp:\n",
    "            fp.write('\\n'.join(all_text))\n",
    "\n",
    "    # def validation():\n",
    "    #     print('Calculating validation loss...')\n",
    "    #     losses = []\n",
    "    #     for batch in tqdm.tqdm(val_batches):\n",
    "    #         losses.append(sess.run(val_loss, feed_dict={val_context: batch}))\n",
    "    #     v_val_loss = np.mean(losses)\n",
    "    #     v_summary = sess.run(val_loss_summary, feed_dict={val_loss: v_val_loss})\n",
    "    #     summary_log.add_summary(v_summary, counter)\n",
    "    #     summary_log.flush()\n",
    "    #     print(\n",
    "    #         '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'\n",
    "    #         .format(\n",
    "    #             counter=counter,\n",
    "    #             time=time.time() - start_time,\n",
    "    #             loss=v_val_loss))\n",
    "\n",
    "    def sample_batch():\n",
    "        return [data_sampler.sample(args.n_ctx) for _ in range(args.batch_size)]\n",
    "\n",
    "\n",
    "    avg_loss = (0.0, 0.0)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if counter % args.save_every == 0:\n",
    "                save()\n",
    "            if counter % args.sample_every == 0:\n",
    "                generate_samples(sp)\n",
    "            # if args.val_every > 0 and (counter % args.val_every == 0 or counter == 1):\n",
    "            #     validation()\n",
    "\n",
    "            if args.accumulate_gradients > 1:\n",
    "                sess.run(opt_reset)\n",
    "                for _ in range(args.accumulate_gradients):\n",
    "                    sess.run(\n",
    "                        opt_compute, feed_dict={context: sample_batch()})\n",
    "                (v_loss, v_summary) = sess.run((opt_apply, summaries))\n",
    "            else:\n",
    "                batch = data_sampler.sample(args.n_ctx)\n",
    "                context_batch = batch[0].reshape(args.batch_size, -1)\n",
    "                label_batch = batch[1].reshape(args.batch_size, -1)\n",
    "                (_, v_loss, v_summary) = sess.run(\n",
    "                    (opt_apply, loss, summaries),\n",
    "                    feed_dict={context: context_batch,\n",
    "                              labels : label_batch})\n",
    "\n",
    "            summary_log.add_summary(v_summary, counter)\n",
    "            avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
    "                        avg_loss[1] * 0.99 + 1.0)\n",
    "\n",
    "            print(\n",
    "                '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
    "                .format(\n",
    "                    counter=counter,\n",
    "                    time=time.time() - start_time,\n",
    "                    loss=v_loss,\n",
    "                    avg=avg_loss[0] / avg_loss[1]))\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        save()\n",
    "    except tf.errors.ResourceExhaustedError :\n",
    "        err_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
